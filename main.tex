\documentclass[11pt,a4paper,headinclude=true,DIV=14,BCOR=8mm,chapterprefix,listof=totoc,twoside,openright,abstracton]{scrbook}

\usepackage[headsepline]{scrpage2}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{float}
\usepackage[intlimits]{amsmath}
% \usepackage{siunitx}
% \usepackage{color}
\usepackage{xcolor}
\usepackage{verbatim}
\usepackage{appendix}
\usepackage{hyperref}
\usepackage{hyperref}
\usepackage{mathtools}
% \usepackage[style=authoryear]{biblatex}
\usepackage{natbib}
% \usepackage{newtxtext}
% \usepackage{newtxmath}
% \usepackage{harvard}
\setcitestyle{aysep={}} 
\bibliographystyle{apalike}
\usepackage{xr}
\usepackage{wrapfig}
% \bibliographystyle{agsm}
%\usepackage{feynmf}
%\usepackage{tensor}
\usepackage[framemethod=tikz]{mdframed} % for a block of text

\setlength{\parindent}{0pt}
\geometry{a4paper, tmargin=3cm, bmargin=3cm, lmargin=3cm, rmargin=3cm, headheight=3em, headsep=2em, footskip=1cm}

\setcitestyle{citesep={,}}

%% --- MY commands --- 
\newcommand{\todo}[1]{\textcolor{red}{$\blacksquare$ TODO: #1}} 
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\magenta}[1]{\textcolor{magenta}{#1}} %% For terms/concepts to remember 
\newcommand{\green}[1]{\textcolor{green}{#1}}
\newcommand{\gray}[1]{\textcolor{gray}{#1}}
\newcommand{\swind}{spiral-wave wind}
\newcommand{\nwind}{$\nu$-component}

\newmdenv[linecolor=cyan,backgroundcolor=cyan!20]{sidenote}


\geometry{a4paper, tmargin=2cm, bmargin=2cm, lmargin=1cm, rmargin=1cm, headheight=2em, headsep=2em, footskip=1cm}

\title{PhD thesis}
\author{Vsevolod Nedora}
\date{today}

\begin{document}
    
    \maketitle

%% --------------- 
%%
%% Theory
%%
%% ---------------

\chapter{General-Relativistic Hydrodynamics}

This chapter is meant to sketch several important parts of the mathematical background. We focus on the aspects relevant for the tools and methods employed in out discussion. We do not aim to provide a comprehensive overview. 
The chapter is divided into \todo{list the parts and their content}

\begin{sidenote}
    \textbf{Note on the exterior algebra} \\
    \textit{Understanding the exterior product, Wedge product} \\
    If $\phi$ and $\psi$ are the 2-forms given for example as 
    \begin{equation}
        \phi = x dx - y dy \hspace{5mm} \text{and} \hspace{5mm}\psi = z dx + x dz
    \end{equation}
    Then the exterior produce is given by 
    \begin{align}
        \phi\wedge\psi &= (x dx - y dy)\wedge(zdx + xdz) = \\
        &=xzdxdx+x^2dxdz-yzdydx-yxdydz= \\
        &=yzdxdy + x^2 dx dz - xydydz
    \end{align}
    as $dxdx=0$ and $dydx=-dxdy$. The product of two 1-forms is a 2-form.
    In general, the wedge product of a$p$-form and $q$-form is a $(p+q)$-form. \\
    
    Next, consider a surface $\mathcal{M}$ and two 1-forms on it $\phi$ and $\psi$ Then the wedge product is 
    \begin{equation}
        (\phi\wedge\psi)(v,w)=\phi(v)\psi(w) - \phi(w)\psi(v)
    \end{equation}
    for any $v$ and $w$ tangent vectors to $\mathcal{M}$. \\
    
    The central idea in exterior algebra is that the operations are designed to create the permutational antisymmetry. Let the $dx_i$ be the basis 1-from $\omega_j$ are the orbitrary $p$-form (of order $p_j$), $a$ and $b$ are arbitrary scalars. Then the wedge product is defined to have properties:
    
    \begin{align}
        (a\omega_1+b\omega_2)\wedge\omega_3 &= a\omega_1\wedge\omega_3+b\omega_2\wedge\omega_3 \hspace{5mm} (p_1 = p_2), \\
        (\omega_1\wedge\omega_2)\wedge\omega_3 &= \omega_1\wedge(\omega_2\wedge\omega_3), \hspace{5mm} a(\omega_1\wedge\omega_2) =  (a\omega_1)\wedge\omega_2\\
        dx_i\wedge dx_j &= -dx_j\wedge dx_i
    \end{align}
    Thus, any arbitrary differential form can be reduced to a coefficient multiplying $dx_i$ or a wedge produce of the generic form 
    \begin{equation}
        dx_i\wedge dx_j \wedge...\wedge dx_p
    \end{equation}
    with the properties allowing to put all coefficients together as 
    \begin{equation}
        a dx_1 \wedge b dx_2 = - a(b dx_2 \wedge dx_1) = -ab(dx_2 \wedge dx_1) = ab(dx_1 \wedge dx_2)
    \end{equation}
    
    The exterior or wedge product acts on tangent vectors. The $\wedge$ or two tangent vectors $\boldsymbol{u}\wedge\boldsymbol{v}$, ($\boldsymbol{u}, \boldsymbol{v}\in T_p(\mathcal{M})$) is an antisymmetric tensor product that in addition to bilinarity requires antisymmetry. 
    \begin{align}
        \boldsymbol{v} =& v^1e_1 + v^2 e_2 + v^3 e_3 \\
        \boldsymbol{u} =& u^1e_1 + u^2 e_2 + u^3 e_3 \\
        \boldsymbol{v}\wedge\boldsymbol{u} =& (v^1u^1 - v^2u^1)(e_1\wedge e_2) + \\
        & + (v^1u^3 - v^3u^1)(e_1\wedge e_1) + \\
        & + (v^2u^3 - v^3u^2)(e_2\wedge e_1)
    \end{align}
    mimicing the behaviour of the cross product. However, this can easly be extended to higher dimensions. \\
    Important, that the resulting object of $\boldsymbol{v}\wedge\boldsymbol{u}$ does not belong to $T_p M$. It is called and alternating bivector and is an element of the vector space $\Lambda^2 T_p (\mathcal{M})$ ,that is called -- second exterior power of $T_p \mathcal{M}$. \\
    Generally one obtains $\Lambda^k T_p (\mathcal{M})$ that is a linear subspace of $T_p ^k (\mathcal{M})$\\
    Consider a cotangent space $T_p ^* \mathcal{M}$. The exterior product on this space is copativle with wedge product on $T_p\mathcal{M}$ and is usually denoted with the same symbol and yeilds: $(\boldsymbol{\alpha}\wedge\boldsymbol{\beta})\in\Lambda^2 T_p ^* \mathcal{M}$.
\end{sidenote}

\begin{sidenote}
    \textbf{Differentia form} \\
    Used for multivariable calculus independent of coordinates. Used for integrands over curves, manifolds. For example, differential form can be used to define a volume element as $f(x,y,z)dx \wedge dy \wedge dz$.\\
    Albegra of diff.forms is organized to reflect the orientation of the domain of integration. For instance: the exterior product $d$ that converts $k$-from into $k+1$-form. This operation is similar to the divergence and the curl of a vector field. \\
    Differential 1-forms are naturally dual to vector field on a manifold. Pairing is done via inner product. \\
    If there are two manifolds, then the albegra of diff.forms and their exterior derivatives is preserved by the \textit{pullblack} under the smooth function. This allows geometrically invariant information to be moved from one space to another via the pullback. \\
    Let $\mathcal{M}$ be an orientated $m$-dimentional manifold and $\mathcal{M}'$ is the same manifold with the opposite orientation and $\omega$ is an $m$-form, then 
    \begin{equation}
        \int_{\mathcal{M}}\omega = -\int_{\mathcal{M}'}\omega.
    \end{equation}
    
    The \textit{exterior algebra} is used to make the notion of an oriented density precise.
    The basic $1$-forms are \textit{differentials} ofthe coordiantes $dx^1,...,dx^n$. Each of them is a \textit{covector} that measures a small displacement in the corresponding coordinate direction. A general $1$-form thus is the combination  of these differentials 
    \begin{equation}
        f_1dx^1\cdot\cdot\cdot f_ndx^n
    \end{equation}
    where $f_k=f_k(x^1,...,x^n)$ are functions of all the coordiantes. \\
    Wedge product is similar to cross product, and is used to be higher diff. forms out of lower ones, as the cross product in vector calculus. \\
    
    The \textit{Exterior derivative}, operator $d$, which is a generalisation of a differential of a function. Let $\omega=fdx^I$ being a simple $k$-form. Then its exterior derivative $d\omega$ is a $(k+1)$-form set by taking differential of the doefficient functions
    \begin{equation}
        d\omega = \sum_{i=1}^n \frac{\partial f}{\partial x^i}dx^i \wedge dx^I
    \end{equation}
    Thus a deferential form, lets say, differential 2-form is called an exterior derivative $da$ of $a=\sum_{j=1}^{n}f_j dx^j$. It is given by
    \begin{equation}
        da = \sum_{j=1}^n df_j \wedge dx^j = \sum_{i,j=1}^n \frac{\partial f_j}{\partial x^i}dx^i\wedge dx^j.
    \end{equation}
    Overall, the $da=0$ is required for a function $f$ such that $a=df$.
    
    On as mooth manifold $\mathcal{M}$ the differential from of degree $k$ is a smooth section of the $k$th exterior power of the cotangent bundle of $\mathcal{M}$. Then, the set of all the $k-$forms on $\mathcal{M}$ is a \textit{vector space} $\Omega^k(\mathcal{M})$. The formal definition then stands. At any point $p\in \mathcal{M}$ a $k-$form $\beta$ defines an element 
    \begin{equation}
        \beta_p\in\Lambda^kT^* _p \mathcal{M}
    \end{equation}
    where $T_p\mathcal{M}$is the tangent space tp $\mathcal{M}$ at $p$. The $T^* _p \mathcal{M}$ is its dual space. Thus, $\beta$ is also a linear functional such that $\beta_p:\Lambda^k T_p \mathcal{M}\rightarrow I\!R$
\end{sidenote}

\begin{sidenote}
    \textbf{Differential forms on a Reimannian maniforld} \\
    There metric defines a fiber-wise isomorphism of the tangent and cotangent spaces. This allows to convert vector fields to covector field and vice versa. It also allows the definition of the \textit{Hodge star operator}.
    
    Hodge star operator $\star$ is a linear map, defined on the exterior algebra of a finite-dimensional oriented vector space endowed with a nongegenerate symmetric bilinear form. Applying the operator to the element of the algebra produces the \textit{Hodge dual} of the element. \\ 
    
    Example. Consider a 3D Euclidean space. Let there be an orientated plane, that is presented by the exteriour product $\wedge$, of two basis vectors. Then its Hodge dual is the normal vector biven by the cross product. \\
    The Hodge operator $\star$ is a one-to-one mapping of $k-$ to $(n-k)$-vectors.\\
    
    The $\star$ can be applied to the cotangent bundle of a pseudo-reimanian manifold -- to differential $k$-forms. This allows the definition of a differential as a Hodge adjoint of the exteior derivative. 
    
    Formal definition. Let $V$ be a $n$-dimensional vector space with nondegenerate symmetric bilinear form $\langle\cdot,\cdot\rangle$ -- the inner product. This induces an inner product on $k-$vectors $\alpha,\beta\in\Lambda^k V$ for $0\leq k \leq n$ by defining it on decomposable $k$-vectors $\alpha = \alpha_1\wedge\cdots\wedge\alpha_k$ and $\beta=\beta_1\wedge\cdots\wedge\beta_k$
    ...
    \\
    The Hodge star operator is a linear operator on the exterior algebra of $V$, mapping $k$-vectors to $(n-k)$-vectors for $0\leq k \leq n$. It has following property that defines it completely
    \begin{equation}
        \alpha\wedge(\star\beta) = \langle\alpha,\beta\rangle\omega 
    \end{equation}
    for every pair of $k-$vectors $\alpha\beta\in\Lambda^kV$ Here the $\omega\in\Lambda^nV$ is the unit $n-$vector defined in terms of an oriented orthonormal basis $\{e_1,...,e_n\}$ of $V$ as
    \begin{equation}
        \omega := e_1 \wedge \cdots \wedge e_n.
    \end{equation}
    
    Dually in the space $\Lambda^n V^*$ of $n-forms$, the dual $\omega$ is the colume form $\textbf{det}$, the function whose value on $v_1\wedge\cdots\wedge v_n$ is the determinant of the $n\times n$ matric assembled from the column vectors of $v_i$ in $e_i$ coordinates. Thus the dual difinition is 
    \begin{equation}
       \text{det}(\alpha\wedge\star\beta) = \langle\alpha,\beta\rangle.
    \end{equation}
    or ecvilalently 
    \begin{align}
        \alpha =& \alpha_1\wedge\cdots\wedge\alpha_k \\
        \beta =& \beta_1\wedge\cdots\wedge\beta_k \\
        \star\beta =& \beta_1 ^{\star} \wedge\cdots\wedge \beta_{n-k} ^ {\star} \\
        \text{det}(\alpha_1\wedge\cdots\wedge\alpha_k\wedge\beta_1 ^{\star}\wedge\cdots\wedge\beta_{n-k}^{\star}) =& \text{det}(\langle\alpha_i,\beta_j\rangle)
    \end{align}
    
    \textit{Geometrical interpetation.}  -- cannot understand
    \textit{Examples}
    Consider 2D with normalized Euclidian metric and orientation given by ordering $(x,y)$. The Hodge star on $k-$forms is given by 
    
    \begin{align}
        \star 1 &= dx \wedge dy \\
        \star dx &= dy \\
        \star dy &= -dx \\
        \star(dx \wedge dy) &= 1.
    \end{align}
    
    Consider a more complex example. A plane that can be regarded as a vector space with a standard sesquilinear form as the metric. There the Hodge operator has a properiy that it is invariant under the holomorphic cahges of cooridantes. Consider $z = x + iy$ holomorphic function of $w=u + iv$. Then in the new coordiantes 
    
    \begin{align}
        \alpha &= pdx +qdy \\
        \star \alpha &= -q dx + p dy
    \end{align}
    
    \textit{3D} \\
    Here the $\star$ can be regarded as a correspondence between vectors and bivectors. Thus in Eucledian $\boldsymbol{R}^3$ with basis $dx,dy,dz$, basis of oneforms one finds
    \begin{align}
        \star dx =& dy\wedge dz \\
        \star dy =& dz\wedge dx \\
        \star dz =& dx \wedge dy \\
    \end{align}
    The relation to the exteriour and cross producs are:
    \begin{equation}
        \star(\boldsymbol{u}\wedge\boldsymbol{v})=\boldsymbol{u}\times\boldsymbol{v}, \hspace{5mm}\star(\boldsymbol{u}\times\boldsymbol{v}) = \boldsymbol{u}\wedge\boldsymbol{v}
    \end{equation}
    Thus in 3D the $\star$ provides and isomorphism between vectors and bivectors, so each axial vector $\boldsymbol{a}$ is associated with the bivector $\boldsymbol{A}$ as $\boldsymbol{A} = \star\boldsymbol{a}$ and $\boldsymbol{a} = \star\boldsymbol{A}$. It can also mean a correspondance betweeen the axis and ifenitesicmal rotation around the axis with the speed equal to the length of the axis vector.
    %% One can see that mapping $\star:V\rightarrow\Lambda^2V\subset V\otimes V$
    Consider a tensor $dx \otimes dy$that correesponds to the matrix with one $dx$ row and $dy$ column. The wedge $dx\wedge dy = dx\otimes dy - dy\otimes dx$ is a 3 by 3 \textit{skew-symmetric matrix} with all 0 excet $01$ and $10$ components that are 1. So the $\wedge$ operator turns $\boldsymbol{v} = adx + bdy + cdz$ into $\star\boldsymbol{v}\approx$ 3x3 matrix with 0 on diaoganals. \\
    
    \textit{4D}\\
    Here $\star$ acts as an endomorphism of the second exteriour power, mapping 2-forms into 2-forms.Consider Minkowski space time with signature $(+---)$ and coordinates $(t,x,y,z)$
    
    \begin{align}
        \star dt &= dx \wedge dy \wedge dz \\
        \star dx &= dt \wedge dy \wedge dz \\
        \star dy &= -dt \wedge dx \wedge dz \\ 
        \star dz &= dt \wedge dx \wedge dy 
    \end{align}
    
    \textit{Wedge on manifold}
    For an $n-$dimensional oriented pseudo-Reimannian manifold $\mathcal{M}$ we apply the construction such that to each cotangent vector space $T^* _p \mathcal{M}$ and its exterior powers $\Lambda^k T_p ^* \mathcal{M}$and hence to all differential $k-$forms $\xi\in\Omega^k(\mathcal{M})=\Gamma(\Lambda^k T^* \mathcal{M})$, the global sections of the bundle $\Lambda^k T^*\mathcal{M}\rightarrow \mathcal{M}$. The Reimannian metric induces inner product on $\Lambda^k T_p ^* \mathcal{M}$ at each point $p\in\mathcal{M}$. We define the Hodge dual of a $k-$form $\xi$ defining $\star\xi$ as a unique $(n-k)$-form satisfying
    \begin{equation}
        \eta\wedge\star\xi = \langle\eta,\xi\rangle\omega
    \end{equation}
    for every $k-$form $\eta$ where $\langle\eta,\xi\rangle$ is a real value function on $\mathcal{M}$ and the volume form $\omega$ is induced by the Reimannian metric.
    
    \textit{In coordiante form}
    Consider an orthonormal basis $\{ \frac{\partial}{\partial x_1}, \cdots,\frac{\partial}{\partial x_n} \}$ the a tangent space $V=T_p\mathcal{M}$. And its dual basis $\{ dx_1, ..., dx_n \}$ in $V^* = T_p ^*\mathcal{M}$, with the metric matrix $g_{ij} = \big(\langle\frac{\partial}{\partial x_i},\frac{\partial}{\partial x_j}\rangle\big)$ and its inverse matrix $g^{ij} = \big(\langle dx_i, dx_j \rangle\big)$. The Hodge dual of a decomposable $k$-form is them 
    \begin{equation}
        \star(dx^{i_1}\wedge\cdots\wedge dx^{i_k}) = \frac{\sqrt{|\text{det}[g_{ab}]|}}{(n-k)!}g^{i_1 j_1}\cdots g^{i_k j_k} \epsilon_{j_1 ... j_n} dx^{j_{k+1}}\wedge\cdots\wedge dx^{j_n}
    \end{equation}
    
\end{sidenote}

\section{The Cauchy Problem in General Relativity}

In this section we briefly recall the initial-value formulation of the Einstein equations of general relativity through the following steps. We start by introducing notations and the basics of GR. We summarize the Einstein field equations. Then we continue with how EFE can be split in a set of evolutionary equations and constraints. For that we focus on the Arnowitt, Deser and Misner, or ADM, formalism. In the end we comment on the stability of the ADM equations, on the need for strongly-hyperbolic formulations of the EFE, and on the choice of gauge conditions commonly used to
evolve spacetimes with singularities. This overview is based in \cite{Arnowitt:1962hi,Landau:1982dva,Wald:1984,Misner:1973,Baumgarte:2002jm}, which we refer to for more detained discussion.

\subsection{Euler-Lagrange equations}

We consider a spacetime defined by the real smooth manifold $\mathcal{M}$ and Lorentzian metric $\boldsymbol{g}$ on $\mathcal{M}$ of signature (-,+,+,+). The $\nabla$ denotes the affine connection associated w.ith $\boldsymbol{g}$, the Levi-Civita connection. \\
We use the convention that all Greek indices lie in $\{0, 1, 2, 3\}$ and Lower case Latin indices $\{1, 2, 3\}$. \\
The $\nabla\boldsymbol{T}$ denotes the covariant derivative of a tensor $\boldsymbol{T}$ and $\nabla_{\boldsymbol{u}}\boldsymbol{T}$ -- covariant derivative along a given vector field $\boldsymbol{u}$.\\
The scalar product of two vectors then 
\begin{equation}
    \boldsymbol{a}\cdot\boldsymbol{b}:=g_{\mu\nu}a^{\mu}b^{\nu}
\end{equation}
The action of a linear form on a vector however is represented as 
\begin{equation}
    \langle\boldsymbol{\omega},\boldsymbol{\upsilon}\rangle=\omega_{\mu}\upsilon^{\mu}
\end{equation}

Let the $\boldsymbol{\alpha}$ be the totally antisymmetric symbol that expresses through coordinates $x^{\mu}$ as
\begin{equation}
    \boldsymbol{\alpha} = dx^0 \wedge dx^1 \wedge dx^2 \wedge dx^3,
\end{equation}
where $\wedge$ denotes exterior product. Then, proper volume pseudo-form of the spacetime is

\begin{equation}
    \boldsymbol{\varepsilon} = \sqrt{-g}\boldsymbol{\alpha},
\end{equation}
where $g$ denotes the determinant of the spacetime metric. \\

In GR, the spacetime is represented by Lorentzian manifold $\mathcal{M}$ and $g$, the Loretzian metric. \\

The action principle of the Lagrangian field theory on the spacetime $(\mathcal{M}; \boldsymbol{g})$ is
\begin{equation}
    S(\boldsymbol{q}, \nabla\boldsymbol{q}) = \int_{\mathcal{M}}\boldsymbol{\alpha}\mathcal{L}(\boldsymbol{q}, \nabla\boldsymbol{q}),
\end{equation}
where $\boldsymbol{q}$ are a set of generalized coordinates for the fields described by the theory, $\nabla$ is the Levi-Civita connection, $\mathcal{L}$ is a scalar density of a scalar quantity $\lambda$ as $\lambda(\boldsymbol{q},\nabla\boldsymbol{q})$. 

Varying the action with respect to the $\boldsymbol{q}$
\begin{equation}
    \delta S(\boldsymbol{q}, \nabla\boldsymbol{q}) = \delta\int\boldsymbol{\alpha}\mathcal{L}(\boldsymbol{q}, \nabla\boldsymbol{q}) = \int\boldsymbol{\alpha}\Big(\frac{\partial\mathcal{L}}{\partial\boldsymbol{q}}\delta\boldsymbol{q}+\frac{\partial\mathcal{L}}{\partial(\nabla\boldsymbol{q})}\delta\nabla\boldsymbol{q}\Big)
\end{equation}

As $\delta$ and $\nabla$ commute, and partially integrating $\nabla$, we obtain

\begin{equation}
    \partial S(\boldsymbol{q}, \nabla\boldsymbol{q}) = \int\boldsymbol{\alpha}\Big(\frac{\mathcal{L}}{\partial\boldsymbol{q}}-\nabla\frac{\partial \mathcal{L}}{\partial(\nabla\boldsymbol{q})}\Big)\delta\boldsymbol{q} + \int_{\mathcal{M}}\boldsymbol{\alpha}\nabla\Big(\frac{\partial\mathcal{L}}{\partial(\nabla\boldsymbol{q})}\delta\boldsymbol{q}\Big)
\end{equation}

The last term is a boundary term and in order to vanish we impose boundary condition. Assume that the fields are defined over only a compact domain. \\
As the choice of $\partial\boldsymbol{q}$ is arbitrary, the 

\begin{equation}
    \partial S(\boldsymbol{q}, \nabla\boldsymbol{q}) = 0
\end{equation}

and the Euler-Lagrange equations are

\begin{equation}
    \frac{\partial \mathcal{L}}{\partial\boldsymbol{q}} - \nabla\Big(\frac{\partial\mathcal{L}}{\partial(\nabla\boldsymbol{q})}\Big) = 0
    \label{eq:theory:eulerlagrange}
\end{equation}

%% ----------------------------------------------- 
\subsection{The Hilbert Action}

The Einstein–Hilbert action allows to obtain an Einstein field equations through ad principle of least action. Here we briefly underline the procedure.

Introduce action that describes the graviatational field, and a matter field $\mathcal{L}_m$:
\begin{align}
    S_g &= \int\frac{1}{2\kappa}R\epsilon, \\
    S_m &= \int\mathcal{L}_{m}\epsilon,
\end{align}
where $R$ is the Ricci scalar and $\kappa$ is the  Einstein's constant. \\

The full action then:
\begin{equation}
    S = \int\Big(\frac{1}{2\kappa}R+\mathcal{L}_m\Big)\epsilon
\end{equation}

The action principle dicatates, that $\delta S = 0$  with respect to the inverse metric $g^{\mu\nu}$. 

\begin{equation}
    \int\Bigg[\frac{1}{2\kappa}\Big(\frac{\delta R}{\delta g^{\mu\nu}}+\frac{R}{\sqrt{-g}}\frac{\delta\sqrt{-g}}{\delta g^{\mu\nu}}\Big) + \frac{1}{\sqrt{-g}}\frac{\delta(\sqrt{-g}\mathcal{L}_m)}{\delta g^{\mu\nu}}\Bigg]\delta g^{\mu\nu}\epsilon
\end{equation}

Owing to the arbitrariness of $\delta g^{\mu\nu}$, the integrant must be zero. 

\begin{equation}
    \frac{\delta R}{\delta g^{\mu\nu}} + \frac{R}{\sqrt{-g}}\frac{\delta\sqrt{-g}}{\delta g^{\mu\nu}} = -2\kappa\frac{1}{\sqrt{-g}}\frac{\delta(\sqrt{-g}\mathcal{L}_m)}{\delta g^{\mu\nu}} = -\frac{2\kappa}{\sqrt{-g}}\frac{\delta S_m}{\delta g_{\mu\nu}} := \kappa T_{\mu\nu},
    \label{eq:theory:action1}
\end{equation}
where we introduced the stress-energy tensor $T_{\mu\nu}$ and te matter action $S_m$ for future use. \\

\todo{this matter action is used in deriving the $T_{\mu} ^{\nu}$ i the invariant fluid formalisn}

The continuation of this deriviation requires taking variation of the Riccia scalar $R$ and the determinantof the metric $\sqrt{-g}$. As this is a length procedure, we provide here the result. 

\begin{equation}
    \frac{\delta R}{\delta g^{\mu\nu}} = R_{\mu\nu},
    \label{eq:theory:deltaR}
\end{equation}
where the $R_{\mu\nu}$ is the Ricci curvature tensor.

\begin{equation}
    \frac{1}{\sqrt{-g}}\frac{\delta\sqrt{-g}}{\delta g^{\mu\nu}} = -\frac{1}{2}g_{\mu\nu}.
    \label{eq:theory:deltagmuny}
\end{equation}

Substituting Eq. \ref{eq:theory:deltaR} and Eq. \ref{eq:theory:deltagmuny} into equation of motion Eq.  \ref{eq:theory:action1} we obtain the Einstein's field equation 

\begin{equation}
    R_{\mu\nu} -\frac{1}{2}g_{\mu\nu}R=8\pi T_{\mu\nu},
    \label{eq:theory:EFE}
\end{equation}
where in the geometrized unit system, \textit{i.e} $c=G=1$, the $\kappa=8\pi$.

%% ----------------
\subsection{3+1 Decomposition of Einstein field equations}

The Einstein field equations (\ref{eq:theory:EFE}) represent a set of 10 non-linear partial differential equations. These equations can be defeined on a while metric $\mathcal{M}$ or a domain $\Omega\subset\mathcal{M}$, where in the latter, the boundary conditions on $\partial\Omega$ are required. \\
It is convenient to chose a null hyersurface $\Sigma\subset\mathcal{M}$ on which to define the initial data, from which the evolution of space-time begins. This, however, requires the spacetime to be strongly hyperbolic, meaning that the foliation $\mathcal{M}=\Sigma\times\mathbb{R}$ is allowed. This foliation can be understood as splitting the spacetime into a set of spacelike hypersurfaces $\Sigma_t$. 


\subsubsection{Spacelike Foliations}
Let the $t$ be the global smooth functions such that, 

\begin{equation}
    \Sigma_{\tau} = \{x^{\alpha}\in\mathcal{M}: t(x^{\alpha})=\tau\},
\end{equation}

and let $\vec{t}$ be a vector such that $\langle\nabla t, \vec{t}\rangle = 1$. This the $t$ can be seen as a "function that advances time" and $\vec{t}$ as a "flow of time" vector field. Continuing the analogy, the rate at which a given tensor quantity $\boldsymbol{q}$ changes between hypersurfaces $\Sigma_t$ is given by the Lie derivative of the $\boldsymbol{q}$ along the vector $\vec{t}$. \\

Consider two hypersurfaces $\Sigma_t$ and $\Sigma_{t+dt}$. A transition from one to another can be decomposed into the part tangent to the hypersurface $\Sigma_{t+dt}$ and expressed in a form of a vector $\vec{\beta}$ and a pert normal to the hypersurface $\Sigma_t$ and expressed as a $\alpha \vec{n}$, where $\vec{n}$ is a unit vector, normal to the $\Sigma_t$ in the diretion to $\Sigma_{t+dt}$. Then, the vector $\vec{t}$ can be written as 

\begin{equation}
    \vec{t} = \alpha\vec{n}+\vec{\beta}.
\end{equation}

$\vec{\beta}$ is called shift vector and $\alpha$ is called lapse-function. \\

The spacetime metric $\boldsymbol{g}$ can be decomposed into a spatial, Riemannian metric $\boldsymbol{\gamma}$  as $\boldsymbol{\gamma} = \boldsymbol{g} + \underline{n} \otimes \underline{n} $, where $\underline{n}$ is the 1-form associated to the vector $\vec{n}$. The Levi-Civita connection can be computed by projecting the $\nabla$ on the space tangent to the hypersurface $\Sigma_t$.

There are exist coordinates that are adapted to the foliation, namely $\{t, x^i\}$ with $\vec{\partial}_i\cdot \vec{n} = 0$. In these coordiantes the $\nabla t = dt$ and $\vec{t} = \vec{\partial}_t$. 

The connection between $\boldsymbol{g}$ and $\boldsymbol{\gamma}$ is $g_{\mu\nu}=\vec{\partial}_{\mu}\cdot\vec{\partial}_{\nu} $ and can be expressed in terms of $\alpha$ and $\vec{\beta}$ as

\begin{align}
    \text{spatial components: } g_{ik}&=\vec{\partial}_{i}\cdot\vec{\partial}_{j} =\gamma_{ik}, \\
    \text{time component: } g_{tt} &= \vec{\partial}_{t}\cdot\vec{\partial}_{t} = \vec{t}\cdot\vec{t} = - (\alpha^2-\vec{\beta}\cdot\vec{\beta}), \\
    \text{mixed components: } g_{ti} &= \vec{\partial}_{t}\cdot\vec{\partial}_{i} = \vec{t}\cdot\vec{\partial}_i = (\alpha\vec{n}+\vec{\beta})\cdot\vec{\partial}_i=\beta_i,
\end{align}
we we made use of $\vec{\beta}$ being the spatial vector, \textit{i.e} $\vec{\beta}\cdot\vec{\beta}=\gamma_{ik}\beta^i\beta^k$.

The line-element can be thus written as
\begin{equation}
    ds^2 = -(\alpha^2-\beta_i\beta^i)dt^2 +2\beta_i dx^i dt + \gamma_{ik} dx^i dx^k.
\end{equation}

\subsubsection{Ex-curse: Hamiltonian Field Theory}

First we recall the generalized coordinates $\boldsymbol{q}$ and their covariant derivatives $\nabla\boldsymbol{q}$. \\
In light of the spacetime decomposition discussed above, we divide the $\boldsymbol{\alpha}$ into the time $dt$ and spatial parts represented by the antisymmetric symbol ${^{(3)}\boldsymbol{\alpha}}$ as 

\begin{equation}
    \boldsymbol{\alpha} = dx^0 \wedge dx^1 \wedge dx^2 \wedge dx^3 = dt \wedge {^{(3)}\boldsymbol{\alpha}}.
\end{equation}

Next, we introduce the "time derivative" as a Lie derivative along the vector field $\vec{t}$ as 

\begin{equation}
    \dot{\boldsymbol{q}} := \mathcal{L}_{\vec{t}}\boldsymbol{q}.
\end{equation}

As the $\Lambda(\boldsymbol{q}, \nabla\boldsymbol{q})$ is the Lagrangian density, a conjugate momentum can be defined as 

\begin{equation}
    \boldsymbol{p} := \frac{\partial\Lambda}{\partial\dot{\boldsymbol{q}}},
\end{equation}

Assuming that $\boldsymbol{p}$ and $\nabla\boldsymbol{q}$ can be expressed as a function of $\boldsymbol{q}$ and $\boldsymbol{p}$, inspired by the Legendre transformation, we define the Hamiltonian and its density density as

\begin{align}
    \mathcal{H} &= \boldsymbol{p}\cdot\dot{\boldsymbol{q}} - \mathcal{L}(\boldsymbol{q}, \nabla\boldsymbol{q}) \\
    H &= \int_{\Sigma}\mathcal{H}{^{(3)}\boldsymbol{\alpha}}
\end{align}

Additionally we define the quantity 

\begin{equation}
    J = \int_{0}^{t}H(\boldsymbol{q},\boldsymbol{p})dt = \int_{0}^{t}dt\int_{\Sigma}\mathcal{H}(\boldsymbol{q},\boldsymbol{p}){^{(3)}\boldsymbol{\alpha}} = \int_{0}^{t}dt\int_{\Sigma}{^{(3)}\boldsymbol{\alpha}}\Big(\boldsymbol{p}\cdot\dot{\boldsymbol{q}} - \mathcal{L}(\boldsymbol{q},\nabla\boldsymbol{q})\Big).
\end{equation}

Consider the variation of the $J$ with respect to the $\delta\boldsymbol{p}$ and $\delta\boldsymbol{q}$ as

\begin{equation}
    \delta J = \int_{0}^{t}\delta H(\boldsymbol{q},\boldsymbol{p})dt = \int_{0}^{t}dt (\dot{\boldsymbol{q}}\delta\boldsymbol{p}+\boldsymbol{p}\delta\dot{\boldsymbol{q}}) - \int_{0}^{t}dt\delta\Lambda(\boldsymbol{q}, \nabla\boldsymbol{q}).
\end{equation}

Consider the last term, the variation of the Lagrangian 

\begin{equation}
    \delta\Lambda = \int_{\Sigma}{^{(3)}\boldsymbol{\alpha}}\Bigg[\frac{\delta\Lambda}{\delta\dot{\boldsymbol{q}}}\delta\dot{\boldsymbol{q}}+\frac{\delta\Lambda}{\delta\boldsymbol{q}}\delta\boldsymbol{q}\Bigg],
\end{equation}

The first term in the square brackets can be reduced to $\boldsymbol{p}\delta\dot{\boldsymbol{q}}$, suingthe definition of the conjugate momentum. The second term can be treated, applying the Euler-Lagrange equations (\ref{eq:theory:eulerlagrange}). These manipulations result in

\begin{equation}
    \delta\Lambda = \int_{0}^{t}dt\int_{\Sigma}{^{(3)}\boldsymbol{\alpha}}(\boldsymbol{p}\delta\dot{\boldsymbol{q}} + \dot{\boldsymbol{p}}\delta\boldsymbol{q}).
\end{equation}

Thus we obtain that 

\begin{equation}
    \int_{0}^{t} \delta H(\boldsymbol{q},\boldsymbol{p})dt =   \int_{0}^{t}dt\int_{\Sigma}{^{(3)}\boldsymbol{\alpha}}(\dot{\boldsymbol{q}}\cdot\delta\boldsymbol{p}-\dot{\boldsymbol{p}}\cdot\delta\boldsymbol{q}),
\end{equation}

and as $\delta\boldsymbol{p}$ and $\delta\boldsymbol{p}$ are arbitrary, the Hamilton equations read

\begin{equation}
    \dot{\boldsymbol{q}}=\frac{\delta H}{\delta\boldsymbol{p}}, \hspace{5mm} \dot{\boldsymbol{p}} = -\frac{\delta H}{\delta\boldsymbol{q}}.
    \label{eq:theory:hamiltoneqs}
\end{equation}

The Hamiltonian formalism can be used to redirive the field-equations in a from that once the initial data is specified on a hypersurface $\Sigma_0$ for $\boldsymbol{q}$ and $\boldsymbol{p}$, the equations (\ref{eq:theory:hamiltoneqs}) would govern whole the evolution.


\subsubsection{Extrinsic Curvature and Constraint equations}

We define the \textit{extrinsic curvature} of a $D-1$-suface $\Sigma_t\subset\mathcal{M}$ at a point $\mathcal{P}\in\Sigma_t$ as mapping $\boldsymbol{K}$ such that $\boldsymbol{K}(\boldsymbol{\upsilon})=-\nabla_{\boldsymbol{\upsilon}}\boldsymbol{n}$. Note, that the $\boldsymbol{K}$ thus does not depend on $\alpha$ and $\vec{\beta}$, it is a purely spatial tensor. The components of the extrinsic curvature are \\

\begin{equation}
    K_{\mu\nu} = -{\gamma^{\alpha}}_{\mu}\nabla_{\boldsymbol{u}}^{\alpha} n_{\nu} = -\frac{1}{2}\mathcal{L}_{\vec{n}}\gamma_{\mu\nu},
    \label{eq:theory:extrcurvdef}
\end{equation}
where $\mathcal{L}_{\vec{n}}$ is the Lie derivative along the vector field $\vec{n}$. \\
From the (\ref{eq:theory:extrcurvdef}) the extrinsic curvature can be interprated as a "speed of the $\vec{n}$ during the parallel transport along the hypersurface $\Sigma_t$".

Codazzi equations relate the $4D$ Ricci tensor to the extrinsic curvature as

\begin{equation}
    D_{\beta}K-D_{\alpha}{K^{\alpha}}_{\beta}=R_{\gamma\delta}n^{\delta}{\gamma^{\gamma}}_{\beta},
    \label{eq:theory:formomentum}
\end{equation}

here $K$ is a trace of the tensor $\boldsymbol{K}$. \\

Gauss equation realtes the $3D$ Riemann tensor $^3{R_{\alpha\beta\gamma}}^{\delta}$ to the $4D$ one and the $\boldsymbol{K}$ as

\begin{equation}
    ^3{R_{\alpha\beta\gamma}}^{\delta} = {\gamma^{\mu}}_{\alpha}{\gamma^{\nu}}_{\beta}{\gamma^{\lambda}}_{\gamma}{\gamma^{\delta}}_{\sigma}{R_{\mu\nu\lambda}}^{\delta}-K_{\alpha\gamma}{K_{\beta}}^{\delta}+K_{\beta\gamma}{K^{\delta}}_{\alpha}.
    \label{eq:theory:forhamiltconst}
\end{equation}

The \textit{momentum constraint} thus cab be obtained by substituting the (\ref{eq:theory:EFE}) into  (\ref{eq:theory:formomentum}) which yields

\begin{equation}
    D_{\beta}K-D_{\alpha}{K^{\alpha}}_{\beta} = -8\pi{\gamma^{\alpha}}_{\beta} n^{\gamma}T_{\alpha\gamma}=:8\pi j_{\beta},
    \label{eq:theory:momconstraint}
\end{equation}
where $j^{\alpha}$ is the ADM momentum density. \\

The \textit{Hamiltonian constrant} can be obtained by substituting EFE (\ref{eq:theory:EFE}) into the (\ref{eq:theory:forhamiltconst}), yielding 

\begin{equation}
    ^3 R+ K^2 - K_{\alpha\beta}K^{\alpha\beta} = 2G^{\alpha\beta}n_{\alpha}n_{\beta} = 16\pi n_{\alpha}n_{\beta} T^{\alpha\beta} =: 16\pi E,
    \label{eq:theory:hamilconstraint}
\end{equation}
where $E$ is the ADM energy density. \\

The obtained constraint equations represent a set of elliptic equations that must be satisfied on every hyprsurface $\Sigma_i$ of the foliation. It is however, possible to show that Eistein equations preserve the constraints, meaning that if they are satisfied at the initial slice $\Sigma_0$ they will be satisfied at any time in the future. 





\subsubsection{The Hamiltonian Formulation of the Einstein Equations}

Here we briefly sketch to path of derivation of the Einstein field equations in the Hamiltonian framework. We will elude most of the intimidate and computationally extensive steps, as well as derivation of the boundary terms. For this we refer to \cite{Poisson:2004}.\\
First it is useful to note that determinant of the three-metric $\sqrt{\gamma}$ can be expressed as $\sqrt{\gamma}=\sqrt{-g}/\alpha$. The $p$ is the trace of the canonical momentum $\boldsymbol{p}$.

Now, consider the scalar curvature, R

\begin{align}
    G_{\mu\nu} &= R_{\mu\nu} - \frac{1}{2}Rg_{\mu\nu} \\
    -Rg_{\mu\nu}n^{\nu}n^{\mu} &= 2(G_{\mu\nu} n^{\nu}n^{\mu}-R_{\mu\nu}n^{\mu}n^{\mu})\\
    -Rn_{\mu}n^{\mu}& = 2(G_{\mu\nu}n^{\nu}n^{\mu} - R_{\mu\nu}n^{\mu}n^{\mu}) \\
    R &= 2(G_{\mu\nu}n^{\mu}n^{\nu} - R_{\mu\nu}n^{\mu}n^{\nu}).
\end{align}

From the Gauss-Codacci equation (\ref{eq:theory:momconstraint}), which relates the spatial curvature $^{(3)}R$ to the spacetime curvature $R$, we have the following constraint
relationship

\begin{equation}
    2G_{\mu\nu}n^{\mu}n^{\nu} = {^{(3)}R} + K^2 - K_{\mu\nu}K^{\mu\nu}.
\end{equation}

The $R_{\mu\nu}n^{\mu}n^{\nu})$ can be expressed as a combination of extrinsic curvature and total divergences as 

From the definition of the Ricci tensor $R_{\mu\nu}$, we have:

\begin{align}
    R_{\mu\nu} &= {R_{\mu\gamma\nu}}^{\gamma} \\
    R_{\mu\nu}n^{\mu}n^{\nu} &= {R_{\mu\gamma\nu}}^{\gamma} \\
    &= -(\nabla_{\mu}\nabla_{\gamma} - \nabla_{\gamma}\nabla_{\mu})n^{\gamma}n^{\nu} \\
    &= n^{\mu}(\nabla_{\mu}\nabla_{\gamma} - \nabla_{\gamma}\nabla_{\nu})n^{\gamma} \\
    &= (\nabla_{\mu}n^{\mu})(\nabla_{\gamma}n^{\gamma}) - \nabla_{\mu}(n^{\mu}\nabla_{\gamma}n^{\gamma}) - (\nabla_{\gamma}n^{\mu})(\nabla_{\mu}n^{\gamma}) + \nabla_{\gamma}(n^{\mu}\nabla_{\mu}n^{\gamma}) \\
    &= K^2 - K_{\mu\gamma}K^{\mu\gamma} - \nabla_{\mu}(n^{\mu}\nabla_{\gamma}n^{\gamma}) + \nabla_{\gamma}(n^{\mu}\nabla_{\mu}n^{\gamma})
\end{align}

In case of variations with compact support, that we are interested in, the total divergences. last two terms, can be neglected. Then the result is

\begin{equation}
    R_{\mu\nu}n^{\mu}n^{\nu}= K^2 - K_{\mu\nu}K^{\mu\nu}.
    \label{eq:theory:rmunu_as_func_k}
\end{equation}

Using the fact that $\sqrt{\gamma}=\sqrt{-g}/\alpha$ and the (\ref{eq:theory:rmunu_as_func_k}) we obtain the Lagrangian density in terms of the variables of the hypersurface:

\begin{align}
    \Lambda &= \sqrt{-g}R \\
    &= \alpha\sqrt{\gamma}R \\
    &= 2\alpha\sqrt{\gamma}(G_{\mu\nu}n^{\mu}n^{\nu} - R_{\mu\nu}n^{\mu}n^{\nu})\\ 
    &= 2\alpha\sqrt{\gamma}\Big(\frac{1}{2}[{^{(3)}R} - K_{\mu\nu}K^{\mu\nu} + K^2] - K^2 - K_{\mu\nu}K^{\mu\nu}\Big)
\end{align}

Together with the contribution from matter fields, we obtain

\begin{equation}
    \Lambda = \Lambda_g+\Lambda_m= \frac{1}{16\pi}\alpha({^{(3)}R} + K_{\mu\nu}K^{\mu\nu} - K^2)\sqrt{\gamma}+\Lambda_m
\end{equation}

Next we note that the extrinsic curvature of a
surface $\Sigma$ is defined as $K_{\mu\nu} = \nabla_{\mu}n_{\nu}$. \\
To relate $K_{\mu\nu}$ to the metric, we make use of the following property of Lie derivatives:

\begin{align}
    \mathcal{L}_{\vec{n}}g_{\mu\nu} &= n^{\gamma}\nabla_{\gamma}g_{\mu\nu} + g_{\gamma\nu}\nabla_{\mu}\upsilon^{\gamma} + g_{\mu\gamma}\nabla_{\nu}\upsilon^{\gamma} \\
    &= \nabla_{\mu}n_{\nu}+\nabla_{\nu}\upsilon_{\nu} \\
    &=2\nabla_{\mu}n_{\nu}
\end{align}

where the second line holds when $\nabla_{\gamma}\mu$ is the natural derivative operator corresponding to the metric $g_{\mu\nu}$ and the third line holds because $K_{\mu\nu}$ is symmetric.

Substituting this into our definition of $K_{\mu\nu}$,

\begin{align}
    K_{\mu\nu} &= -\frac{1}{2}\mathcal{L}_{\vec{\vec{n}}}g_{\mu\nu} \\
    &= -\frac{1}{2}\mathcal{L}_{\vec{\vec{n}}}(\gamma_{\mu\nu}-n_{\mu}n_{\nu}) \\
    &= -\frac{1}{2}\mathcal{L}_{\vec{\vec{n}}}\gamma_{\mu\nu} \\
    &= -\frac{1}{2}[n^{\gamma}\nabla_{\gamma}\gamma_{\mu\nu} + \gamma_{\gamma\nu}\nabla_{\mu}\upsilon^{\nu} + h_{\mu\gamma}\nabla_{\nu}\upsilon^{\gamma}] \\
    &= -\frac{1}{2\alpha}[\alpha n^{\gamma}\nabla_{\gamma}\gamma_{\mu\nu} + \gamma_{\gamma\nu}\nabla_{\mu}\alpha\upsilon^{\nu} + h_{\mu\gamma}\nabla_{\nu}\alpha\upsilon^{\gamma}] \\
    &= -\frac{1}{2\alpha}{\gamma_{\mu}}^{\gamma}{\gamma_{\nu}}^{\delta}[\mathcal{L}_{\vec{t}}\gamma_{\gamma\delta}-\mathcal{L}_{\vec{\beta}}\gamma_{\gamma\delta}] \\
    &= -\frac{1}{2\alpha}{\gamma_{\mu}}^{\gamma}{\gamma_{\nu}}^{\delta}[\partial_t\gamma_{\mu\nu}-D_{\mu}\beta_{\nu}-D_{\nu}\beta_{\mu}]
\end{align}

and on the hypersurface $\Sigma$ the projection operators are not needed. So we obtain

\begin{equation}
    K_{\mu\nu} = -\frac{1}{2}\mathcal{L}_{\vec{n}}\gamma_{\mu\nu}=-\frac{1}{2\alpha}(\partial_t\gamma_{\mu\nu}-D_{\mu}\beta_{\nu}-D_{\nu}\beta_{\mu})
\end{equation}

which us to express the canonical momentum $p^{\mu\nu}$ as

\begin{align}
    p^{\mu\nu} &= \frac{\partial\Lambda}{\partial\dot{\gamma}_{\mu\nu}} \\
    &= -\frac{\sqrt{\gamma}}{16\pi}\alpha\Bigg[\frac{\partial {^{(3)}R}}{\partial\dot{\gamma}_{\mu\nu}} + \frac{\partial(K_{\mu\nu}K^{\mu\nu})}{\partial\dot{\gamma}_{\mu\nu}} - \frac{\partial K^2}{\partial\dot{\gamma}_{\mu\nu}}\Bigg] \\
    &= \frac{\sqrt{\gamma}}{16\pi}(K\gamma^{\mu\nu} - K^{\mu\nu}),
\end{align}
where 
\begin{equation}
    \frac{\partial K_{\mu\nu}}{\partial \dot{\gamma}_{\mu\nu}} = \frac{1}{2\alpha}, \hspace{5mm} \frac{\partial {^{(3)}R}}{\partial \dot{\gamma}_{\mu\nu}} = 0, \hspace{5mm}\frac{\partial K^2}{\partial \dot{\gamma}_{\mu\nu}} = \frac{\gamma^{\mu\nu}K}{\alpha}
\end{equation}

assuming that there is no explicit dependency of the $\Lambda$ on $dot{\gamma}_{\mu\nu}$.

Since, $\alpha$ and $\vec{\beta}$ are related to the the gauge freedom, as there are many ways manifold $\mathcal{M}$ can be split into hypersurfaces, the momenta associated with these function and vector is zero. 

Thus, the Hamiltonian density is

\begin{align}
    \mathcal{H} &= p^{\mu\nu}\dot{\gamma}_{\mu\nu} - \Lambda \\
    &= -\sqrt{\gamma}\alpha{^{(3)}R} + \frac{\alpha}{\sqrt{\gamma}}\Big[p^{\mu\nu}p_{\mu\nu}-\frac{1}{2}p^2\Big] + 2p^{\mu\nu} D_{\mu}\beta_{\mu} -\Lambda_m \\
%    &=  \frac{\sqrt{\gamma}}{16\pi}\Bigg\{\alpha\Big[-{^{(3)}R}+h^{-1}p^{\mu\nu}p_{\mu\nu}-\frac{1}{2}h^{-1}p^2\Big] - 2\beta_{\nu}\big[D_{\mu}(h^{-1/2}p^{\mu\nu})\big] + D_{\mu}(h^{-1/2}\beta_{\nu}p^{\mu\nu})\Bigg\} \\
    &= \frac{\sqrt{\gamma}}{16\pi}\Bigg\{\alpha\Big[ -{^{(3)}R} + \gamma^{-1}p^{\mu\nu}p_{\mu\nu}-\frac{1}{2}\gamma^{-1}p^2\Big] +  2\beta_{\nu}\Big[D_{\mu}(\gamma^{-1/2}p^{\mu\nu})\Big] - 2D_{\mu}(\gamma^{-1/2}\beta_{\nu}p^{\mu\nu}) \Bigg\} - \Lambda_m,
\end{align}
where we restored the correct $16\pi$ factor in the last line.

As the we consider variations with compact suppot, the last boundary term, can be neglected. \\

Now we consider the variation of the matter action $S_m$ with respect to the $\alpha$ and $\vec{\beta}$

\begin{align}
    \frac{\delta S_m}{\delta \alpha} &=-\alpha\frac{\delta S_m}{\delta g_{00}} = -\alpha\sqrt{-g}T^{00} = -\alpha^2\sqrt{\gamma}T^{00} = -\sqrt{\gamma}T^{\mu\nu}n_{\mu}n_{\nu} \\
    \frac{\delta S_m}{\delta \beta_{\mu}} &= \frac{\delta S_m}{\delta g_{\mu 0}} =\frac{1}{2}\sqrt{-g}T^{\mu 0} = -\frac{1}{2} \sqrt{\gamma}T^{\mu\nu}n_{\nu}.
\end{align}

As the variation of the Hamiltonian $H$ with respect to a quantity with vanishing canonical momentum is zero, we obtain two equations 

\begin{align}
    \frac{\delta H}{\delta \alpha} &= 0 = -{^{(3)}R} + \gamma^{-1}p^{\mu\nu}p_{\mu\nu}-\frac{1}{2}\gamma^{-1}p^2 + 16\pi T^{\mu\nu}n_{\mu}n_{\nu} \\
    \frac{\delta H}{\delta \beta_{\mu}} &= 0 = - D_{\mu}(\gamma^{-1/2}p^{\mu\nu}) + 8\pi{\gamma^{\mu}}_{\nu}n_{\gamma}T^{\nu\gamma}.
    \label{eq:theory:hamiltonianvariation}
\end{align}


Note, that the $\delta H / \delta\beta_{\mu}$ is actually a Frech\'et differential $dH$, $\delta \beta_{\mu}$, which is writes as
\begin{equation}
    \langle dH,\delta\beta \rangle = \delta\beta_{\mu}\big[-D_{\nu}(\gamma^{-1/2}p^{\mu\nu})+8\pi n_{\gamma}T^{\mu\nu}\big], 
\end{equation}
containing $\delta\beta_{\mu}$ which is spatial. Thus only the spatial part is being constrained in the equation above. To account for that the procector ${\gamma^{\mu}}_{\nu}$ is added to the $\delta H/\delta \beta_{\mu}$. \\

The pair of equations (\ref{eq:theory:hamiltonianvariation}) is in fact the constraint equations derived before, namely the (\ref{eq:theory:momconstraint}) and (\ref{eq:theory:hamilconstraint}), and as we now see, they are related to the coordinate freedom of $\mathcal{M}$ decomposition and a coodrinate freedom on hypersurfaces. \\

Proceeding with the Hamiltinan formalism we note that equation \ref{eq:theory:hamiltoneqs} leads to the evolution equations for the three-metric, assuming that $\Lambda$ explicitly does not depend on the momentum

\begin{equation}
    \dot{\gamma}_{\mu\nu} =\frac{\delta H}{\delta p^{\mu\nu}} = 2\gamma^{-1/2}\alpha\big(p_{\mu\nu}-\frac{1}{2}\gamma_{\mu\nu}p\big) - D_{\nu}\beta_{\mu}-D_{\mu}\beta_{\nu}
%    -2D_{(\mu}\beta_{\nu)},
    \label{eq:theory:_adm_metric_evo}
\end{equation}

The evolution equations for the canonical momentum can read

\begin{align}
    \dot{p}^{\mu\nu} = -\frac{\delta H}{\delta \gamma_{\mu\nu}} = &+ \alpha\gamma^{1/2}\big({^{(3)}R}^{\mu\nu}-\frac{1}{2}{^{(3)}R\gamma^{\mu\nu}}\big) \\
    & - \frac{1}{2}\alpha\gamma^{-1/2}\gamma^{\mu\nu}\big(p_{\gamma\delta}p^{\gamma\delta}-\frac{1}{2}p^2\big) \\
    & + 2\alpha\gamma^{-1/2}\big(p^{\mu\gamma}{p^{\nu}}_{\gamma}-\frac{1}{2}pp^{\mu\nu}\big) \\
    & - \gamma^{1/2}\big(D^{\mu}D^{\nu}\alpha-\gamma^{\mu\nu}D^{\gamma}D_{\gamma}\alpha\big) \\
    & - \gamma^{1/2}D_{\gamma}\big(\gamma^{-1/2}\beta^{\gamma}p^{\mu\nu}\big) \\
    &+ 2p^{\gamma(\mu}D_{\gamma}\beta^{\nu)} + 8\pi \alpha \gamma^{1/2}S^{\mu\nu},
    \label{eq:theory:_adm_mom_evo}
\end{align}
where $A_{(\mu\nu)} = 0.5(A_{\mu\nu}+A_{\nu\mu})$ the convention was used. \\

where $S^{\mu\nu}={\gamma^{\mu}}_{\alpha}{\gamma^{\nu}}_{\beta}T^{\alpha\beta}$. 
Taking the variation of the matter field we noted that 


\begin{equation}
    \frac{\delta S}{\delta \gamma_{ik}} = \frac{\delta S_m}{\delta g_{ik}} = \frac{1}{2}\sqrt{-g}T^{ik}
\end{equation}

The set of equations (\ref{eq:theory:hamiltonianvariation}), (\ref{eq:theory:_adm_metric_evo}) and (\ref{eq:theory:_adm_mom_evo}) comprise the ADM system. A more widely used from of these equations is in turns of $\gamma_{ij}$ and $K_{ij}$ that reads

\begin{align}
    (\partial_t - \mathcal{L}_{\vec{\beta}})\gamma_{ik} &= -2\alpha K_{ik}; \\
    (\partial_t - \mathcal{L}_{\vec{\beta}})K_{ik} &= -D_{i}D_{k}\alpha + \alpha\big(R_{ik} - 2K_{ij}{K^j}_k+KK_{ik}\big) - 8\pi\alpha\big(S_{ik} - \frac{1}{2}\gamma_{ik}(S-E)\big); \\
    {^{(3)}R} + K^2 - K_{ik}K^{ik} &= 16\pi E; \\
    D_{i}K-D_{k}{K^k}_i &= 8\pi j_i,
    \label{eq:theory:adm}
\end{align}
where $S = \gamma^{ij}S_{ij}$.
These equations constitute the IVP for Einstein field equations and are known as ADM equations. The last two equations are the constraint equations. They determine how to set the initial data on the hypersurface $\Sigma_0$, via prescribing the three-metric and extrinsic curvature. The first two equations then govern the evolution.

\todo{make sure that the coefficients in formuals are consistent, $16\pi$ might me missing or $-$}
\todo{Makse sure that $\Lambda$ stands for largangian density and $\mathcal{L}$ for lie derivative}

\subsubsection{Strongly Hyperbolic Formulations of the Einstein Equations}

It has been shown, that the ADM system of equations in its original form (\ref{eq:theory:adm}) is only weekly hyperbolic \cite{Baumgarte:2002jm}. It was shown that in such system the errors tend to couple with zero-velocity modes \cite{Alcubierre:1999rt}.  \\
In an attempt to mitigate this problem, different formulations of the Einstein equations as initial-value problem were created. In particular, the the generalized-harmonic formulation \cite{Friedrich:1985,Lindblom:2005qh,Lindblom:2009}, the BSSNOK formulation, derived by Baumgarte, Shapiro, Shibata, Nakamura, Oohara and Kojima \cite{Nakamura1987,Shibata:1995we,Baumgarte:1998te} and and the Z4 formulation \cite{Bona:2003fj,Bernuzzi:2009ex,Ruiz:2010qj,Weyhausen:2011cg,Alic:2011gg}. We do not attempt to elaborate on any of these formations and only aim to emphasize that a search for a new and better formulations of Einstein equations for numerical applications is ongoing. We limit ourselves to sketching only the conformal-covariant variant of the Z4 formulation, also known as Z4c. The numerical implementation of this formulation was used to obtain the results discussed in this thesis. 


\subsubsection*{The CCZ4 Formulation}

The idea behind the Z4 formulation is to derive a set of evolution equations that is free from the zero-speed modes of the original ADM and thus -- strongly-hyperbolic. This is achieved by not explicitly enforcing the constraints and treating the deviation from them as an dependent variable $Z_{\mu}$. The $Z_{\mu}$ is also called the Z4 four-vector.

One starts with the covariant Lagrangian
\begin{equation}
    \Lambda = g^{\mu\nu}[R_{\mu\nu} + 2\nabla_{\mu}Z_{\nu}]\sqrt{g} + \Lambda_m,
\end{equation}

and applying Palatini-type variational principle \cite{Bona:2010is}, obtains an evolution equations

\begin{equation}
    R_{\mu\nu} + \nabla_{\mu}Z_{\nu} + \nabla_{\nu}Z_{\mu}=8\pi\Big(T_{\mu\nu} - \frac{1}{2}Tg_{\mu\nu}\Big),
    \label{eq:theory:z4fieldeq}
\end{equation}

and two sets of constraint equations

\begin{equation}
    \nabla_{\rho} g^{\mu\nu} = 0, 
    \label{eq:theory:z4connect}
\end{equation}

and

\begin{equation}
    Z_{\mu} = 0,
\end{equation}

where the latter is called an algebraic constraint. If its derivative vanishes, it is equivalent to imposing the ADM momentum and Hamiltonian constraints \cite{Bona:2009}. 

The Einstein field equations themselves are recovered from (\ref{eq:theory:z4connect}) and (\ref{eq:theory:z4fieldeq}) when the algebraic constraint is satisfied. \\

The Z4 system preserves the constraint, $\partial_t (Z_{\mu})= 0$. This allows to obtain the solution of the Einstein equations. 

However, the numerical solution of the system of equations introduces error, that leads to a constraint violation during the evolution. To mitigate this problem the Z4 system is further modified to enforce the dampening of the constraint violation propagation \cite{Gundlach:2005eh}.

A new version of Z4 was recently introduced by \cite{Alic:2011gg}. It incorporates the constraint-damping properties of the original Z4 and also allows for a better black hole treatment via \textit{moving-puncture}, that will be discussed later. 
The CCZ4 system reads 

\begin{align}
    \partial_{t}\widetilde{\gamma}_{ij} = & -2\alpha\widetilde{A}_{ij}^{\text{TF}} + 2\widetilde{\gamma}_{k(i}\partial_{j)}\beta^k - \frac{2}{3}\widetilde{\gamma}_{ij}\partial_k \beta^k + \beta^k\partial_k\widetilde{\gamma}_{ij}, \\
    \partial_{t}\widetilde{A}_{ij}^{\text{TF}} = & \phi^2\big[-\nabla_i\nabla_j\alpha + \alpha\big({^{(3)}R}_{ij}+\nabla_{i}Z_{j} + \nabla_{j}Z_{i}- 8\pi S_{ij}\big)\big]^{\text{TF}} \\
    & + \alpha\widetilde{A}_{ij}(K-2\Theta)-2\alpha\widetilde{A}_{il}{\widetilde{A}^l}_{j} + 2\widetilde{A}_{k(i}\partial_{j)}\beta^{k} \\
    & -\frac{2}{3}\widetilde{A}_{ij}\partial_{k}\beta^{k} + \beta^{k}\partial_{k}\widetilde{A}_{ij} \\
    \partial_{t} \phi = & \frac{1}{3}\alpha\phi K - \frac{1}{3}\phi\partial_{k}\beta^{k} + \beta^{k}\partial_{k}\phi \\
    \partial_{t}K = &-\nabla^{i}\nabla_{i}\alpha + \alpha\big({^{(3)}R} + 2\nabla_{i}Z^{i} + K^2 - 2\Theta K\big) + \beta^{j}\partial_{j}K \\
    & - 3\alpha\kappa_1(1+\kappa_2)\Theta + 4\pi\alpha (S-3E) \\
    \partial_{t}\Theta = &\frac{1}{2}\alpha\Big(R + 2\nabla_{i}Z^{i} - \widetilde{A}_{ij}\widetilde{A}^{ij} + \frac{2}{3}K^2 - 2\Theta K\Big) - Z^{i}\partial_{i}\alpha \\
    & + \beta^{k}\partial_{k}\Theta - \alpha\kappa_1(2 + \kappa_2)\Theta - 8\pi\alpha E \\
    \partial_{t}\hat{\Gamma}^j = & 2\alpha\Bigg({\widetilde{\Gamma}^i}_{jk}\widetilde{A}^{ij} - 3\widetilde{A}^{ij}\frac{\partial_{j}\phi}{\phi} -\frac{2}{3}\widetilde{\gamma}^{ij}\partial_{j}K\Bigg) + 2\widetilde{\gamma}^{ki}\Big(\alpha\partial_{k}\Theta - \Theta\partial_{k}\alpha - \frac{2}{3}\alpha K Z_{k}\Big) \\
    & - 2\widetilde{A}^{ij}\partial_{j}\alpha + \widetilde{\gamma}^{kl}\partial_{k}\partial_{l}\beta^{i} + \frac{1}{3} \widetilde{\gamma}^{ik}\partial_{k}\partial_{l}\beta^{l} + \frac{2}{3}\widetilde{\Gamma}^i\partial_{k}\beta^{k} \\
    & - \widetilde{\Gamma}^k\partial_{k}\beta^{i} + 2\kappa_3\Big(\frac{2}{3}\widetilde{\gamma}^{ij}Z_{j}\partial_{k}\beta^{k} - \widetilde{\gamma}^{jk}Z_{j}\partial_{k}\beta^{i}\Big) + \beta^{k}\partial_{k}\hat{\Gamma}^i \\
    & -2\alpha\kappa_1\widetilde{\gamma}^{ij}Z_{j}- 16\pi\alpha\widetilde{\gamma}^{ij}S_j,
    \label{eq:theory:ccz4equations} % used for Whisky Code description
\end{align}

where $\Theta:=n_{\mu}Z^{\mu}=\alpha Z^0$, the $\widetilde{\Gamma}^i:=\widetilde{\gamma}^{jk}{\widetilde{\Gamma}^i}_{jk} = \widetilde{\gamma}^{ij}\widetilde{\gamma}^{kl}\partial_{l}\widetilde{\gamma}_{jk}$ and $\hat{\Gamma}:=\widetilde{\Gamma}^i + 2\widetilde{\gamma}^{ij}Z_j$, constants $\kappa_1$ and $\kappa_2$ are related to the constraint damping terms, the $\kappa_3$ is the additional constant for further adjustments, the The three-dimensional Ricci tensor ${^{(3)})R}_{ij}$ is split into conformal part $\widetilde{R_{ij}^{\phi}}$ and the $\widetilde{R_{ij}}$ that contains the derivatives of the conformal metric

\begin{align}
    \widetilde{R_{ij}} &= -\frac{1}{2}\widetilde{\gamma}^{lm}\partial_{l}\partial_{m}\widetilde{\gamma}_{ij} + \widetilde{\gamma}_{k(i}\partial_{j)}\widetilde{\Gamma}_{(ij)k} + \widetilde{\gamma}^{lm}\big[2\widetilde{\Gamma}^{k}_{l(i}\widetilde{\Gamma}_{j)km} + \widetilde{\Gamma}^{k}_{im}\widetilde{\Gamma}_{kjl}\big] \\
    \widetilde{R_{ij}}^{\phi} &= \frac{1}{\phi^2}\big[\phi\big(\widetilde{\nabla}_{i}\widetilde{\nabla}_{j}\phi + \widetilde{\gamma}_{ij}\widetilde{\nabla}^{l}\phi\widetilde{\nabla}_{l}\phi\big) - 2\widetilde{\gamma}_{ij}\widetilde{\nabla}^{l}\phi\widetilde{\nabla}_{l}\phi\big]
\end{align}

And as one sees, the ecolution of $Z_i$ is now included in $\hat{\Gamma}$. 
\todo{understand the conformal stuff and add some steps to show how the ccz4 was made}

\subsubsection{Gauge conditions}

During the discussion of the original ADM system, the choice of the lapse function, \textit{i.e} slicing condition, and shift vector \textit{i.e} spatial gauge condition was left open. The right choice however, is crutual for the stable evolution and in itself presents a broad and rapidly evolving subject. Here we are going to discuss only the gauge that is relevant for our work. 

\paragraph{Slicing condition} One of the widely used conditions is so called 'maximal slicing' that sets $K=0$, which in turn results in the equation

\begin{equation}
    D^{i}D_{i}\alpha = \alpha\big[K_{ij}K^{ij} + 4\pi(e+S)\big].
\end{equation}

This conditions has an advantage of being \textit{singularity-avoiding}. For example, it was shown that in the case of Schwarzschild black hole, the $\alpha$ goes to zero at a finite distance from singularity \cite{Geyer:1995}. However implementation of this condition in from of a elliptic equations is computationally expensive.   \\
A class of slicing conditions in form of hyperbolic equations that are more favorable from numerical standpoint and that reproduces the desired behavior of the maximal scicing was proposed in \cite{Bona:1994dr}. It is read 

\begin{equation}
(\partial_t - \beta^i\partial_i)\alpha = \alpha^2 f(\alpha)K
\label{eq:theory:gauge_onepluslog}
\end{equation}

which in CCZ4 reads 

\begin{equation}
    (\partial_t - \beta^i \partial_i )\alpha = \alpha^2 f(\alpha)(K-2\Theta)
\end{equation}

where $f(\alpha)$ is a positive function. For many numerical applications, including those that are discussed in this work, the "1 + log" slicing is adopted, the $\beta_i=0$. Then, integrating equation (\ref{eq:theory:gauge_onepluslog}) yields 

\begin{equation}
    \alpha = 1 + \log\gamma
\end{equation}

This condition is numerically more favorable and as $f\rightarrow\infty$ in the vicinity of a singularity, allows to treat black holes well like maximal slicing \cite{Baumgarte:2002jm}.

\todo{add/modify some text.}

\paragraph{Spatial gauge conditions}

The requirements for the gauge are similar as in the case of the $\alpha$, namely hyperbolicity and minimization of numerical distortions for more stable evolution.  

One of the widely used shift conditions is so called \textit{Gamma driver} condition \cite{Alcubierre:2002kk}, 

\begin{align}
    \partial_t\beta^i &= \frac{3}{4}\alpha B^i, \\
    \partial_t B^i &= \partial_t\widetilde{\Gamma}^i - \eta B^i,
\end{align}

where $\eta$ is a dumping coefficient. \\

This gauge condition tries to decrease the coordinate stretching that occur in the vicinity of a singularity. It was shown to be effective in numerical applications, in particular for a single moving black hole. However it has a zero-speed mode, that can amplify the numerical errors and destabilize the system \cite{vanMeter:2006vi}.

A modified \textit{Gamma driver}, gauge that does not have zero or small speed modes:

\begin{align}
    (\partial_t - \beta^j\partial_j)\beta^i &= \frac{3}{4}B^i \\
    (\partial_t - \beta^j\partial_j)B^i &= (\partial_t - \beta^j\partial_j)\widetilde{\Gamma}^i-\eta\beta^i,
\end{align}

was proposed by \cite{vanMeter:2006vi} and was applied to study binary black holes by \cite{Campanelli:2005dd}.

\subsection{The Equations of General-Relativistic Hydrodynamics}

In this section we discuss the equations of general relativistic hydrodynamics. We consider the fluid on a Lorentzian manifold and how its flow affects the spacetime. \\ 

The topics that we are going to touch are:
\begin{itemize}
    \item fluid kinematics,
    \item equations of motion for perfect fluids (assuming that there is no thermal conduction or viscosity)
    \item the “Valencia formulation” of the hydrodynamic equations.
\end{itemize}

We note that the following description is very brief and is based on the following works: \cite{Misner:1973},\cite{Schutz:2009a},\cite{Gourgoulhon:2006bn},\cite{Andersson:2006nr},\cite{Rezzolla:2013} to which we refer the reader for more details.  

\subsubsection{Kinematics of a Relativistic Fluid}

In Newtonian physics, a fluid is an "entity" whose dynamics is described by flows of quantities such as energy density, mass, momentum density. However, in general and special relativity, the these quantities are not well defined and depend on the observer. In other words, different observers perceive the the same fluid being in different thermodynamic state. Hence, a description of the fluid dynamics in relativity requires a new formulation, a formulation in which a fluid is not represented by a scalar and vector fields, that are observer-dependent, but implicitly by a "flow" in spacetime. These are \textit{flux-conservative formulations} of hydrodynamics.

Consider the classical mass density, a scalar $\rho$, usually defined as total umber of particles $N$ of rest-mass $m$ in the volume $V$. Then, the total mass is given by

\begin{equation}
    \int_V \rho \text{d}^3x = m\int_V n \text{d}^3 x = mN.
\end{equation}

However, while the number of particles $N$ would be the same regardless of the observer, the $\text{d}^3x$ would be measured differently by observers moving in relation to each other. Hence, the $n$ would differ. One of the solutions is to chose a frame of reference, say comoving with the fluid and define the $\rho$ there. However, this would hinder our ability to generalize to other reference frames.\\ A better soution is to construct a \textit{covariant description in terms of invariant quantities}. 
 
We start by defining the flow of the fluid density in space-time, the 3 pseudo-form $\boldsymbol{\rho}$ that on any three dimensional submanifold describes the flow of mass transverse to the submanifold as

\begin{equation}
    \int_{\Sigma} \boldsymbol{\rho},
\end{equation}

where $\Sigma$ be a spacelike hypersurface,  $\vec{n}$ -- the future-oriented normal vector. This is the density measured by an observer with 4-velocity $\vec{n}$. 

To define a mass flow measured by an Eulerian observer across any spacelike surface $\Omega\subset\Sigma$, we need to construct a two-form $\boldsymbol{\rho}(\vec{n}, \cdot, \cdot)$ given by the interior product between the 3 pseudo-form $\boldsymbol{\rho}$ and $\vec{n}$. Then the mass flow is 

\begin{equation}
\int_{\Omega} i_{\vec{n}}\boldsymbol{\rho}.
\end{equation}

The conservation of the number of particles of the fluid is expressed by the vanishing exterior product of the density form, i.e. $\text{d}\boldsymbol{\rho}=0$, or in an integral form 

\begin{equation}
\int_{\partial\Omega} \boldsymbol{\rho} = \int_{\Omega}\text{d}\boldsymbol{\rho} = 0,
\end{equation}

that reads as the following: the net flow across any sufficiently regular surface $\partial\Omega$ enclosing a four-dimensional open set $\Omega\subset\mathcal{M}$ is zero.

Next we define a flux. First, let us reintroduce the volume pseudo-form

\begin{equation}
\text{Vol}_x ^4 = \sqrt{-g}dx^0 \wedge dx^1 \wedge dx^2 \wedge dx^3,
\end{equation}

where $g$ is the determinant of the spacetime metric. \\
On a on the submanifold $\Sigma$, the intrinsic volume then would be defined as 

\begin{equation}
\text{Vol}_x ^3 = i_{\vec{n}} \text{Vol}_x ^4.
\end{equation}

A flux of a vector field can be described by a three-form, for which on a pseudo-Riemannian manifold there exist a vector field associated with it.

A vector field associated with density is called \textit{rest-mass density four-vector} and is denoted by $\vec{j}$.

It is constructed from the one-form by rasing indexes, $\vec{j} = {^{\#}\underline{j}}$. The one-form $\underline{j}$ is obtained as $\underline{j}\star\boldsymbol{\rho}$, where $\star$ is the Hodge dual operator (see \textit{e.g.,} \cite{Frankel:1982dva}). 

Then if the $\boldsymbol{\rho} = i_{\vec{j}}\text{Vol}_x ^4$ the flux of $\vec{j}$ can be shown as 

\begin{equation}
\int_{\Sigma} \boldsymbol{\rho} = - \int_{\Sigma}\vec{j}\cdot\vec{n}\text{Vol}_x ^3,
\end{equation}

where $\vec{n}$ is the future-oriented unit-timelike normal to $\Sigma$.


\textcolor{gray}{
[Direct copy... maybe not needed] More generally the flux associated with a flow defined by a vector field, $\vec{X}$, across a hypersurface, $\Sigma$, transverse to it and with normal $\vec{\nu}$ (with appropriate sign depending on the signature of the metric and on $\Sigma$), is given 
\begin{equation}
\int_{\Sigma} \star\underline{X} = \int_{\Sigma}i_{\vec{X}}\text{Vold}^n = \int_{\sigma}i_{\vec{X}}\big[\underline{\nu}\wedge\text{Vol}^{n-1}\big] = \int_{\Sigma}\vec{X}\cdot\vec{\nu}\text{Vol}^{n-1}
\label{eq:theory:flux_of_flow}
\end{equation}
}
\textcolor{red}{this piece is used in Liuille theorem though}

We note that $\vec{j}$ is time like (or null). It is given by the the flux of particles across any future-oriented spacelike hypersurface is positive (or zero). If $\vec{j}$ is timelike, there exists a unique decomposition 

\begin{equation}
\vec{j} = \rho \vec{u},
\label{eq:theory:defofjandu}
\end{equation}
where the scalar $\rho$ can be seen as density in the comoving frame and unit-timelike vector $\vec{u}$ as a fluid four-velocity.\\

The divergence of vecotor $j$ then gives a familiar mass conservation expression

\begin{equation}
0 = \nabla_{\mu}j^{\mu} = \frac{1}{\sqrt{-g}}\partial_{\mu}[\sqrt{-g}\rho u^{\mu}].
\label{eq:theory:nablamu_jmu}
\end{equation}

\textcolor{gray}{Similarly energy and momentum of a fluid can be defined, using the Cartan formalism... but this is a PAIN! and is done to show that div(T)=0 is not really energy/momentum conservation...}

Next, let us introduce the mixed tensor $\boldsymbol{T}$. Since the three-forms are equivalent to vectors, we can define a flow of the $\nu$ momentum across the volume element orthogonal to $dx^{\mu}$ as 

\begin{equation}
{T^{\mu}}_{\nu}=\boldsymbol{T}(dx^{\mu},\partial_{\nu}).
\end{equation}

${T^{\mu}}_{\nu}$ is the stress energy tensor that was already introduced earlier \ref{eq:theory:action1}. 

Note, that if the Einstein equation are satisfied the Bianchi identities dictate that the $\nabla_{\mu}{T^{\mu}}_{\nu}$ must vanish as

\begin{equation}
\nabla_{\mu}{T^{\mu}}_{\nu} = 0= \frac{1}{\sqrt{-g}}\partial_{\mu}(\sqrt{-g}{T^{\mu}}_{\nu}) - {\Gamma^{\alpha}}_{\mu\nu}{T^{\mu}}_{\alpha}.
\label{eq:theory:nablamu_tmunu}
\end{equation}

However, this statement does not imply the conservation of the energy and momentum of the fluid in a general sense. The conservation of the $\nu$-momentum requires $\vec{\partial}_{\nu}$ to be a Killing vector.


\todo{define somewhere an eulerian observer}

\subsubsection{Dynamics of a Relativistic Fluid}

In the previous subsection we have introduced the fluid kinematic, and defined the important quantities such as mass, energy and momentum and their "conservation" in \ref{eq:theory:nablamu_jmu} and \ref{eq:theory:nablamu_tmunu}.

In this thesis we consider only the \textit{perfect fluid}, meaning that in the co-moving frame, there is not heat conduction and there is no viscosity. \todo{actually we do have a viscous part -- you have to add this...}. The former criterion implies that the fluid is in local thermodynamic equilibrium (LTE). The latter however requires more explanation. There is still no consensus on the correct mathematical formulation, especially with respect to the numerical applications, of the viscous and/or thermally conducting fluids in general-relativity (see e.g., \cite{Andersson:2006nr} and references therein). \textcolor{blue}{however in recent youers there have been some progress GRELS models and David's implementation I must add!}. \\

Consider a stress-energy tensor of a perfect fluid in the comoving frame with the fluid. To construct it, we return to the fluid's four velocity $\vec{u}$ from (\ref{eq:theory:defofjandu}). If $e_{i}$ is the basis vector, the scalar product $\vec{u}\cdot\vec{e}_i=0$ and $\vec{e_i}\cdot\vec{e}_k = \delta_{ik}$. then the orthonormal tetrad $\{\vec{u},\vec{e}\}$ is comoving with the fluid, and the $\{\underline{u},\underline{e}^i\}$ is the dual basis. \\
Tensor $\boldsymbol{T}$ is the stress-energy tensor with the following components: 

\begin{itemize}
    \item $\boldsymbol{T}(\underline{u}, \vec{u})$ energy-density in the rest-frame of the fluid, the scalar $e$
    \item $\boldsymbol{T}(\underline{u}, \vec{e}_i) = 0$ represent the energy flowing transverse to the four-velocity, which we set to $0$ in the absence of the heat-conduction.
    \item $\boldsymbol{T}(\underline{e}^i, \vec{e}_k) = 0$ represent the $k$ component of the force exchanged across the surface element orthogonal to $\underline{e}_i$.
\end{itemize}

Taking into account that the $\boldsymbol{T}$ must be invariant with respect to the rotations of the $\{\vec{e}_i\}$ and that the viscosity is not included, force exchange can be effectively desibed by a scalar $p$, that we call pressure as

\begin{equation}
    \boldsymbol{T}(\underline{e}^i,\vec{e}_k) = p {\delta^i}_k,
\end{equation}

Combining the aforementioned description of the components of $\boldsymbol{T}$ we get

\begin{equation}
\boldsymbol{T} = (e + p)\vec{u}\otimes \underline{u} + p\boldsymbol{\delta}.
\end{equation}

Defining the enthalpy of the fluid as $h = 1 + \epsilon = p/\rho$, where $\epsilon$ is the specific internal energy, we rewrite $\boldsymbol{T}$ as 

\begin{equation}
\boldsymbol{T} = \rho h \vec{u}\otimes\underline{u} + p\boldsymbol{\delta}
\label{eq:theory:stressenergytensor}
\end{equation}

In addition to the fluid kinematics (eqs. \ref{eq:theory:nablamu_jmu} and \ref{eq:theory:nablamu_tmunu}) and the description of motion (eq. \ref{eq:theory:stressenergytensor}), the relation between the pressure, internal energy and density is needed to fully describe the dynamics of the fluid. This relation is usually called the equation of state.

The commonly adopted equations (EoS) of state are the the ideal-gas, or gamma-law EoS $\rho = (\Gamma-1)\rho\epsilon$, where $\Gamma$ is the polytropic index of the gas, the polytropic EoS $p = K\rho^{\Gamma}$ and the microphysical equation of state \todo{that you need to discuss more..., as we use only it.}

Combined with an EoS, equations \ref{eq:theory:adm}, \ref{eq:theory:nablamu_jmu}, \ref{eq:theory:nablamu_tmunu} and \ref{eq:theory:stressenergytensor} form a hyperbolic
system of equations that can be evolved, once initial data is prescribed. The complete evolution of spacetime and the dynamics of the matter requires initial data to be set on the Cauchy surface.

\subsubsection{Conservative Formulations}

In the pioneering works of May and White \cite{May:1966} and Wilson \cite{Wilson:1972} the equations of general relativistic hydrodynamics were solved using the finite-difference (FD) schemes after casting them a from of non-linear advection-like equations. To avoid excessive oscillations at shocks a combination of upwinding and artificial-viscosity methods was employed. This however led to severl limitations, such as difficulty with tunning the artificial viscosity to still allow shocks to develop, and the limit on a flows being only mildly relativistic \cite{Font:2008fka}. \\
A next big advancement in the numerical relativistic hydrodynamics was made after the non-conservative nature of the Wilson’s approach was pointed out \cite{Marti:1991wi} and the conservation formulation was developed. 

An important example of the conservation formulation that is adopted to $3 + 1$ formalism is the "Valencia formulation" \cite{Banyuls:1997} that can be represented as following

\begin{equation}
    \frac{\partial\boldsymbol{F}^{0}(\boldsymbol{u})}{\partial t} + \frac{\partial\boldsymbol{F}^{i}(\boldsymbol{u})}{\partial x^{i}} = \boldsymbol{S}(\boldsymbol{u})
    \label{eq:theory:valencia_formalism}
\end{equation}

where $u$ is a “vector” of \textit{primitive quantities}, such as the rest-mass density or the specific internal energy, $\boldsymbol{F}^0$ is a “vector” of \textit{conserved quantities} and $\boldsymbol{F}^i$ and $\boldsymbol{S}$ are their fluxes and sources respectively. \\

This formulation allowed to study ultra-relativistic flows and resolve shocks without spurious oscillations and without need for artificial viscosity.

It was shown to be especially well suited for use with numerical methods that take into account the conservation laws. These are the finite-volume (FV) FD high-resolution shock capturing (HRSC) methods, that will be discussed in Chapter \ref{chapter:num_methods} \\

Many recent advancements in numerical relativistic hydrodynamics and magnetohydrodynamics (MHD) have relied on these methods (\textit{e.g.,} \cite{Giacomazzo:2010bx} [274]\cite{Rezzolla:2011da} and references therein \todo{add recolla/bernuzzi/radice/shibata}).

There are other conservative formulations and methods (see \textit{e.g.,} \cite{Papadopoulos:1999kt}). However, we will limit our focus to the "Valencia formulation". \\

To begin we split the four-velocity $\vec{u}$ into the component parallel to the normal vector $\vec{n}$ and a purely spatial component as

\begin{equation}
    \vec{u} = (-\vec{u} \cdot \vec{n})(\vec{n} + \vec{\upsilon}),
\end{equation}

where naturally the Lorentz factor, measured by the Eulerian observer $W = (-\vec{u}\cdot\vec{n})$ emerges, and the $\upsilon$ is the fluid three-velocity measured by the Eulerian observer, 

\begin{equation}
    \vec{\upsilon} = \frac{\vec{u}}{W} -\vec{n},
\end{equation}

components of which are

\begin{equation}
    \upsilon^i = \frac{u^i}{W}+ \frac{\beta^i}{\alpha}, \hspace{10mm} \upsilon_i= \frac{u_{i}}{W}.
\end{equation}

Divergence of the rest-mass density four-vector $j$, (\ref{eq:theory:nablamu_jmu}) can easily be cast as 

\begin{eqnarray}
    0 = \nabla_{\mu}j^{\mu} = \frac{1}{\sqrt{-g}}\partial_{t}[\sqrt{\gamma}\rho W] + \frac{1}{\sqrt{-g}}\partial_{i}[\sqrt{\gamma}\rho(\alpha \upsilon^{i} - \beta^{i})]
\end{eqnarray}

where $D=\rho W = -\vec{j}\cdot \vec{n}$ is the conserved density.

To write the energy and momentum equations we note that for any vector field $\vec{p} $ \cite{Rezzolla:2013}, 

\begin{equation}
    \nabla_{\mu}[{T^{\mu}}_{\nu}p^{\nu}].
\end{equation}

To obtain the Valencia formulation we set $\vec{p}$ to have zeroth component $-\vec{n}$ and spatial components $\vec{\partial}_i$. Then the

\begin{itemize}
    \item ${T^0}_{\nu}p^{\nu}$ represent the conserved quantities,
    \item ${T^i}_{\nu}p^{\nu}$ are associated fluxes,
    \item ${T^{\mu}}_{\nu}p^{\nu}$ are sources
\end{itemize}

with the former being 

\begin{equation}
    S_{i} = \alpha {T^0}_{\nu}(\partial_i)^{\nu}=-\boldsymbol{T}(\vec{n},\vec{\partial}_i), \hspace{10mm} E = -\alpha{T^0}_{\nu}n^{\nu} = \boldsymbol{T}(\vec{n},\vec{n})
\end{equation}

for numerical reasons we will replace the total internal energy density $E$ with $\tau = E-D$, where $D$ is the rest mass density. This is done to avid errors emerging due to $E$ being much smaller then $D$. 

Now we can combine the obtained expressions for the conserved quantities, associated fluxes and sources with eq. (\ref{eq:theory:valencia_formalism}) and obtain

\begin{equation}
    \frac{1}{\sqrt{-g}}\Big[\frac{\partial\sqrt{\gamma}\boldsymbol{F}^{0}(\boldsymbol{u})}{\partial t} + \frac{\partial\sqrt{-g}\boldsymbol{F}^{i}(\boldsymbol{u})}{\partial x^i}\Big] = \boldsymbol{S}(\boldsymbol{u}),
    \label{eq:theory:grhdeq_thc} % used for THC section Code
\end{equation}

where primitive quantities being

\begin{equation}
    \boldsymbol{u} = [\rho,\: \upsilon_i,\: \epsilon],
\end{equation}

conserved quantities: 

\begin{equation}
    \boldsymbol{F}^0(\boldsymbol{u}) = [D,\: S_j,\: \tau] = [\rho W,\: \rho h W^2 \upsilon_j,\: \rho h W^2 - p - \rho W],
\end{equation}

associated fluxes

\begin{equation}
    \boldsymbol{F}^i(\boldsymbol{u})=\Bigg[D\Big(\upsilon^{i}-\frac{\beta^i}{\alpha}\Big),\: S_{j}\Big(\upsilon^{i}-\frac{\beta^i}{\alpha}\Big)+p{\delta^i}_j ,\: \tau\Big(\upsilon^{i}-\frac{\beta^i}{\alpha}+p\upsilon^i\Big)\Bigg]
\end{equation}

and sources 

\begin{equation}
    \boldsymbol{S}(\boldsymbol{u}) = \Bigg[0,\: T^{\mu\nu}\Big(\frac{\partial g_{\nu j}}{\partial x^{\mu}} - \Gamma^{\delta}_{\nu\mu}g_{\delta j}\Big),\: \alpha\Big(T^{\mu 0}\frac{\partial\log\alpha}{\partial x^{\mu}}-T^{\mu\nu}\Gamma^{0}_{\nu\mu}\Big)\Bigg]^T
\end{equation}

The from of the obtained general relativistic hydrodynamics equations resemble the one of the Newtonian gas dynamics. If the latter is adopted for numerical solutions. \\
There are however several complications. In particular there is no explicit inverse relation between the primitive quantities and the conserved ones. Thus one has to resort to the root-finding algorithms to reconstruct them (More on this in later chapters). In addition, it was pointed out that the $W$ cpuples the equation for the momenta in different direction \cite{Pons:2000,Rezzolla:2002ra,Rezzolla:2002cc,Aloy:2006rd}. This leads to the fact that the dynamics of the shock wave can be affected by the non-zero tangential velocity. Hence, the increased complexity if he problem of GR hydrodynamics \cite{Mignone:2005ns,Zhang:2005qy}.


\subsection{The General-Relativistic Boltzmann Equation}

In special relativity the Boltzmann equation was expressed by Synge \cite{Synge:1957}. Later Chernikov \cite{Chernikov:1962} and Tauber and Weinberg \cite{Tauber:1961} proposed its extension to the general relativity. \\
The list of applications of the Boltzmann equation was limited to the relativistic gas at first \cite{Israel:1963}. Later the list was supplemented by transient relativistic thermodynamics \cite{Israel:1979wp}, radiative transfer \cite{Lindquist:1966}, core-collapse supernovae \cite{Bruenn:1985} and others (see \textit{e.g.}, \cite{Cercignani:2002} and references therein). \\

Different formulations of the general relativistic Boltzmann equation exists in the literature. Lindquist \cite{Lindquist:1966} and Ehlers \cite{Ehlers:1971} proposed a geometrical interpretation. Later, a formulation based on Riemannian structure of tangent bundles was proposed by Sasaki \cite{Sasaki:1958,Sasaki:1962}. In addition, Debbasch and van Leuuwen \cite{Debbasch:2009a,Debbasch:2009b} recently provided a detailed derivation, albeit strongly focused on the algebraic aspects while eluding simple geometrical interpretation. \\
Here we recall the detailed derivation of the general relativistic Boltzmann equation, using modern differential geometry notation by Radice. 

\textcolor{red}{This.Is.Tough. Pure math. Copied from David + his sources.}

\subsubsection{The geometry of the tangent bundle}

Let the $\mathcal{M}$ be $4$ dimensional differential manifold such that $(\mathcal{M},\: g_{\alpha\beta})$ form the $C^2$ spacetime. The set of tangent vectors of $\mathcal{M}$ constitutes \textit{tangent bundle} of $\mathcal{M}$, the we denote as $T\mathcal{M}$. The set of all unit vectors of $\mathcal{M}$ constitute the \textit{subbundle} of $T\mathcal{M}$. \\
\textcolor{gray}{incompressible vector field}\\
\textit{Every Killing vector field of $\mathcal{M}$ is in incompressible vector field}

\paragraph{Extended transformation and extended tensors}

Let the $T\mathcal{M}$ be the set of all the tangent vectors of $\mathcal{M}$. The $T\mathcal{M}$ has a natural topology, bundle structure with $\mathcal{M}$ and the base - linear vector space $E^i$. We call $T\mathcal{M}$ the \textit{tangent bundle} of $\mathcal{M}$. Natural projection, or a projection map $\pi:\: T\mathcal{M}\rightarrow\mathcal{M}$.  \\

Let $U$ be a coordinate neighborhood, or a coordinate patch of $\mathcal{M}$ with $n$ variables $x^{\alpha}$ as coordinates. Then, every tangent vector of $\mathcal{M}$ at a point $p\in U$ with $2n$ variables $(x^i,\upsilon^{\alpha})$. Here $x^{\alpha}$ are coordinates of $p$ with respect to the coordinate patch ${x^{\alpha}}$ and $\upsilon^{\alpha}$ are components of a tangent vector in the natural frame that constitutes by the vectors $\partial/\partial x^4$ at $q$. Thus, the vector $\vec{p}$ at $q$ can be written as:

\begin{equation}
    \vec{p} = p^{\alpha}\frac{\partial}{\partial^{\alpha}}
\end{equation}

and its dual as 

\begin{equation}
    \underline{p} = p_{\alpha}dx^{\alpha}:=g_{\alpha\beta}p^{\beta}dx^{\alpha}
\end{equation}

In addition we introduce a coordiante patch $TU$, $\{z^A\}$, where $A$ runs from $0$ to $7$ of $T\mathcal{M}$ as 

\begin{equation}
    z^{\alpha} = z^{\alpha}, \hspace{10mm} z^{\alpha+4} = p^{\alpha}.
\end{equation}

Now, let the $U(x^{\alpha})$ and $\hat{U}(\hat{x}^{\alpha})$ be the two coordinate patches of $\mathcal{M}$ such that $U\cap\hat{U}$ is not empty. Then the intersection of the coordinate patches is also not empty. 
for every coordinate transformation of $\mathcal{M}$, there is a corresponding matrix $\frac{\partial \hat{x}^{\alpha}}{\partial x^{\beta}}$.
The coordinate transformation is then

\begin{equation}
    \hat{x}^{\mu} = \hat{x}^{\mu}(x), \hspace{5mm} \hat{p}^{\mu} = \frac{\partial\hat{x}^{\mu}}{\partial x^{\nu}}p^{\nu}
\end{equation}

which denotes the extended transforation of the $\hat{x}^{\mu} = \hat{x}^{\mu}(x)$. \\


The corresponding Jacobian matrix is 
\renewcommand\arraystretch{1.6} %% it stretches the matrix
\begin{equation}
\frac{\partial\hat{z}^A}{\partial z^B} = 
    \begin{pmatrix}
    \frac{\partial\hat{x}^{\alpha}}{\partial x^{\beta}} & 0 \\
    \frac{\partial^2\hat{x}^{\alpha}}{\partial x^{\beta} \partial x^{\gamma}}p^{\gamma} & \frac{\partial\hat{x}^{\alpha}}{\partial x^{\beta}} 
    \end{pmatrix}
\end{equation}
\renewcommand\arraystretch{1.0}




\paragraph{Vectors on $T\mathcal{M}$}

As we will need to introduce connections on a tangent bundle, here we discuss the double tangent bundle, ot a second tangent bundle. Since $T\mathcal{M}$ is a vector bundle on its own right, its tangent bundle has the secondary vector bundle structure $TT\mathcal{M}$. Let point $b\in TU$ and $T_b T\mathcal{M}$ be the tangent space to $T\mathcal{M}$ at $b$. \\
Given a vector $\partial/\partial x^{\alpha}$ at a point $b$, it can be "pushed forward" to the point on the $TT\mathcal{M}$ by means of so called \textit{differential of} $\pi$, whitten as $\pi_*$ \cite{Frankel:2002}.
On a natural basis the push-forward acts as 

\begin{equation}
    \pi_*\Big[\frac{\partial}{\partial x^{\alpha}}\Big] = \frac{\partial}{\partial x^{\alpha}}, \hspace{5mm} \pi_* \Big[\frac{\partial}{\partial p^{\alpha}}\Big] = 0,
\end{equation}

and the pull back 

\begin{equation}
    \pi^* {\text d} x^{\alpha} = {\text d} x^{\alpha}.
\end{equation}

Consider a vector field $\vec{X} \ in TT\mathcal{M}$  in a vicinity of the point $b$, which is associated with the point $q$ of $\mathcal{M}$ and vector $\vec{x}\in T_{q}\mathcal{M}$. Let $b{\lambda}$ be the flow of $b$ generated by $\vec{X}$. The $b(\lambda)$ is associate with $q(\lambda)$, the one parameter family of points of $\mathcal{M}$. The $b(\lambda)$ is also associated with $\vec{x}(\lambda)$ the one parameter family of vectors on $T\mathcal{M}$.  \\

The vector field $\vec{X}$ is called \textit{vertical} if the $q(\lambda)\in\mathcal{M}$ are constant along the flow. Similarly, the vector field $\vec{X}$ is called \textit{horizontal} if $\vec{x}(\lambda)\in T_p \mathcal{M}$ is "constant" along the flow, meaning that $\vec{x}(\lambda)$ is just $\vec{x}$ that is \textit{parallel transported} to $q(\lambda)$. \\
As there is no unique way to perform a parallel transport, the linear connection $\nabla$ on $\mathcal{M}$ has to be chosen. This choice is akin choosing two vector spaces $\mathcal{O}_b$ and $\mathcal{V}_b$ of the horizontal and vectical vectors respectively at each point $b$ that the direct sum of these spaces yields

\begin{equation}
    \mathcal{O}_b\oplus \mathcal{V}_p = T_b T\mathcal{M}.
\end{equation}

Having the connection allows to prescribe a manner of lifting curves from the base manifold $T\mathcal{M}$ into the $T_b T\mathcal{M}$ \textcolor{red}{I need to fix this and understand}. A lift is the unique horizontal vector $\vec{X}\in T_bT\mathcal{M}$ whose projection is a vector $\vec{x}\in T_q\mathcal{M}$.\\ 
\textcolor{red}{fill it}
Let us now define a \textit{connection vector basis} adopted to the aforementioned split of $T_b T\mathcal{M}$ $\{\text{D}/\partial x^A \}:=\{\text{D}/\partial x^{\alpha}, \partial/\partial p^{\alpha} \}$ where 

\textcolor{red}{I did not find where this is derived from... difficult}

\begin{equation}
    \frac{\text{D}}{\partial x^{\alpha}}{\partial x^{\alpha}} := \frac{\partial}{\partial x^{\alpha}} - {\Gamma^{\beta}}_{\alpha\gamma}p^{\gamma}\frac{\partial}{\partial p^{\beta}}.
\end{equation}

Similarly a connection can be build for differential forms. Using the pull-back $\pi^*$ the dual basis $\{ \text{D}z^{A} \}:=\{\text{d}x^{\alpha}, \text{D}p^{\alpha}\}$ that satisfies 

\begin{equation}
    \text{D} = \text{d} p ^{\alpha} + {Gamma^{\alpha}}_{\beta\gamma}p^{\gamma}\text{d}x^{\beta}.
\end{equation}

\paragraph{Metric on $T\mathcal{M}$}

Note that 

\begin{equation}
    \frac{\partial^2 \hat{x}^{\mu}}{\partial x^{\nu}\partial x^{\lambda}}p^{\lambda} = {\hat{\Gamma}^{\mu}}_{\delta\gamma}p^{\lambda}\frac{\partial\hat{x}^{\delta}}{\partial x^{\nu}}.
\end{equation}

Let us assume that for any point $b\in T\mathcal{M}$ there exist an open set $TU$, such that $b\in TU$ with a coordinate system on $TU$ that satisfies

\begin{equation}
    G_{AB} = (\boldsymbol{\eta}\otimes\boldsymbol{\eta})_{AB},
\end{equation}

where $\boldsymbol{\eta} = \text{diag}(-1, 1, 1, 1)$. \\
Let the $\hat{x}^A$ denote the generic coordinate system on $TU$. Then the metric in this coordinate system can be expressed as

\begin{align}
    \hat{G}_{\mu\nu} &= \frac{\partial \hat{x}^{\alpha}}{\partial x^{\mu}}\frac{\partial \hat{x}^{\beta}}{\partial x^{\nu}}\eta_{\alpha\beta} + \frac{\partial \hat{x}^{\alpha}}{\partial x^{\mu}}{\hat{\Gamma}^{\gamma}}_{\:\:\:\alpha\lambda}p^{\lambda}\frac{\partial \hat{x}^{\beta}}{\partial x^{\nu}}{\hat{\Gamma}^{\delta}}_{\:\:\:\beta\xi}p^{\xi}\eta_{\gamma\delta}; \\
    \hat{G}_{\mu\: \nu+4} &= \frac{\partial \hat{x}^{\alpha}}{\partial x^{\mu}}\frac{\partial \hat{x}^{\gamma}}{\partial x^{\nu}}{\hat{\Gamma}^{\beta}}_{\:\:\:\gamma\lambda}p^{\lambda}\eta_{\alpha\beta}; \\
    \hat{G}_{\mu+4 \: \nu+4} &= \frac{\partial \hat{x}^{\alpha}}{\partial x^{\mu}}\frac{\partial \hat{x}^{\beta}}{\partial x^{\nu}} \eta_{\alpha\beta}
\end{align}

and the line element 

\begin{align}
    dS^2 &= \hat{G}_{AB}d\hat{z}^A d\hat{z}^B = \hat{g}_{\mu\nu}\text{d}\hat{x}^{\mu}\text{d}\hat{x}^{\nu} + \hat{g}_{\mu\nu}[\text{d}p^{\mu} + {\hat{\Gamma}^{\mu}}_{\:\:\:\alpha\beta}p^{\beta}\text{d}x^{\alpha}] [\text{d}p^{\nu} + {\hat{\Gamma}^{\nu}}_{\:\:\:\alpha\beta}p^{\beta}\text{d}x^{\alpha}] \\
    &= \hat{g}_{\mu\nu}\text{d}\hat{x}^{\mu}\text{d}\hat{x}^{\nu} + \hat{g}_{\mu\nu}\text{D}\hat{x}^{\mu}\text{D}\hat{x}^{\nu}
\end{align}

It is possible to show that the determinant $|\text{det}\boldsymbol{G}| = g^{2}$ as the transformation from the natural frame to the connection frame is unimodular \cite{Lindquist:1966}. Thus the volume pseudo-form on $T\mathcal{M}$ is in the coordiante patch $TU$

\begin{align}
    \text{Vol}^8 &:= -g \text{d}x^{0} \wedge \text{d}x^{1} \wedge ... \wedge \text{d}p^{3} := - g\text{d}^{4}x \text{d}^{4}p, \\
    &:= -g \text{d}x^{0} \wedge \text{d}x^{0} \wedge ... \wedge \text{D}p^{3} :=-g \text{d}^{4}x\text{D}^4 p
\end{align}

\textcolor{red}{I kinda gave up here and just copied.}


\paragraph{the Liuville theorem}

Let us start by introducing a \textit{cotangent bundle}. Let $\mathcal{M}$ be a differentiable manifold. Similarly to the construction of the tangent bundle, we can make a set of covectors on a given manifold into a vector bundle over $\mathcal{M}$, denoted $T^*\mathcal{M}$ and called \textit{cotangent bundle} of $\mathcal{M}$.  Similarly we can define a contangent bundle of a tangent one $T^*T\mathcal{M}$. The contangent bundle $T^*\mathcal{M}$ is the vector bundle dual to the tangent bundle $T\mathcal{M}$. 

Let us start by defining \textit{Poincar\'e} 1-form on $T\mathcal{M}$, $\underline{\lambda}\in T^* T\mathcal{M}$. Consider point $q$ on a manifold $\mathcal{M}$ and a point $A$ associated with $q$ on a tangent bundle $T\mathcal{M}$. Let there be a 1-form $\underline{\alpha}\in T^* _q\mathcal{M}$. The $\underline{\lambda}$ and $\underline{\alpha}$ are uniquely connected $\underline{\lambda} = \pi^* \alpha$, and the former is called the \textit{Poincar\'e} 1-form. In local coordinate patch, $TU$ it is expressed as

\begin{equation}
    \underline{\lambda} = p_{\alpha} \text{d}x^{\alpha}.
\end{equation}

the associated vector is 

\begin{equation}
    \vec{\lambda} = p^{\alpha} \frac{\text{D}}{\partial x^{\alpha}} = p^{\alpha}\frac{\partial}{\partial x^{\alpha}} - p^{\alpha}{\Gamma^{\beta}}_{\alpha\gamma}p^{\gamma}\frac{\partial}{\partial p^{\beta}},
\end{equation}

is called the $\textit{geodesic flow field}$. \\
This flow represents a phase-space flow of particles moving along geodesics. \\

Consider a mass shell, that at a point $q\in U$can be defined as a set:\\
\textcolor{red}{remider: I have no idea how is this possible...}

\begin{equation}
    \mathcal{S}_m = \big\{ p^{\alpha}\in T_q\mathcal{M}: p_{\mu}p^{\mu}+m^2 =:f(p) = 0 \big\}.
\end{equation}

The normal to the mass-shell is 

\begin{align}
    \text{if } m &\neq 0 \hspace{5mm} \underline{\pi}:=\frac{q}{2m}\text{d}f, \hspace{5mm} \text{d}f = \frac{\partial f}{\partial x^{\mu}} + \frac{\partial f}{\partial p^{\mu}}\text{d}p^{\mu} = 2p_{\mu}\text{d}p^{\mu}, \\
    \text{if } m &= 0 \hspace{5mm} \underline{\pi}:=\frac{1}{2}\text{d}f
\end{align}

Next, we introduce a unique form $\underline{\nu}$ whose restriction on $T_q\mathcal{M}$ is equal to $\underline{\pi}$. 

\begin{align}
    \text{if } m &\neq 0 \hspace{5mm} \underline{\nu} = \frac{1}{m}p_{\alpha}\text{D}p^{\alpha} \\
    \text{if } m &= 0 \hspace{5mm} \underline{\nu} = p_{\alpha}\text{D}p^{\alpha}
\end{align} 

Note that $\underline{\nu} = 0$, meaning that the $\underline{\nu}$ is irrotational. It becomes clear if we re-express it as 

\begin{equation}
    \underline{\nu} = \frac{1}{2m}\frac{\text{D}f}{\partial p^{\alpha}}\text{D}p^{\alpha}
\end{equation} 

for massive particle case. For the mass-less the procedure is analogous. \\

It can be shown that $\underline{\lambda}$ is ncompressible \textit{i.e.,} $\text{d}^{\star}\underline{\lambda} =\star \text{d}\star\underline{\lambda} =0$. \\

In addition, both $\underline{\lambda}$ and $\underline{\nu}$ are harmonic forms as 

\begin{equation}
    \nabla\underline{\nu} = 0 , \hspace{5mm}
    \nabla\underline{\lambda} = [\text{dd}^{\star} + \text{d}^{\star}\text{d}]\underline{\lambda} = 0.
\end{equation}

Let us now consider the density of states in the phase space, of particles moving along tgeodeiscs with velocities on the mass shell. \\ 
In the previous section we introduced a flux of the vector field $\vec{X}$ across $\Sigma$ in \ref{eq:theory:flux_of_flow}, we define the following six-form

\begin{align}
    \boldsymbol{\omega} &= \star\big(\underline{\nu}\wedge\underline{\lambda}\big) = i_{\vec{\lambda}} i_{\vec{\nu}}\text{Vol}^8 \\
    &= i_{\vec{\lambda}}\Big[i_{\vec{\nu}}\big(\text{Vol}^{4}_{x}\wedge\text{Vol}^{4}_{p}\big)\Big] = i_{\vec{\lambda}} \big[\text{Vol}^{4}_{x}\wedge\text{Vol}^{3}_{p}\big],
\end{align}

where we used the definition of $\text{Vol}^8$. \\
The introduced four forms read,

\begin{align}
    \text{Vol}_x ^4 &:= \sqrt{-g} \text{d}x^{0} \wedge \text{d}x^{1} \wedge \text{d}x^{2} \wedge \text{d}x^{3}, \\
    \text{Vol}_p ^4 &:= \sqrt{-g} \text{D}p^{0} \wedge \text{D}p^{1} \wedge \text{D}p^{2} \wedge \text{D}p^{3}, \\
    \text{Vol}_p ^3 &:= i_{\vec{\nu}}\text{Vol}_p ^4,
\end{align}

where the four-forms are on the $TU$ and the latter three-form is on the mass shell $S_m$. \\

Consider coordinates adopted to the mass-shell, where $\underline{\nu} = (p_0/m)\text{D}p^0$ and $\underline{\nu} = p_0\text{D}p^0$ in the massive nad massless cases respectively, the three-form becomes

\begin{equation}
    \text{Vol}^3 _p =\frac{\sqrt{-g}}{-p_0}\text{D}p^1\wedge\text{D}p^2\wedge\text{D}p^3
\end{equation}

Now we have a three-form $\text{Vol}^3 _p$ and a four-form $\text{Vol}_x ^4$. In the context of the ADM foliation, we split spacetime manifold as $\mathcal{M}=\mathcal{R}\times\Sigma$, with $x^0 = \text{const}$ being constant hypersurfaces with normal $\underline{n} = - \alpha\text{d}x^0$ and $\alpha$ -- the lapse function. We can now simplify the $\boldsymbol{\omega}$, splitting $\text{Vol}_x ^3$ as 

\begin{align}
    \text{Vol}_x ^4 &= -\underline{n}\wedge\text{Vol}_x ^3 \hspace{5mm} \text{where,} \\
    \text{Vol}_x ^3 &= i_{\vec{n}}\text{Vol}_x ^4 = \sqrt{\gamma}\text{d}x^1\wedge\text{d}x^2\wedge\text{d}x^3
\end{align}

and the $\boldsymbol{\gamma}$ is the three-metric induced on the slices.  \\
The resulted coordinates, adapted to the mass shell and the spacetime foliation read

\begin{align}
    \boldsymbol{\omega} &=-(\vec{p}\cdot\vec{n})\frac{1}{-p_0}\sqrt{\gamma}\sqrt{-g}\text{d}x^1\wedge\text{d}x^2\wedge\text{d}x^3 \wedge\text{D}p^2\wedge\text{D}p^2\wedge\text{D}p^3 \\
    &= \frac{p^0}{-p_0}|g|\text{d}x^1 \wedge\text{d}x^2\wedge\text{d}x^3 \wedge\text{D}p^2\wedge\text{D}p^2\wedge\text{D}p^3
\end{align}

Now, consider a six-vector, $\delta_i x \delta_i p$ with $i\in\{1,2,3\}$. The $\delta_i x$ are tangent vectors to the slice $\Sigma$ and the $\delta_i p$ are tangent to mass shell $S_m$. \\
The action of $\boldsymbol{\omega}$ on the six-vectors $\delta_1 x$, $\delta_2 x$, $\delta_3 x$, $\delta_1 p$, $\delta_2 p$, $\delta_3 p$ yields

\begin{align}
    \boldsymbol{\omega}(\delta_1 x,...,\delta_3 p) =& \frac{p^0}{-p_0}|g|\big[\text{d}x^1\wedge\text{d}x^2\wedge\text{d}x^3\big](\delta_{1}x,\delta_{2}x,\delta_{3}x)\times \\
    & \hspace{10mm} \Big[\text{D}p^1\wedge\text{D}p^2\wedge\text{D}p^3\Big](\delta_1 p, \delta_2 p, \delta_3 p) \\
    & \hspace{2mm} -\frac{p^0}{-p_0}|g|\big[\text{d}x^1\wedge\text{d}x^2\wedge\text{d}x^3\big](\delta_{1}p,\delta_{2}p,\delta_{3}p)\times \\
    & \hspace{10mm} \Big[\text{D}p^1\wedge\text{D}p^2\wedge\text{D}p^3\Big](\delta_1 x, \delta_2 x, \delta_3 x) = \\
    =& \frac{p^0}{-p_0}|g|\big[\text{d}x^1\wedge\text{d}x^2\wedge\text{d}x^3\big](\delta_{1}x,\delta_{2}x,\delta_{3}x)\times \\
    & \hspace{10mm} \Big[\text{D}p^1\wedge\text{D}p^2\wedge\text{D}p^3\Big](\delta_1 p, \delta_2 p, \delta_3 p) = \\
    =& \frac{p^0}{-p_0}|g|\big[\text{d}x^1\wedge\text{d}x^2\wedge\text{d}x^3\big](\delta_{1}x,\delta_{2}x,\delta_{3}x)\times \\
    & \hspace{10mm} \Big[\text{d}p^1\wedge\text{d}p^2\wedge\text{d}p^3\Big](\delta_1 p, \delta_2 p, \delta_3 p), \\
\end{align}

where we used that $\text{d}x^i(\delta_j p)=0$ and the relation

\begin{equation}
    \text{D}p^{i}(\delta_j p) = \text{d}p^{i}(\delta_j p) - {\Gamma^i}_{\alpha\beta}p^{\alpha}\text{d}x^{\beta}(\delta_j p) = \text{d}p^i(\delta_j p)
\end{equation}

Thus, on the space-like hypersurface $\Sigma$ and on the mass shell we have

\begin{equation}
    \boldsymbol{\omega} = \frac{p^0}{-p_0}|g|\text{d}x^1\wedge\text{d}x^2\wedge\text{d}x^3\wedge\text{d}p^1\wedge\text{d}p^2\wedge\text{d}p^3 =: \boldsymbol{\Omega}
\end{equation}

The $\boldsymbol{Omega}$ can be split as 

\begin{align}
    \boldsymbol{\Omega} &= \boldsymbol{\Lambda} \wedge \boldsymbol{\Pi}, \hspace{5mm} \text{where} \\
    \boldsymbol{\Lambda} &= p^0 \sqrt{-g}\text{d}x^1\wedge\text{d}x^2\wedge\text{d}x^3 \\
    \boldsymbol{\Pi} &=  \frac{1}{-p_0}\text{d}p^1\wedge\text{d}p^2\wedge\text{d}p^3
\end{align}

The defined forms $\boldsymbol{\Lambda}$ and $\boldsymbol{\Pi}$ can be written in a coordinate-independent way at any point $q\in\mathcal{M}$ as 

\begin{equation}
    \boldsymbol{\Lambda} = \star_{\mathcal{M}}\underline{\lambda}, \hspace{5mm} \boldsymbol{\Pi} = \star_{T_q\mathcal{M}}\underline{\pi}
\end{equation}

and this are intrinsic forms in $T\mathcal{M}$. In addition, the $\boldsymbol{\Lambda}$ and $\boldsymbol{\Pi}$ are the proper geodesics flux
volume form on $\Sigma\in\mathcal{M}$ and mass shell $S_m\in T_q\mathcal{M}$ at a point $q\in U$ respectively. \\

Let us now consider the geodesic flow $\vec{\lambda}$. It generates a "tube" in a phase space, that we limit with $S_1$ and $S_2$ sections. Then the flux of points in phase space associated with geodesic flow is $\int_{S}\boldsymbol{\omega}$. It is possible to show that the flux satisfies

\begin{equation}
    \int_{S_1}\boldsymbol{\omega} = \int_{S_2}\boldsymbol{\omega}
    \label{eq:theory:liuville}
\end{equation}

which is the \textit{Liouville’s Theorem} in the relativistic case. \\

To see taht this is indeed the case, consider the exterior differential of $\boldsymbol{\omega}$

\begin{equation}
    \star\text{d}\omega = \text{d}^{\star}(\underline{\nu}\wedge\underline{\lambda}) = d^{\star}\underline{\nu}\wedge\underline{\lambda} + \underline{\nu}\wedge\text{d}^{\star}\underline{\lambda}.
\end{equation}

The $\text{d}^{\star}=\text{const}=k$ as $\text{dd}^{\star}\underline{\nu}=0$. In addition, the $\text{d}^{\star}\underline{\lambda}=0$. This allow us to write 

\begin{equation}
    \text{d}\omega = -k(\star\lambda).
\end{equation}

We note the $\star\underline{\lambda}$ is the volume form of the hypersurfaces orthogonal to $\vec{\lambda}$. Hence, the $\star\underline{\lambda}[\vec{\lambda},...]=0$ along the "tube" in phase space. 

\begin{equation}
    \int_S\text{d}\boldsymbol{\omega} = 0.
\end{equation}

the \ref{eq:theory:liuville} is recovered, if we use the Stoke’s Theorem, and using the fact that the $\boldsymbol{\omega}$ vanishes along the part of the boundary tangent to $\vec{\lambda}$.

\textcolor{red}{Note that I still have no Idea what I have written. I need to go through the original materal, which I could not find... at least I could not find what I could read and understand. }

\subsubsection{The Boltzmann equation}

Let us introduce the phase space version of the mass flux, the 6-form representing the number of phase-space trajectories crossing $S$ of the phase tube between $S_1$ and $S_2$ cross sections. \\
In the absence of collisions we obtain 

\begin{equation}
    \int_{S_1}\boldsymbol{\mu} = \int_{S_2}\boldsymbol{\mu}.
\end{equation}

Remembering that $\boldsymbol{\omega}$ represents the density of states in phase space of particles moving along geodesics, we obtain 

\begin{equation}
    \boldsymbol{\mu} = F\boldsymbol{\omega},
\end{equation}

where $F$ is \textit{invariant distribution function}, \textcolor{gray}{i.e. F is the Radon-Nikodym derivative of $\boldsymbol{\mu}$ with re $\boldsymbol{\omega}$}. \\

Consider now that collisions change the number of phase trajectories as 

\begin{equation}
    \delta N = \int_{S_2} \boldsymbol{\mu} - \int_{S_1}\boldsymbol{\mu} = \int_S \text{d}\boldsymbol{\mu} = \int_S \text{d}F\wedge\boldsymbol{\omega}.
\end{equation}

where 

\begin{equation}
    \text{d}F\wedge\boldsymbol{\omega} = \text{d}F\wedge\star (\underline{\nu}\wedge\underline{\lambda}) = \langle\text{d}F,\underline{\lambda}\rangle\star\underline{\nu} - \langle\text{d}F,\underline{\nu}\rangle\star\underline{\lambda},
\end{equation}

where $\langle\cdot,\cdot\rangle$ is a scalar product between forms and can be written as

\begin{equation}
    \langle\boldsymbol{\alpha},\boldsymbol{\beta}\rangle\text{Vol}^8 := \boldsymbol{\alpha}\wedge\star\boldsymbol{\beta},
\end{equation}

and with the $\star\underline{\lambda}=0$ on $S$ we obtain 

\begin{equation}
    \delta N = \int_S\langle\text{d}F\underline{\lambda}\rangle\star\underline{\nu} = \int_S\mathcal{C}[F]\star\underline{\nu},
\end{equation}

where we denoted the effect of collisions as

\begin{equation}
    \langle\text{d}F\underline{\lambda}\rangle = \mathcal{C}[F].
\end{equation}

This is the Boltzmann equation. In component from it reads as 

\begin{equation}
    p^{\alpha}\frac{\partial F}{\partial x^{\alpha}} - {\Game^{\gamma}}_{\alpha\beta}p^{\alpha}p^{\alpha}\frac{\partial F}{\partial p^{\gamma}} =\mathcal{C}[F].
\end{equation}

In the coordinate system adapted to the equation, when $P^0 = p^0(p^i)$, the equation reads \cite{Cercignani:2002}:

\begin{equation}
p^{\alpha}\frac{\partial F}{\partial x^{\alpha}} - {\Game^{i}}_{\alpha\beta}p^{\alpha}p^{\alpha}\frac{\partial F}{\partial p^{i}} =\mathcal{C}[F].
\end{equation}

Remembering that that the $\underline{\lambda}$ is incompressible, we can write

\begin{equation}
    \langle\text{d}F,\underline{\lambda}\rangle = \text{d}^{\star}[F,\underline{\lambda}],
\end{equation}

This allows to obtain a conservative formulation of the Boltzmann equation, that indicates the conservation of the number of particles \cite{Cardall:2002bp}

\begin{equation}
    \text{d}^{\star}[F\underline{\lambda}] = \mathcal{C}[F].
\end{equation}

Next, we re-introduce the Levi Civita connection $\nabla$ in phase space. For incompressible $\underline{\lambda}$ it gives

\begin{equation}
    \nabla_A\lambda^A=0,
\end{equation}

while the Boltzmann equation becomes 

\begin{equation}
    \lambda^A\partial_A F=\mathcal{C}[F]
\end{equation}

and its conservative form 

\begin{equation}
    \nabla_A[Fp^{A}] = \mathcal{C}[F],
    \label{eq:theory:liouvilletheorem}
\end{equation}

or in component form

\begin{equation}
    \frac{1}{|g|}\frac{\partial}{\partial x^{\mu}}\Bigg[|g|Fp^{\mu}\Bigg] + \frac{p_0}{|g|}\frac{\partial}{\partial p^{k}}\Bigg[\frac{|g|}{-p_0}{\Gamma^k}_{\alpha\beta}p^{\alpha}p^{\beta}F\Bigg] = \mathcal{C}[F].
\end{equation}

\subsubsection{From the Boltzmann Equation to the Euler Equation}

In order to derive from the Boltzmann equation the equations of hydrodynamics, we need to first define the needed varaibles of the kinetic description, such as mass and energy fluxes. We not that the density flux can easly be obtained from $\boldsymbol{\mu}$. We write the mass flow then as 

\begin{equation}
    \boldsymbol{\rho} = \int_{S_m} \boldsymbol{\mu}.
\end{equation}

Recalling the definition of the rest-mass denisty four-vector $J$, we note that

\begin{equation}
    \int_{\Sigma}(-J^{\mu}n_{\mu})\text{Vol}_x ^3 = \int_{\Sigma\times S_{m}} Fp^0\boldsymbol{\Pi}\text{Vol}_x ^3 = \int_{\Sigma\times S_{m}} F\boldsymbol{\Omega} = \int_{\Sigma}\boldsymbol{\rho}
\end{equation}

Thus, the $J$ can be written as 
\textcolor{red}{this is not clear how it was obtain. Must go through again.}

\begin{equation}
J^{\mu} = \int_{S_m}Fp^{\mu}\boldsymbol{\Pi}
\end{equation}

The second moment of the distribution function $F$ in a similar way gives the stress energy tensor 

\begin{equation}
    T^{\mu\nu} = \int_{S_m} F p^{\mu}p^{\nu}\boldsymbol{\Pi}
\end{equation}

the components of which in the frame comoving with the fluid are

\begin{equation}
    T^{\mu\nu} = 
    \begin{pmatrix}
    E & \vec{F} \\
    \vec{F} & \boldsymbol{P} \\
    \end{pmatrix}
\end{equation}

where $E$ and $\vec{F}$ are the energy density and flux respectively, and $\boldsymbol{P}$ is the stress tensor. 

The nature of the collisional operation ultimately defines the equilibrium configuration distribution function $F$ \cite{Cercignani:2002}, and thus the form of the stress-energy tenor. 

Now we use the Liouville Theorem to obtain the equations of hydrodynamics. 
To accomplish that we insert the $\boldsymbol{\Psi}$, a tensorial funcition of $p^i$, into the theorem, eqiation \ref{eq:theory:liouvilletheorem} on both sides and integrate with respect to $\boldsymbol{\Pi}$ as

\begin{equation}
    \int_{\text{I\!R}}\nabla_{A}[F\lambda^A\boldsymbol{\Psi}]\boldsymbol{\Pi}=\int_{\text{I\!R}}\mathbb{C}[F]\boldsymbol{\Psi}\boldsymbol{\Pi}
\end{equation}

where we used that $\lambda^A\nabla_{A}\boldsymbol{\Psi}=0$ as $\vec{\lambda}$ is the geodesic flow. \\

Letting the $F$ decay for large momenta we obtain the transfer equation \cite{Israel:1963,Cercignani:2002}:

\begin{equation}
    \nabla_{\mu}\int_{\text{I\!R}} F\boldsymbol{\Psi}p^{\mu}\boldsymbol{\Pi} =\int_{\text{I\!R}} \mathcal{C}[F]\boldsymbol{\Psi}\boldsymbol{\Pi},
    \label{eq:theory:transferequation}
\end{equation}

\textcolor{red}{check the sources. White intermediate steps}

In a particular case of a simple gas $\Psi$ can be shown to be one of the $\{1,p^0,p^1,p^2,p^3\}$ \cite{Cercignani:2002}. Then the right hand side of the transfer equation becomes 

\begin{equation}
    \int_{\text{I\!R}} \mathcal{C}[F]\boldsymbol{\Psi}\boldsymbol{\Pi} = 0.
\end{equation}

These $\boldsymbol{\Psi}$ are related to the quantities conserved by the collisional operator and are called \textit{collisional invariants}. Choice of $1$ would yield the mass conservation, while $p^{\mu}$ -- the energy and momentum conservation. \\

Now, having cancelled the R.H.S of the eq. \ref{eq:theory:transferequation}, we obtain the conservation laws in a following form

\begin{equation}
    \nabla_{\mu}J^{\mu} =0 \hspace{10mm}\nabla_{\nu}T^{\mu\nu} =0
\end{equation}

\subsection{Overview}

We start this section by revisiting the fundamental concepts, such as manifold, tangent and cotangent bundles, with vectors and differential forms defined on them, and operations such ans exterior, Wedge product and Hodge star operator. \\

Then we set ourselves a goal to obtain the general form of general relativistic hydrodynamics. This includes the equations for space-time evolution adopted adopted for use in numerical applications and Euler equations for the fluid, which we aim to obtain through the Liuville's theorem and Boltzmann equations. \\

To derive the Einstein field equations we perform the variation of the so-called Hilbert action, applying the Euler-Lagrange equation. Thus we start by briefly deriving the Euler-Lagrange equations, using the fact that the fields we are interested in are defined over only a compact domain and that the choice of the variation of coordinates is arbitrary, i.e. $\partial S(\boldsymbol{q}, \nabla \boldsymbol{q}) = 0$. Then, in a similar way, the variation of the Hilbert action, yields the Einstein Field equation. \\

For practical applications it is useful to express the EFE as a initial value boundary problem. The hamiltonian formalism allows to do that, which we briefly review. We then sketch the $3+1$ decomposition procedure, introducing the spacelike foliation and extrinsic curvature that allow us to obtain the constraint equations, that has to be satisfied on every hyper-surface of the hypersurface. Then, emplying the EFE and Gauss-Codacci equations we write the Hamiltonian density, whose variation with respcet to the variables of folliation $\alpha$ and $\beta$ yileds constraint equation. The evolution equations then are obtained through the variation of the Hamiltinan with respect to the three-metric and momentum. \\

The obtained ADM system is however not well suited for numerical applications, being only weekly hyperbolic. We thus briefly touch on a strongly hyperbolic formulation, the Z4 formulation, that exhibit such usefull for numercs properties as constraint violation dumpening (constrain preservation) and its evolution, the CCZ4 formulation that is furhter adopted for BH evolution. \\

After, we briefly touch on the gauge conditions, as in the 3+1 we are left with the freedom on how to do the foliation, namely, chosing the lapse function and shift vector. \\

Then we proceed with deriving equations of general relativistc hydrodynamics, aiming to provide a flux-conservative formulation. We first define the kinematics of the relativistc fluid, \textit{i.e.,} a covariant description in terms of invariant quantities. With this goal in mind we define the rest-mass density vector, whose divergence give the number of particles conservation. Then we re-introduce the stress energy tensor ans show that via Bianki identities, its divergence alse vanishes. \\

To discuss the dynmaics of the fluid, we first, set its type. We consider the fluid that hs and thermal conductivity and no viscosity, \textit{i.e.}, the perfect fluid. In addition to fluid kinematics and the stress-energy tensor, describing tis motion, we discuss an equation of state. Together they from a hyperbolic ssytem of equations that describes the evolution of the fluid in space-time, once the initial data is set.\\

For the reasosns of numerical stability, a special formulation of the equations of general relativistic hydrodynamics is required. Such is the Valencia formulation. The main idea is to constract an advection-like equation for fluxes of conserved quantites, from which the promitive quantities can be reconstructed.\\ 

To derive these fluxes we first decompose the four velocity into the component parallel to the normal to the hypersurface and a purely spatial part. This leads us to the defention of the conserved density. Then we introduce a vector whose zeroth component is just a norm and spatial component which is a \textcolor{gray}{tangent vector to hypersurface}. This allows us to write the conserved and primitive qiantities of the formulatio, as well as the sourve term. \todo{not all quntities in Val.Form. are clear. What is $S_j$ and $\epsilon$} \\

Next we consider geometrical approach to the general-relativistic boltzmann equation \textcolor{gray}{still not sure why though}. To do that we first introduce necessary tools, namely vectors and tensors that are needed to define the phase-space. There there are $2n$ components with the first $n$ being coordinates and the second $n$ being impulses. In addition, we introduce the coordinate trnaformation and finally, the metric on the tangent bundle. \\

Having tools set, we derive the Liuville theorem. In order to do that we write phase-space flow of particles moving along geodesics which is represented by the Poincare 1-form and associated vecotr. In addition we define a mass shell, a norm to it and a irrotational form on a tangent bundle. Together with the poincare form it allows us to define the denisty of the phase-space trjectories which we denote ad a Hodge operator of the wedge product of these two forms. After some calculations, we obtain that this form can be expressed in a coordiante independed way as a split of two froms, the proper geodeiscs flux froms on the hypersurface and the mass shell respectively. By considering the "phase tube" with two crossections, we arrive that itegrated flux is conserved, which constitutes the Liouville\'s Theorem. \\

After that we proceed with deriving the Boltzmann equation. Which we start by intoducing the number of phase-space trajectories crossing the section of a "phase tube". The relation between the number of the phase space trajectoies and the density defined above, yilds the invariant distribution function. Considering the change in the number of particles due to collisions. The change in scalar product between the exteriour derivative of the distribution function and poincare 1-form due to collisions constitudes the Boltzmann equation. \textcolor{gray}{revise this.}. Re-introducing the Levi Civita connection in phase space, and taking an advantage of the incompressibility if the poincare 1-form, we obtain a conservative form of the Boltzmann equation. \\

From Boltzmann equation we can now obtain an equations of hydrodynamics. For that we firs redefine the variables of the kinetic description, such as mass and energy fluxes, recalling the definition of the rest-mass density four-vector. Similarly how the vector can be now exprressed as a first moment of the distribution function, the second moment gives the sress energy tensor. However, we note that the nature of the collisional operation ultimately denes the equilibrium conguration distribution function and thus the form of the stress-energy tenor. \\

Next we use the Liouville Theorem to obtain the equations of hydrodynamics introducing a tensorial function of the momenta, that is related to the quantities conserved by the collisional operator and are called collisional invariants into the integral form onf the theorem. Letting the distribution function decay for large momenta we obtain the transfer equation. For as simple gas this can be reduced to already familiar equations where divergence of the rest mass vector J and stress energy tensor is zero. 



\chapter{Numerical Approximation of Conservation Laws}

In this chapter we aim to briefly remark on the theory of the numerical approximation of conservational laws. Owing to its key importance in physics, there is quite an extensive amount of literature concerning the topic. We therefore select the aspects that are of relevance to this work, namely high-order, state-of-the-art numerical methods for the solution of conservation laws. For the sake of compactness we would refrain from stating complete descritions of these schemes, focusing on key ideas behind them, their advantages and drawbacks in the context of general-relativistic hydrodynamics. \\

This chapter is structured as following. In Section \ref{sec:theory:conserv_laws:theorback} we state the basics behind the theory of conservation laws and their numerical approximation. In Section \textcolor{red}{[??]} we give a brief overview of the Godunov-like finite-volume schemes. Next, in Section \textcolor{red}{[??]} we focus on the high-resolution shock-capturing (HRSC) finite-difference schemes. \textcolor{gray}{Finally, in Section [??????] we present discontinuous Galerkin methods.}

\textcolor{red}{note that here section is capital 'S'}

\section{Theoretical Background}
\label{sec:theory:conserv_laws:theorback}

In this section we briefly recall the basics behind the mathematical theory of conservational laws and subsequently, their numerical application. We start by defining the weak and entropic solutions, and present certain results regarding existence and uniqueness of these solutions for conservation laws. We then proceed with reviewing numerical approximation to conservation laws, as well as concepts of consistency, stability and convergence, \textcolor{gray}{briefly stating the Lax-Richtmeyer theorem}. Then we conclude with reviewing the extension to the case of non-linear equations. This section is based on the descriptions provided in \cite{LeVeque:1992,Tadmor1998}, and we refer the reader to these sources for more in-depth discussion. \\

\subsection{Conservation Laws}

Let us consider the consercation laws in the following form

\begin{align}
    \partial_t\boldsymbol{u} + \nabla\cdot\boldsymbol{f}(\boldsymbol{u}) = 0, \hspace{10mm} &(t,x)\in \text{I\!R}_{+}\times\text{I\!R}^d , \\
    \boldsymbol{u}(0, x) = \boldsymbol{u}(x), \hspace{18mm} &x\in \text{ I\!R},
    \label{eq:theory:conservlaws}
\end{align}

where $\boldsymbol{u}$ is the vector of $m$ unknowns, $\boldsymbol{f}=(\boldsymbol{\boldsymbol{f}^1,...,\boldsymbol{f}^m})$ is a $d$-dimensional flux and $\boldsymbol{u_0}\in\big[L^{\infty}(\text{I\!R}^d)\big]^m$ is the initial data. \\

Investigations of the system \ref{eq:theory:conservlaws} showed irrespective of the initial data, the solution can develop discontinuities (shocks) in a finite time. Thus, the system should be viewed in distribution formalism. There, if for all test functions $\upsilon\in C_0 ^1 (\text{I\!R}^{d+1})$ and $i=1,2,...,m$  we obtain

\begin{equation}
    \int_{0}^{\infty}\text{d}t\int_{\text{I\!R}^d}\big[u^i \partial_t\upsilon + \boldsymbol{f}^i(\boldsymbol{u})\cdot\nabla\upsilon\big]\text{d}x = \int_{\text{I\!R}^d} u_0 ^i \upsilon \text{d}x,
\end{equation}

then, the vector $\boldsymbol{u}\in\big[\text{I\!R}_{+}\times\text{I\!R}^d\big]^m$ is a \textit{weak solution} of \ref{eq:theory:conservlaws}.\\

It can be shown that that multiple weak solutions are allowed even for a scalar conservation law. Let us then consider the concept of the entropic solution. A convex function \textcolor{red}{what is it?}, $\eta(\boldsymbol{u})$, is said to be an entropy function if its Hesian \textcolor{red}{what is it???}, $\nabla_{\boldsymbol{u}}^2\eta$, symmetrizes the spatial Jacobian, $\nabla_{\boldsymbol{u}}f^i$ \textcolor{red}{WHAT IS IT?!},

\begin{equation}
    \nabla_{\boldsymbol{u}}^2\eta\cdot\nabla_{\boldsymbol{u}}\boldsymbol{f}^i = [\nabla_{\boldsymbol{u}}\boldsymbol{f}^i]^{T}\cdot\nabla_{\boldsymbol{u}}^2\eta, \hspace{10mm} i = 1, ... , m.
\end{equation}

Here we infer the compatibility relation, introducing the entropy flux $\boldsymbol{\psi} = (\boldsymbol{\psi}^1,...,\boldsymbol{\psi}^m)$, as 

\begin{equation}
    [\nabla_{\boldsymbol{u}}\eta]^T\cdot\nabla_{\boldsymbol{u}}\boldsymbol{f}^i = [\nabla_{\boldsymbol{u}}\boldsymbol{\psi}^i]^T, \hspace{15mm} i = 1,...,m.
\end{equation}

The pair $\eta\boldsymbol{\psi}$ is referred to as \textit{entropy pair}. \\

Then, the \textit{entropic solution} is a weak solution that for any $(\eta\boldsymbol{\psi})$, in the sense of   admits

\begin{equation}
    \partial_t\eta(\boldsymbol{u}) + \nabla\cdot\boldsymbol{\psi}(\boldsymbol{u})\leq 0.
\end{equation}

Concerning distributions, the \textit{entropic solution} requires that for any positive test function $\upsilon\in C_0 ^1 (\text{I\!R}_+\times\text{I\!R}^d)$ that 

\begin{equation}
    \int_{\text{I\!R}_+\times\text{I\!R}^d} \big[\eta(\boldsymbol{u})\partial_t\upsilon + \boldsymbol{\psi}(\boldsymbol{u}) \cdot \nabla\upsilon \big]\text{d}t\text{d}x = 0
\end{equation}

It is possible to show \cite{LeVeque:1992}, that this condition in a scalar cast is equivalent to making characteristic line impinged into shock waves. This constitutes the the process that resulted in a shock forming is irreversible and that the time-symmetry is not longer applies. \\

Considering the scalar case, where $m=1$, it is possible to prove the existence and uniqueness of the entropic solution under very general conditions \cite{Kruzkov:1970}. It s is also can be extended to measure-valued solutions \cite{DiPerna:1985} and to the case of conservation laws on manifolds \cite{Benartzi:2007}. \\

On the other hand, uniqueness and stability of entropic solutions is not well understood in case of systems of conservation laws, as not even the existence of $(\eta\boldsymbol{\psi})$ for the general system of equation has been proven. In \cite{Chen:2009} a novel approach has been employed, based on divergence-measure vector fields. For one dimensional
Riemann problem, where the initial data in a form 


\begin{equation}
    \boldsymbol{u}_0(x) = 
    \begin{dcases}
        \boldsymbol{u}^L, \hspace{5mm} \text{if } x<0; \\
        \boldsymbol{u}^R, \hspace{5mm} \text{if } x>0.
    \end{dcases}
\end{equation}

it allowed to prove the existence, uniqueness and stability of the entropic solution of the Euler equations for a classical ideal-gas \cite{Chen:2003}. \\

However the mere exestiance of the weak solution to the Riemann problem for general equation of state is not guaranteed \cite{Curtis:1972}. (for more recent results regarding classical Euler equations, see the review \cite{Chen:2006}). \\

On the other hand, using Glimm’s method \cite{Glimm:1965} in the existence of solutions to the Riemann problem was shown in the relativistic case but for the ultrarelativistic equation of state \cite{Smoller:1993}. \\

For \textit{strictly hyperbolic systems} \textit{i.e.,} when $\nabla_{\boldsymbol{u}}\boldsymbol{f}$ has a complete set of real eigenvalues and eigenvectors, the existence of weak solutions was proven in case when the initial data having small enough initial jump \cite{Lax:1957}.

\subsection{Consistency, Stability and Convergence}

Now we consider how the conservation laws can be treated numerically. We limit the discussion to the $m=1$ case, as the non-linear theory is well established only for the scalar fields. Thus we define a problem as 

\begin{align}
    \partial_t u + \nabla\cdot\boldmath(u) = 0&, \hspace{10mm} (t,x) \in I\!R_{+}\times I\! R^d \\ 
    u(0, x) = u_o(x)&, \hspace{15mm} x\in I\!R^{d},
\end{align}

where $u$ is now just a scalar function. \\

Let us introduce the following notation. The form of equations \ref{eq:theory:conservlaws} allows us to view the solution to this system in a form of a "curve" in an infinite dimensional vector sapce $L^{\infty}I\!(R^d)$, or as a sequence of bounded functions $u(t,\cdot)\in L^{\infty}(I\! R^d)$, that we can consider only being functions of time $u(t)$ with values in the vector space. Owing to the curve being bounded in $L^{\infty}(I\!R^d)$, the sequence of bounded functions read $u(t)\in L^{\infty}[I\!R_{+};L^{\infty}(I\!R^d)]$. Then the system  \ref{eq:theory:conservlaws} can be seen as a system of ordinary differential equations (ODEs), which can be represented as 

\begin{equation}
    \frac{\text{d}u(t)}{\text{d}t} = \mathcal{L}[u(t)], \hspace{10mm} u(0) = u_0,
    \label{eq:theory:conservlawsode}
\end{equation}

where we associate the operator $\mathcal{L}(\cdot)$ with the $-\nabla\cdot\boldsymbol{f}(\cdot)$. It is important to remember, that as the $u(t)$ is not a smooth function of time, the equation \ref{eq:theory:conservlawsode} should be considered from a point of view of distributions. In addition, we note that stating $u(0) = u_0$ is in all mathematical rigor is a restriction of general functions $L^{\infty}[I\!R_{+};L^{\infty}(I\!R^d)]$ to the set of zero Lebesgue measure, which is not define. For the purpose of keeping the discssion brief we refer to the \cite{Kruzkov:1970} for the related discussion. For the numerical applications we further restrict $u(t)$ to be a smooth function of time, and leave the mathematical subtleties out of discussion.  \\

Transitioning from general formulation \ref{eq:theory:conservlaws} to one adopted for a scalar function $u(t)$ \ref{eq:theory:conservlawsode}, we can introduce the numerical approximation, that is depended in a discrimination parameter $\Delta$. This approximation reads

\begin{equation}
    \frac{\text{d}u^{\Delta}(t)}{\text{d}t} = L^{\Delta}[u^{\Delta}(t)], \hspace{10mm} u^{\Delta}(0) = P^{\Delta}[u_0],
    \label{eq:theory:conservlawsodepde}
\end{equation}

where $u^{\Delta}$ and $L^{\Delta}$ are approximations of $u$ and $\mathcal{L}$, \textit{i.e.,} $u^{\delta}\approxeq u u$, $L^{\Delta}\approxeq \mathcal{L}$ and $P^{\Delta}$ is a projection operator. However, as the error associated with it is negable in comparison with other errors arizing in discritisation of conservation laws, we will ignore it, effectively assinging that $u^{\Delta}(0) = u_0$. One of such errors is the trucaction error. \\
The \textit{ local truncation error} can be defined as 

\begin{equation}
    r^{\Delta} = L^{\Delta}[u(t)] - \mathcal{L}[u(t)],
\end{equation}

where $u(t)$ is the exact solution to \ref{eq:theory:conservlaws}. We call a numerical scheme \textit{consistent} if the $r^{\Delta}\rightarrow 0$ when $\Delta\rightarrow 0$ in a given norm for all possible initial data $u_0$. Note, however, that the choice of norm is problem- and method- dependent and may limit the allowable initial data. \\

We then call a scheme to be of order $r$ if 

\begin{equation}
    || r^{\Delta}(t) || = \mathcal{O}(\Delta^r).
\end{equation}

\begin{sidenote}
    \textbf{Infimum and Supremum} \\
    Im math, the \textit{infimum (inf)} of a subset $S$ of a partially ordered set $T$ is the greatest element of $T$ that is less than or equal to all elements of $S$. \\
    The \textit{supremum (sup)} of a subset $S$ of a partially ordered set $T$ is the least element in $T$ that is greater than or equatl to all elements of $S$. The upper bound of a subset $S$ of a partially ordered set (P,$\leq$) is an element $b$ of $P$ such that $b\geq x$ for all $x$ in $S$. If a supremum of a subset $S$ exhists it is unique. The supremum of a subset $S$ of partially ordered set $P$ does not necessearly belongs to $S$. But if it does, it is the maximum, or the greatest element of $S$. \\
    Examples of suprema.
    The supremum of a set of real numbers of $\{1,2,3\}$ is $3$. The number $4$ is an upper bound but it is not the least upper bound and hence not the supremum. \\
    \begin{align}
        \sup\{x\in\text{I\!R}|0<x<1\} = 1. \\
        \sup\{(-1)^n - 1/n | n = 1,2,3,... \} = 1. \\
        \sup\{x\in\mathbb{Q} | x^2 < 2\} = \sqrt{2}
    \end{align}
\end{sidenote}

A scheme is considered to be \textit{stable} if the norm $L^{\Delta}$ is limited 

\begin{equation}
    |||L^{\Delta}||| := \sup \frac{||L^{\Delta}||}{||\upsilon||}\leq C,
\end{equation}

where $C\geq 0$ is a constant independent of $\upsilon$. \\
A scheme is considered to be \textit{convergent} if 

\begin{equation}
    \lim_{\Delta\rightarrow 0} ||u^{\Delta}(t)-u(t)|| = 0, \hspace{10mm} \text{a.e. } t\in \text{I\!R}_{+}.
\end{equation}

The relation between the consistency, stability and convergence is given by the Lax-Richtmeyer equivalence theorem \cite{Lax:1956}. It states that the numerical approximation of well-posed problems is convergent if and only if the scheme is stable and consistent. In addition, consider a scheme of the order $r$, then

\begin{equation}
    || u^{\Delta}(t) - u(t) || = \mathcal{O}(\Delta^r).
\end{equation}

However, in the non-linear case, the \textit{non-linear stability} is required in addition to the stability and consistency are to assure convergence. 

\subsection{Non-Linear Equations and Non-Linear Stability}

The system \ref{eq:theory:conservlawsodepde} is a system of ordinary differential equations, where the discritisation of time has to made as well as space. However, up to now we were discussing only the latter, introducing the operator $\mathcal{L}$ and its approximation $L^{\Delta}$. The reason for that is the following. The spatial discritisation introduces much more prominent truncation error than the time descritisation, which allowed us to limit the discussion of non-linear conseravation laws to different choices in constructing $L^{\Delta}$. However, the properties of the time discritisation start to play an important role in the context of non-linear stability and convergence of numerical schemes in the non-linear case. Thus, we shall discuss a \textit{fulli descrete} schemes. \\

Let us start by introducing a one-parameter family of evolution operators $\{\mathcal{T}_{s\in\text{I\!R}_+}\}$. Following \cite{Kruzkov:1970}, the $\{\mathcal{T}_{s\in\text{I\!R}_+}\}$ allows an operation of composition that forms a semi-group and the $\mathcal{T}_t(u_0)$ yields a solution \ref{eq:theory:conservlaws} at a time $t$, given the initial data $u_0$, \textit{i.e.,}

\begin{equation}
    u(t) = \mathcal{T}_t(u_0), \hspace{10mm} \mathcal{T}\circ\mathcal{T}_t = \mathcal{T}_{t+s}.
\end{equation}

Its descrete version then

\begin{equation}
    u^{\Delta}(k\Delta t) = T^{\Delta}_{\Delta t}(u_0), \hspace{10mm} T^{\Delta}_s\circ T^{\Delta}_t = T^{\Delta} _{t+s}
\end{equation}

\textit{i.e.,}

\begin{equation}
    u^{\Delta}(t+\Delta t) = T^{\Delta} _{\Delta t}[u^{\Delta}(t)]
\end{equation}

in a fully desrete form. \\

Importantly, there is only one discretization parameter $\Delta$ for time and space. This is allows due to the  Courant–Friedrichs–Lewy (CFL) condition, which is in essence a linear stability condition of a time-integrator, that links two discretizations. \\

For a fully-discrete from, the truncation error reads 

\begin{equation}
    r^{\Delta}(t) = T^{\Delta}_{\Delta t}[u(t)] - u(t + \Delta t) = T^{\Delta}_{\Delta t}[u(t)] - \mathcal{T}_{\Delta t}[u(t)],
\end{equation}

where $u(t)$ is the exact solution to \ref{eq:theory:conservlaws}. \\
For the consistency, it is required that the norm $||\cdot||$, $||r^{\Delta}(t)||\rightarrow 0$ when $\Delta\rightarrow 0$. \\
A scheme is said to be of an order $r$ if $||r^{\Delta}(t) = \mathcal{O}(\Delta^r)||$. \\
A scheme is regarded to be linearly stable if 

\begin{equation}
    |||T^{\Delta}_{\Delta t}||| \leq C,
\end{equation}

where $C$ is a constant. \\

Let us consider how a numerical scheme in a non0-linear case then can be constructed. The Lax-Wendroff theorem \cite{Lax:1960} reads, that for a function $u$ to be a weak solution of \ref{eq:theory:conservlaws}, the numerically approximated solution $u^{\Delta}$ to the \ref{eq:theory:conservlawsode} obtained via conservative and consistent scheme should converge strongly. By conservative we understand a scheme such that 

\begin{equation}
    \int_{\text{I\!R}^d} T^{\Delta}_s(\upsilon)\text{d}x = \int_{\text{I\!R}^d}\upsilon\text{d}x, \hspace{10mm} \text{for any } \upsilon\in L^1(\text{I\!R}^d),
\end{equation}

where $L^1$ is norm and the strong convergence implies that this $L^1$ is the norm to the function $u$. \\

Thus, the important point is to obtain a conditions that is sufficient for a scheme to be convergent. Then, the ax-Wendroff theorem will ensure that this solution is a weak solution. It is however important to note that the theorem does not guarantee that the solution also entropic, and an additional criterion of satisfying the entropy inequality, then has to be imposed. \\

\begin{sidenote}
    \textbf{Meaasure in mathematics} \\
    measure of a set is a systematic way to assign a number to each suitable subset of that set intuitively interpreted as its size. Thus, a measure is a generalisation of length, area, volume. Example: \textit{Lebesgue measure on a Euclidean space} that assignes the conventional length area and volume to Eucledian geometry to the suitable subsets of the $n-$dimensional Eucledian space $\mathbb{R}^n$. For instance the Lebesgue measure of the interval $[0,1]$ in the real numbers is this length. \\
    Measure is a function, that assigns a non-negative real number to certain subsets of a set $X$. It mist be countably addictive. \\
    The formal defitinion: let $X$ be a set and $\Sigma$ a $\sigma$-algebra over $X$. A function $\mu$ from $\Sigma$ to the extended real number line is called measure if it sutisfies the following property:
    \begin{itemize}
        \item Non negativity: For all $E$ in $\Sigma$ we have $\mu(E)\geq 0$
        \item Null empty set: $\mu(\emptyset) = 0$.
        \item Countable addictivity or $\sigma$ addictivity: for all countable collections $\{E_k\}_{k=1}^{\infty}$ of pairwise disjoint sets in $\Sigma$: $\mu\Big(U_{k=1}^{\infty}E_k\Big) = \sum_{k=1}^{\infty}\mu(E_k)$.
    \end{itemize}
    \textbf{Lebesgue measure} \\
    This is a standart way of assigning a measure to subset of $n-$dimensional Eucledian space. For $n=1,2,3$ it coinsides with simply length, area and volume. In general it is also called a $n-$dimensional volume or $n-$volume. Usually a measure of the Lebesgue-measurable set $A$ is denoted with $\lambda(A)$. \\
    Formal definition: \\
    Given a subset $E\subseteq\mathbb{R}$ with the length of interval $I=[a,b]$ (or $I=(a,b)$) given by $l(I) = b-a$ Lebesgue outer measure $\lambda^*(E)$ is defined as 
    \begin{equation}
        \lambda^*(E) = \inf\Bigg\{\sum_{k=1}^{\infty}l(I_k):(I_k)_{k\in N} \text{ is a sequence of open intervals with } \subseteq U_{k=1}^{\infty}I_k\Bigg\}
    \end{equation}
    The first part of the definition states that the subset $E$ of the real numbers is reduced to its outer measure by coverage by sets of open intervals. Each of these sets of intervals $I$ covers $E$E in the sense that when the intervals are combined together by union, they contain $E$. The total length of any covering interval set can easily overestimate the measure of $E$, because $E$ is a subset of the union of the intervals, and so the intervals may include points which are not in $E$. The Lebesgue outer measure emerges as the greatest lower bound (infimum) of the lengths from among all possible such sets. Intuitively, it is the total length of those interval sets which fit $E$ most tightly and do not overlap.\\
    That characterizes the Lebesgue outer measure. Whether this outer measure translates to the Lebesgue measure proper depends on an additional condition. This condition is tested by taking subsets $A$ of the real numbers using $E$ as an instrument to split $A$ into two partitions: the part of $A$ which intersects with $E$ and the remaining part of $A$ which is not in $E$: the set difference of $A$ and $E$. These partitions of $A$ are subject to the outer measure. If for all possible such subsets $A$ of the real numbers, the partitions of $A$ cut apart by $E$ have outer measures whose sum is the outer measure of $A$, then the outer Lebesgue measure of $E$ gives its Lebesgue measure. Intuitively, this condition means that the set $E$ must not have some curious properties which causes a discrepancy in the measure of another set when $E$ is used as a "mask" to "clip" that set, hinting at the existence of sets for which the Lebesgue outer measure does not give the Lebesgue measure. (Such sets are, in fact, not Lebesgue-measurable.)\\
    \textbf{$L^p$ space} \\
    The $L^p$ spaces are function spaces defined using a natural generalisation of the $p-$form for a fintie-dimensitiona vector space. They are also called \textit{Lebesgue spaces}. $L^p$ spaces form an important class of \textit{Banach spaces} in functional analysis, and of topological vector spaces. \\
    \textit{$p-$form in finite dimensions} \\
    Consider a length vector $x=(x-1, x_2...,x_n)$ in the $n-$ dimensional real vector space $R^n$ is usually bien by the Euclidean norm 
    \begin{equation}
        ||x||_2 = (x_1 ^2 + ... + x_n ^2)^{1/2}.
    \end{equation}
    For real numbers the $p-$norm or $L^p$-norm of $x$ is defined by
    \begin{equation}
        ||x||_p = (|x_1|^p + ... + |x_n|^p)^{1/p}.
    \end{equation}
    \textit{$l^p$ spaces}
    an $L^p$ space may be defined as a space of measurable functions for which the $p-$th power of an absolute value is  Lebesgue integrable where functions which agree almost everywhere are identified.
    \textbf{Total variation.} \\
    The total variation of a real-valued function $f$ defined on an interval $[a,b]\subset R$ is the quantity 
    \begin{equation}
        V_b ^a (f) = \sup_{\mathcal{P}}\sum_{i=0}^{n_p -1}|f(x_{i+1} - f(x_i)|,
    \end{equation}
    where the supremum runs over the set of all partitions $\mathcal{P} = \{\mathcal{P}=\{x_0,...,x_{np}\}|P\text{ is the partition of }[a,b]\}$ of the given interval. \\
    \textit{total variation for function of $n>1$ real variables} \\
    Let $\Omega$ be an open subset of $\mathbb{R}^n$. Given a function $f$ belonging to $L^1(\Omega)$, the total variation of $f$ in $\Omega$ is defined as 
    \begin{equation}
        V(f,\Omega):=\sup\Bigg\{\int_{\Omega}f(x)\text{div}\phi(x)\text{d}x:\phi\in C_{c}^1(\Omega, \mathbb{R}), ||\phi||_{L^{\infty}(\Omega)}\leq 1\Bigg\}
    \end{equation}
    where $C_c ^1(\Omega, \mathbb{R}^n)$ is the set of continuously differentiable vector functions of compact support contained in $\Omega$ and $||\cdot||_{L^{\infty}(\Omega)}$ is the essential supremum norm. 
    \textbf{Bounded variation} \\
    A function of bounded variation also known as $BV$ dunction is a real valued function whose total variation $TV$ is bounded or finite. \\
    In case of a continous function of one variable, for that function to be of  bounded variation implies taht the distance along the direction of the $y-$axis, neglecting the $x-$axis motion contribution, traversed by a point, moving along the grap, has a finite value. \\
    In case of several variables a function $f$ defined on an open subset $\Omega$ of $\mathbb{R}^n$ is staid to have bounded variation it its distributional derivative is a vector-valued finite Radon measure.\\
    Importantly, functions of bounded variation form an algebra of dinsontinous functions whose first derivative exists almost everywhere. Thus, they can be used to define generalized solutions of nonlinear problems involving functionals. ODE, PDE.\\
    Returning to the definition of the total variation $TV$ for a function of one variable, $V_a ^b (f)$. If $f$ is differentiable and its derivative is Reimann-integrable, its total variation is the verical component of the arc-length of its graph, \textit{i.e.,}
    \begin{equation}
        VC_a ^b(f) - \int_a ^b |f'(x)|\text{d}x.
    \end{equation}
    A condinous real-valued function $f$ on the real line is said to be of bounded variation \textit{BV function} on a chosen interval $[a, b]\subset \mathbb{R}$ if its total variation is finitie, \textit{i.e.,}
    \begin{equation}
        f\in\text{BV}([a,b]) \leftrightarrows V_a ^b (f) < +\infty
    \end{equation}
    The space of bounded varaition (BV functions) can be defined as 
    \begin{equation}
        \text{BV}(\Omega) = \{u\in L^1(\Omega): V(u, \Omega) < +\infty\}
    \end{equation}
\end{sidenote}

Let us then proceed with derivation of the conditions that is sufficient for convergence. We start by introducing the total-variation of a function $u$, belonging to $L^1(\Omega)$ in a domain $\Omega$

\begin{equation}
    \text{TV}(\upsilon;\Omega):=\sup\Bigg\{ \int_{\Omega} \upsilon\nabla\cdot\boldsymbol{\phi}\text{d}x: \boldsymbol{\phi}\in [C_0 ^1 (\Omega)]^d, ||\boldsymbol{\phi}||_{L^{\infty}(\Omega)}\leq 1 \Bigg\}, \hspace{5mm} \text{ for any }\boldsymbol{\phi} \in C_0 ^1 (\Omega)
\end{equation}

where $C_0 ^1(\Omega)$ is the space of continuously differentiable vector functions $\boldsymbol{\phi}$ of compact support contained in $\Omega$ and $||\cdot||_{L^{\infty}(\Omega)}$ is the essential supremum norm. \\

It is possible to show that (\textit{e.g.,} \cite{Luigi:2002}) if $\text{TV}(\upsilon ; \Omega) < \infty$, then $\text{TV}(\upsilon ; \Omega) = |D\upsilon|(\Omega)$ where $|\cdot|$ stands for the vector-valued measure $D\upsilon$ or 

\begin{equation}
    \text{TV}(\upsilon; \Omega) = \int_{\Omega}|D\upsilon|\text{d}x
\end{equation}

where $D\upsilon$ is a gradient of $\upsilon$ is a sense of distributions. \\
Further we introduce the vector space BV, $\text{BV}(\Omega)$, that is the spce of all functions in $L^1(\Omega)$ with finite total variation. This is a Banach space with respect to the norm 

\begin{equation}
    || \upsilon ||_{\text{BV}(\Omega)} = \int_{\Omega}\big(|\upsilon| + |D\upsilon|\big)\text{d}x.
\end{equation}

The importance of BV spaces lies in the following. It was shown \cite{Conway:1966} that for any $t\geq 0$ $u(t)\in \text{BV}(\text{I\!R}^d)$, and if the initial data $u_o\in \text{BV}(\text{I\!R}^d)$, then the function $u \ in \text{BV}(\text{I\!R}\times\text{I\!R}^d)$. This constitutes the invariance property of the BV spaces. It is important to note, however, that the validity is confirmed only for scalar conservation laws. A system of conversational laws on the other can have a solution that is not BV even if the initial data was in BV space. Consider for example and Euler equation in Lagrangian coordinates. It was shown the the solution can develop vacuum regions, \textit{i.e.,} not BV, if the initial data exhibit sufficiently large variations, while still being in BV space.  Such a solution becomes an $L^1$ function or a Radon measure \cite{Chen:2006}. Another reason why BV are important is that boudned sets in BV$(\Omega)$ are sequentially compact in $L^1 _{\text{loc}}(\Omega)$ \textit{e.g.,} \cite{Luigi:2002}. This implies that for any sequence of functions $[\upsilon_n]\in\text{BV}(\Omega)$ there is a subsequence converging in the $L^1$-norm to a function $L^1 _{\text{loc}}(\omega)$. \\

Then, the goal is to construct a numerical scheme that is able to mimic the BV-invariance of the exact conservation laws. Then the Lax-Wendroff Theorem would assure that the numerical scheme that is $L^1$ stable if consistent produces a sequence of solutions, for different discritisation parameter $\Delta$, that will converge in $L^1$-norm to a weak solution as we decrease $\Delta$. It is however important to note, that not a sequence of solutions, but a union of subsequences $[u^{\Delta}]$ is produced, each of which is converging to a possible different weak solution. If this is not the case, then there exist a sequence $\{u^{\Delta_i}\}$ and $\epsilon > 0$ such that $\Delta_i\rightarrow 0 $ when $i\rightarrow \infty$ and $\text{dist}(W,u^{\Delta_i})>\epsilon$ for any $i$. Such schemes are called \textit{TV-stable}. The important propery of such scheme is, that for all initial data $u_0\in L^{\infty}(\text{I\!R}^d)\cap \text{BV}(\text{I\!R}^d)$ there exist $\Delta_0 > 0$ and $C\geq 0$ such that 

\begin{equation}
    \text{TV}(T_s ^{\Delta}(u_0); \text{I\!R}^d)\leq C \text{ for any } \Delta < \Delta_0.
\end{equation}

For example, if a scheme shows the property 

\begin{equation}
    \text{TV}(T_{\Delta t} ^{\Delta}(\upsilon_0); \text{I\!R}^d) \leq \text{TV}(\upsilon,  \text{I\!R}^d), \text{ for any } \upsilon\in L^{\infty}(\text{I\!R}^d)\cap \text{BV}(\text{I\!R}^d),
\end{equation}

it is one of the \textit{total-variation diminishing} (TVD) schemes. 

In the context of non-linear scalar conservation laws, an example of a convergent scheme is a \textit{monotone} scheme \textit{i.e.,}

\begin{equation}
    u \geq \upsilon \text{ a.e. } \rightarrow \hspace{5mm} T_{s}^{\Delta}(u) \geq T_{s}^{\Delta}(\upsilon).,
\end{equation}

and if $T_{s}^{\Delta}$ is conservative and monotone, it does satisfy the strong stability condition 

\begin{equation}
    || T^{\Delta} _{\Delta t} - T^{\Delta} _{\Delta t}(\upsilon)||_{L^1(\text{I\!R}^d)}, \text{  for any  } u.\upsilon\in L^1(\text{I\!R}^d),
\end{equation}

\textit{i.e.,} it is an $L^1$-contraction \cite{Crandall:1980proc}, which in implies that the method is TVD \cite{LeVeque:1992}. Crandall and Majda \cite{Crandall:1980} has shown that numerical solution thus, obtained with a consistent stable and momotone scheme converges to a weak solution of \ref{eq:theory:conservlaws}. In addition authors showed that the entropy inequality is satisfied by these schemes and hence the numerical solution converges to an entropic (unique) solution of the \ref{eq:theory:conservlaws}. The accuracy of the scheme has however been shown to be limited to the first order by Harten et al. \cite{Harten:1976}. Thus, while requireing a scheme to be monotonic allows for a non-linear stability, it prohibits the higher order accuracy.\\

The solution to the problem was first to use the non-linear dissipation methods that reduces convergence only locally to the tirst order in the vicinity of the dinsintiniuties. These schemes are high-order accuracy, high resolution shock-capturing (HRSC) schemes. They are not monotone, while being TVD in one dimentional case. And Goodman and LeVeque \cite{Goodman:1985} has shown that this is a limitation of TVD schemes. Than can be desigend to be any ordder of accuracy in 1D but can only but in multidminsinal caes they are at most first-order accuracy. \\

Multuple other high order accuracy schemes have been proposed whith however weaker conditions of non-linear stability and not yet fully explored convergence. For example, schemes that satisfy the maximum-principle \textit{i.e.,} such that if $m < u_0 < M$ then $u^{\Delta}(t,\cdot)\in[m, M]$. Such is the second-order central-scheme by Kurganov and Tadmor \cite{Kurganov:2000}. \\

It is however remains that the commonly used modern multidimensional HRSC schemes are not TVD or even has been shown to be TV-stable. Numerical evidence to support a hypothesis that these schemes converge to the entropic solution of conservation laws, how the mathematical proof is yet to be provided. Additionally, even the stabolity and convergence in one-dimensional case has not been forven for a general system of non-linear conservation laws, \cite{LeVeque:2002}, while there also numerical evidence for the convergence of these schemes. \\

It is believed that a more precise characterization of piecewise-regular entropic solutions of conservation laws would lead to a proof of convergence for high order schemes \cite{Tadmor1998}. Similarly, to prove the convergence of their numerical approximation likely requiresa  better understanding of the mathematical properties of systems of conservation laws. In light of a high demand for a practical approach, however, a more heuristic approach has been employed to the study of HRSC schemes for systems of conservation laws. And usually a starting point for such study is a one-dimensional scalar case for which TV-stability and convergence can be more easily explored.

\subsection{Finite-Volume Methods}

\begin{sidenote}
    \textbf{Reimann problem} \\
    Reimann problem is a specifit initial value problem composed of a conservation equation together with piece-wise constant initial data which has a single discontinuituy in the domain of interest. In numerical analysis it appears naturally in finite volume method for the solution of conservation law equations due to the discreteness of the grid. 
    \textbf{Reimann solver} \\
    A Reimann solver is a numerical method used to solve a Reimann problem. Generally speaking, Reimann solvers ar especific methods for computing the numerical flux across a discontinuityin the Riemann problem. They are wifely used in high resolution schemes. Usually, left and right states for the reimann Problem are calculated using some form on non-linear reconstraction, such as flux limiteror WENO method, and then used as inut for the reimann solver. \\
    An iterative solution to the RP is too costly, especially in MHD. Popular approximations are:
    \begin{itemize}
        \item Roe solver - using linearisation of the Jacobian, which then is solved exactly \\
        \item HLLE solver - approximate solution to the RP which is only based on the integral form of the conservational las and the largest and smallest signal velocities at the interface.
        \item HLLC solver - resotores the missing Rarefaction wave by some estiamtes, line linearesations. Efficient, But more diffusive. 
        \item Rotated-hybrid Riemann solvers. 
    \end{itemize}
\end{sidenote}

The finite-volume method, the Godunov scheme \cite{Godunov:1959}, was one of the fist \textit{monotonicity-preserving schemes}\footnote{Different from the monotonic schemes discussed above} that was able to yield solutions with discontinuities without spurious numerical extrema and with minimum numerical dissipation. There is a plethora of variations of Godunov-type, methods for conservation laws, built on top of the original method, see \textit{e.g.,} \cite{Toro:1999} for an up-to-date overview of the subject. \\
In this section we consider the basic Godunov method for the first order schemes. Then, we iterate on the second-order Godunov-type schemes, that are the most widely used in the field of relativistic hydrodynamics. In the end of the subsetion, we elaborate on the extension of FV methods to even higher orders, and pint out the difficulties arising in application to the relativistic hydrodynamics 



\subsubsection{The Godunov Method}
\textcolor{red}{my overall understanding of this method is rather poor and thus I copied David. It needs to be revised more.}

The pioneer work by Godunov \cite{Godunov:1959}, established principles that most modern hock-capturing methods are based upon. Studying the numerical approaches to linear-advection equation, the author showed that all monotonicity-preserving schemes for this equation are at most first-order accurate. The linearity the discrimination scheme was a basic assumption in this proof. As it was shown later, this assumtion has to be lifted to create a higher-order, monotonicity preserving schemes can be constructed. Boris \cite{Boris:1971} and van Leer \cite{vanLeer:1973} showed that a higer order scheme necessarily have to be nonlinear, even for linear equations. \\

In his work, Godunov has elaborated on the advantages of using the first-order upwind algorithm for the advection equation. In addition, author suggested a way to extend the method to non-linear case, establishing the Godunov scheme. \\

Let us consider a simple case of the one dimensional scalar hyperbolic equation, the advection equation

\begin{equation}
    \partial_t u + \partial_x f(u) = 0.
    \label{eq:theory:fv:adveq}
\end{equation}

that we cast on a uniformly-spaced grids in space and time 

\begin{equation}
    x_i = i\Delta^1, \hspace{5mm} i\in\mathbb{Z}, \hspace{10mm} t_n = n\Delta^0, \hspace{5mm} n\in\mathbb{N}
\end{equation}

Let us now consider control volumes, which in one dimension reduce to $[x_{i-1/2},x_{i+1/2}]$, over which the averages of $u$ read

\begin{equation}
    \bar{u}_i ^n = \frac{1}{\Delta^1}\int_{x_{i-1/2}}^{x_{i + 1/2}} u(x, t^n) \text{d}x.
\end{equation}

Averaging the avdection equation \ref{eq:theory:fv:adveq} over one control volume and one time-step we obtain

\begin{equation}
    \frac{\bar{u}_{i}^{n+1}-\bar{u}_{i}^{n}}{\Delta^0} = \frac{1}{\Delta^0}\int_{t_n}^{t_{n+1}}\big\{f[u(t,x_{i-1/2})] - f[u(t,x_{i+1/2})]\big\}\text{d}t
    \label{eq:theory:fv:intadveq}
\end{equation}

without making any approximation. We note that the computation of integrals on the r.h.s. of \ref{eq:theory:fv:intadveq} only requires to solve a sequence of Riemann problems centered at the intefaces between control-volumes.\\
To construct a FV scheme, we now approximate the integral from of the conservation law \ref{eq:theory:fv:intadveq}. At each times-step we heve an approximate solution $\{U_{i}^{n}\}_{i\in\mathbb{Z}} \approx \{\bar{u}_{i}^{n}\}_{i\in\mathbb{Z}}$ and evaluate the solution at the next time-step, $\{U_{i}^{n+1}\}_{i\in\mathbb{Z}}$. The assumption, introduced by Godunov, that allows to obtain the solution is the following. Assume that a solution can be represented as a piece-wise constant, \textit{i.e.,}

\begin{equation}
    U^n(x) = \sum_{i\in\mathbb{Z}}U_i ^n \chi_i (x)
\end{equation}

where $\chi_i(x)$ is the cahracteristic function of the control volume, \textit{i.e.,}

\begin{equation}
    \chi_i(x) = 
    \begin{cases}
    1, \text{ if } x\in[x_{i-1/2},x_{i+1/2}] \\
    0, \text{ otherwise, }
    \end{cases}
\end{equation}

then, for a sufficiently small time-step $\Delta^0$ the integral form of advection equation \ref{eq:theory:fv:intadveq} can be solved exactly. This is allowed as the Riemann problems, needed to compute the integrals in the r.h.s. of \ref{eq:theory:fv:intadveq}, are centered about the interfaces between adjacent control-volumes. Solution of these Riemann problems yields $u_{t, x_{i-1/2}}$ for all $i$ and all $t\in[t_n, t_{n+1}]$. The condition, that has to be satisfied for a time-step is the CFL condition, 

\begin{equation}
    \text{CFL}:=\frac{\Delta^0}{\Delta^1}\leq\frac{1}{c},
\end{equation}

where $c$ is the maximum propagation speed. \\
It allows for the interface value of the solution of the various Riemann problems to be computed exactly for most conservation laws, and be independent from each other. \\

The time integraion in this scheme is drasitcally simplifed as the $u(t, x_{i-1/2})$ does not depend on time \cite{LeVeque:1992}. \\

Let us summarize the the Godunov scheme

\begin{itemize}
    \item Given the solution at time $t= t_n$ as $\{U_i ^n\}_{i\in\mathbb{Z}}$, a piece-wise constant function $U^n(x)$ is constructed, such that it is only non-zero within a given control volume, \textit{i.e.,} $U^n(x) = U_i ^n$ if $x\in[x_{i-1/2}, x_{i+1/2}]$
    \item Then, the integral from of the advection equation \ref{eq:theory:fv:intadveq} is evolved exaclty for one time-step with initial data given by $U^n(x)$ to obtain $\{U^{n+1}_i\}_{i\in\mathbb{Z}}$ 
\end{itemize}

It is important to underline, that the assumption that the solution can be represented as a piece-wise constant in each control-volume, is the sole approximation of the Godunov scheme. \textcolor{red}{grphical representation... do I need it?} \\

Let us now focus on the r.h.s. of the equation \ref{eq:theory:fv:intadveq}. The solution $u(t, x_{i-1/2})$ depends on $U_{i-1} ^n$ and $U_{i} ^n$ as $u(t, x_{1/2}) =: u^* (U_{i-1}^{n}, U_{i}^n)$. It is convenient to express this dependence via \textit{numerical flux} $F(U_{i-1}^{n}):=f[u^*(U_{i-1}^{n}, U_{i}^{n})]$. The Godunov scheme is them reads

\begin{equation}
    \frac{U_{j}^{n+1} - U_{j}^{n}}{\Delta^0} = \frac{1}{\Delta^1}[F(U_{i-1}^{n}) - F(U_{i}^{n}, U_{i+1}^{n})].
    \label{eq:theory:fv:discrete}
\end{equation}

or, in the semi-descrete form:

\begin{equation}
    \frac{\text{d}U_i}{\text{d} t} = \frac{1}{\Delta^1}[F(U_{i-1}, U_{i}) - F(U_i, U_{i+1})].
    \label{eq:theory:fv:semi-discrete}
\end{equation}

To compute \ref{eq:theory:fv:semi-discrete}, a time integrator has to be chosen. The simplest option is the Euler method, which incidentally, transforms \ref{eq:theory:fv:semi-discrete} into its origignal form \ref{eq:theory:fv:discrete}. Time integrators of higher orders are not needed in this case, as the Godunov method is already exact with respect to time-update. \\

The semi-discrete form is overall useful in contracting higher order FV schemes as well as in discussing a coupling between hydrodynamic equations with some other system of equations \textit{e.g.,} the spacetime evolution equations, that is not solved using a FV scheme. We thus shall restrain ourselved to the discussion in semi-discrete form, leaving a complete discussion of FV methods for the sake of brevity. We refer the reader to \textit{e.g.,} \cite{Toro:1999} for a comprehensive description of FV methods and to \cite{Gassner:2011} for a more recent discussion regarding the higher order time discretization methods for FV. \\ 

Now, let us consider three dimensional Cartesian grid 

\begin{equation}
    \boldsymbol{x}_{i,j,k} = (i\Delta^1,j\Delta^2,k\Delta^3), \hspace{5mm} i, j, k \in \mathbb{Z},
\end{equation}

there the integral from of the FV scheme of the advection equation reads

\begin{align}
    \frac{\text{d}U_{i,j,k}}{\text{d}t} &= \frac{1}{\Delta^1}[F^1(U_{i-1,j,k}, U_{i,j,k}) - F^1(U_{i,j,k}, U_{i+1,j,k})] \\
    &+ \frac{1}{\Delta^2}[F^2(U_{i,j-1,k}, U_{i,j,k}) - F^2(U_{i,j,k}, U_{i,j+1,k})] \\
    &+ \frac{1}{\Delta^3}[F^3(U_{i,j,k-1}, U_{i,j,k}) - F^3(U_{i,j,k}, U_{i,j,k+1})]
    \label{eq:theory:fv:1storder3dscheme}
\end{align}

where $F^1$, $F^2$ and $F^3$ are the numerical fluxes associated with $f^1$, $f^2$ and $f^3$ respectively. \\


Extending the FV scheme to the multi-dimensional case and to a general unstructured grid, in a semi-discrete from we gain

\begin{equation}
    \frac{\text{d}U_a}{\text{d}t} = \frac{1}{|\Omega|}\int_{\partial\Omega}\boldsymbol{g}\cdot\boldsymbol{\nu}\text{d}\Sigma,
\end{equation}

where$\Omega_a$ is a control volume, $\boldsymbol{\nu}$ is the inwards pointing normal to $\Omega$.

As long as the solution of the Riemann problem can be constructed, the FV scheme can be applied to a system of equations. \\

%It is important to mention, that as at every time step we average the solution, $U^n(x) = \Sigma_i U_i ^n \chi_i(x)$, we destroy most of the information regarding full solution of the Riemann problem at every interface. Thus, a construction of a monotone FV scheme would required increasing the diffusivity of the method, even with the approximate Reimann solvers \cite{Harten:1983}. In case of a relativistic Euler equations, where the solution of the Reimann problem is complex, requiring iterative procedure, this property as shown be very usefull \cite{Marti:1994,Pons:2000,Giacomazzo:2005jy}. 
A popular examples or such solvers are HLLE solver \cite{Roe:1981}, the Marquina flux-formula \cite{Donat:1996} and HLLE solver \cite{Einfeldt:1988}. However, not all the flux-formulas constructed starting from an approximate solution of the original Reimann problem, \textit{i.e.},

\begin{equation}
    F(U_L, U_R) = f(u^*(U_L, U_R)), 
\end{equation}

even through being often called "approximate Riemann solvers". The $U_L$ and $U_R$ here are the left and right states across the interface. In certain cases there is a they approximate the flux-function directly. It has been further show by Harten et al. \cite{Harten:1983}, that a scheme is consistent and conservative\footnote{In particular the Lax-Wendroff theorem holds.} even if it is constructed via approximating numerical flux instead of the Godunov one, if the collowing criterion is satisfied by the flux formula

\begin{equation}
    F(u, u) = f(u) \text{ for any } u\in \mathbb{R}.
\end{equation}

Thus, a scheme converges to the correct entropic solution of the conservation law if it is non-linearly stable and the flux-formula is compatible with the entropy inequality. \\

Notably, higher order schemes have lower numerical dissipation. Thus, in high order FV schemes use of approximate flux is more justified.


\subsubsection{TVD Finite-Volume Methods}

The loss of information regarding the solution of the Reimann problem in the averaging procedure prevents us from constracting higher order FV scheme. However, it was shown, that the information on the solution can be \textit{reconstructed} from the averages $\{U_i ^n\}_{i\in\mathbb{Z}}$ using non-linear reconstruction procedure. This prevents spurious oscillations from arising. The procedure can be outlined as a following. Consider a solution withing a given control volume $U(x)$. Instead of setting $U(x) = U_i$ when $x\in[x_{i-1/2},x_{i+1/2}]$, the second order approximation of $u$ is used as 

\begin{equation}
    U_i(x) = U_i + \sigma_i(x-x_i), \hspace{10mm}x\in[x_{i-1/2},x_{i+1/2}]
\end{equation}

where $\sigma_i$ is the reconstructed slope in $[x_{i-1/2},x_{i+1/2}]$. \\

The $U_i(x)$ is essentially a profile of the solution $U$. We however set it with index $i$ as the profiles of two different control volumes might not agree at the interface between them. It is thus conveneitn to introduce the $\pm$ to indicate the from what side the slope is considered, talking about the inteface \textit{i.e.,} $U^{\pm} _{i-1/2}$ stands for $U_{i-1}(x_{i-1/2})$ and $U_i(x_{i-1/2})$. Applying this to the semi-discrete FV scheme we obtain 

\begin{equation}
    \frac{\text{d} U_i}{\text{d}t} = \frac{1}{\Delta^1}\big[F(U^{-}_{i-1/2},U^{+}_{i-1/2}) - F(U^{-}_{i+1/2},U^{+}_{i+1/2})\big]
    \label{eq:theory:fv:semi-disc-2ord}
\end{equation}

This is allowed as long as we are considering a semi-discrete formulation, as in this case the the instantaneous flux is given by $F(U_L, U_R)$, \textit{i.e.,} the solution at the interface between two cells at time $t_n ^+$ depends only on the jump between the two state $U_L$ and $U_R$. Thus, the second order scheme in time and space can be constructed, if a second order time-integrator is used in \ref{eq:theory:fv:semi-disc-2ord}. This is no longer the case in a fully descrete scheme, as $u(t, x_{i-1/2})$ is no longer time-independent. There, a solution of a generalized Riemann problem\footnote{A problem, with initial data having a piecewise linear profile} a use of predictor-corrector approach to compute the fluxes with second-order accuracy in time. \\

Now let us consider the choice of $\sigma_i$. It can be shown that if 

\begin{equation}
    \text{TV}\Bigg[\sum_{i}U_{i}(x)\chi_{i};\mathbb{R}\Bigg] \leq \text{TV}\Bigg[\sum_{i}U_{i}\chi_{i}(x);\mathbb{R}\Bigg]
\end{equation}

then the scheme \ref{eq:theory:fv:semi-disc-2ord} is TVD \cite{LeVeque:1992}. To satisfy this condition a special limiter, \textit{slope-limiter} has to be employed. The reason for that is that the scheme has to be limited to the first order near shocks to prevent oscillations. Consider an example a of a particular limiter, the \textit{minmod} limiter

\begin{equation}
    \sigma_i = \frac{1}{\Delta^1}\text{minmod}(U_{i+1} - U_{i}, U_{i}-U_{i-1}),
\end{equation}

where

\begin{equation}
    \text{minmod}(z_1,...,z_n) = 
    \begin{cases}
        \text{min}_i z_i \text{ if } z_i > 0 \text{ for any } i, \\
        \text{max}_i z_i \text{ if } z_i < 0 \text{ for any } i, \\
        0,  \text{               otherwise }.
    \end{cases}
\end{equation}

The minmod$2$ reconstruction is an another example 

\begin{equation}
    \sigma_i = \frac{1}{\Delta^1}\text{minmod}\Big[2(U_{i+1} - U_i), \frac{U_{i+1} - U_{i-1}}{2}, 2(U_i - U_{i-1})\Big].
\end{equation}

Notably, most of the slop-limiting techniques are only first order accurate at extrema. \\

The extension of a TVD FV scheme to a multi-dimensions is a Cartesian grids is done via a direction-by-direction approach. For an unstructured grid, however, the reconstruction in barycentric coordinates has to be performed. 

To extend the sheme to a system of equations the reconstraction can be performed component-vise. In case of the the relativistic Euler equation, the fact that $\bar{u}_i = u(x_i)$ is of second order accuracy is often unitized. It is usually conventient and less computationally expensive to perform the reconsturction in primitive variables, as the vallues at the interface are less likely to be unphysical. 

\textcolor{red}{what is actually reconstraction?..}

\subsubsection{Higher-Order Finite-Volume Methods}

In the previous subsection we showed that the extension of the Godunov scheme to second order can be achieved via introducing the profile $\sigma_i$, or a slope. This we effectively performed a linear regression of the solution $u$. This can be extended to a polynomial regression, to express the solution $u$ as a piecewise polynomial. The reconstruction however, becomes more involved.

\paragraph{Reconstruction Operators}

Let us beging with a simple one-dimensional case and expand on it later. Let $\upsilon(x)$ be a generic function whose volume average is 

\begin{equation}
    \widetilde{\upsilon}_i := \frac{1}{\Delta^1}\int_{x_{i-1/2}}^{x_{i+1/2}}\upsilon(x)\text{d}x.
\end{equation}

and consider how it can be reconstructed at higher orders. A non-linear operator  $\mathcal{R}$ that allows that returns a high-order approximation of $\upsilon$ at a given point $x$ from $\widetilde{\upsilon}$ volume averages, is a reconstruction operator. As, in general, function $\upsilon(x)$ can contain discontinuities, lift-biased ($\mathcal{R}^-$) and right-biased ($\mathcal{R}^+$) reconstruction operators. Consider an $\mathcal{R}$ reconstraction operator of order $r$, that acts on a set of averages $\{\widetilde{\upsilon}_i\}$, then

\begin{align}
    [\mathcal{R}^{-}(\{\widetilde{\upsilon}_{i}\})](x) &= \lim_{y\rightarrow x^{-}} \upsilon(y) + \mathcal{O}(\Delta^r), \\
    [\mathcal{R}^{+}(\{\widetilde{\upsilon}_{i}\})](x) &= \lim_{y\rightarrow x^{+}} \upsilon(y) + \mathcal{O}(\Delta^r).
\end{align}

The result of acting with $\mathcal{R}^-$ ($\mathcal{R}^+$) on the $\upsilon$ is then at $x_{i+1/2}$ is $\upsilon^{-}_{i+1/2}$ ($\upsilon^{+}_{i+1/2}$). Examples of reconstruction operators are: the piecewise parabolic method (PPM) \cite{Colella:1984,Colella:2008}, the piecewise hyperbolic method (PHM) \cite{Marquina:1994}, the essentially nonoscillatory (ENO) \cite{Harten:1987,Shu:1988,Shu:1989}, weighted essentially non-oscillatory (WENO) \cite{Liu:1994,Jiang:1996} and monotonicity-preserving (MP5) \cite{Suresh:1997} algorithms. \\
The PPM reconstruction is based on the piece-wise parabolic interpolation, that extends the slope-limiter approach. Series of limiters used in PPM have been extensively investigated for the presence of spurious oscillations, and even though this method has not been shown to be TVD, it is widely used in many FV schemes and in numerical relativistic hydrodynamics \cite{Baiotti:2004wn,Mignone:2005ns}\\
The ENO reconstruction scheme employs standard Lagrange interpolation. It can, in principle  be extended to any order of as it utilizes the recursive procedure. By selecting a stencil that yields the smoothest solution it allows to avoid Gibbs oscillations due to interpolation across discontinuities. And while the ENO scheme is not TVD, it was shown that 

\begin{equation}
    \text{TV}[\mathcal{R}(\{\widetilde{\upsilon}_i\}); \text{I\!R}] \leq \text{TV}[\{\widetilde{\upsilon}; \text{I\!R}\}] + \mathcal{O}\big((\Delta^1)^r\big)
\end{equation}

where $r$ is thge order of reconstruction \cite{Harten:1987}. \\

In the WENO scheme a weighted average of the reconstructed polynomial on each stencil is taken instead of selecting the one with which yields smoothest solution. The order of accuracy is maximized in smooth regions, while non-smooth stencils are suppressed through small weights. Thus, by combining results from multiple reconstructions, it achieves the $2r-1$ order of accuracy (where $r$ is an order of accuracy of ENO scheme). This is a modified ENO approach that considerably reduces the computational resources needed, as the conditional statements are removed. Generally, WENO schemes of order below 7 ($r\leq 4$) are used. Higher order schemes have shown to be unstable without special order-reducing techniques \cite{Gerolymos:2009,Tchekhovskoy:2007zn} or additional limiters \cite{Balsara:2000}.\\
Examples of the WENO schemes,with respect to how the weights are constructed are the following. In mapped-WENO scheme reduced dissipation is achieved via mapping procedure \cite{Henrick:2005}. In WENOZ scheme, improved non-linear weights are prescribed, that yields a results comparable with the fifth order mapped-WENO scheme, while being less computationally expensive. 

In a extensive study, Gerolymos et al. \cite{Gerolymos:2009} have compared various WENO schemes, providing tabulated coefficients and implementation details.\\

The basic WENO reconstruction yilelds the smoothosts soluton, utililizing the smoothness indicator, that, in case of the smooth solution maximizes the order of accuracy, while minimizing it when the discontinuities are detected. The final solution is then an weighted average of a set of lower order reconstructions of $\widetilde{\upsilon}_i '$ on numerous overlapping stencils. A different approach in employed in a bandwidth-optimized WENO schemes, where wieghts are adjusted to minimize the attenuation of high-frequency modes, instead of maximising the smopoothness of the solution. \\
In the basic WENO scheme, the smooth function is constructed in a ways as to much as many terms as possible in a Teylor decomposition of a target function. In a bandwidth-optimized WENO scheme, the function is constracted in a way as to provide the best approximation to the Fourier coefficients of the targeted function. In addition, "over-adaptation" is prevented by using the modified non-linear smoothness indicators. This allows to reduce the numerical dissipation. We refer the reader to \cite{Martin:2006} and \cite{Taylor:2007} for more detailed discussion. \\

The MP5 scheme utilizes the fifth order reconstraction alongside with flattening prcedure, that prevents the artificial extrema in the targeted function from arising. Esseincially, it is a monotonicity-preserving PPM scheme, extended to fifth order. By design, the reconstraction sheme, does not introduce spurious oscillations. This is hiwever, not certain in case of WENO schemes. Meanwhile, the MP5 scheme contains multiple conditional stataemtens in limiting procedure, that are absent in WENO.

\paragraph{Very-High-Order Finite-Volume Schemes}

A high order version of FV method in 1D can be constracted following the procedure we outlined in desiging the second-order FV scheme. Making use of high order, non-oscillatory reconstruction operators $\mathcal{R}$, that we split into $\mathcal{R}^+$ and $\mathcal{R}^-$, that when acting on a suluition $U$ yield $U^+$ and $U^-$, we write 

\begin{equation}
    \frac{\text{d}U_i}{\text{d}_t} = \frac{1}{\Delta^1}[F(U^{-}_{i-1/2},U^{+}_{i-1/2}) - F(U^{-}_{i+1/2},U^{+}_{i+1/2})],
\end{equation}

in semi-discrete form. \\
An such a scheme would have the order of accuracy equal to the one of the reconstruction algorithm. However, as the algorithms always fall to the first order near in the vicinity of discontinuities, the actual accuracy order would depend on the solution. \\

A seemingly simple approach of extending high order FV to multi-dimensional case, however, faces a serious challenge. In the formula \ref{eq:theory:fv:1storder3dscheme} we have use simple fluxes, which for a high order schemes have to be computed via suitable quadrature formulas. This makes the sheme complex. \\

In addition, with respect to the system of equations, to avoid spurious oscillations in higher order schemes, the reconstruction needs to be performed on local characteristic variables, instead of previously mentioned, component-vise approach. \\

Moreover, as it is generally not possible to preserve higher order accuracy in conversion from volume-averaged conserved variables to volume-averaged primitive ones. Thus, the reconstruction have to be done in terms of the former. \\

The high-order FV schemes coupled with general relativity become numerically very expensive. The reason for that is that they require high-order quadrature of the metric source terms as well as high-order
schemes to interpolate the metric at the quadrature points for the calculation of the fluxes. \\

Overall, even though considerable complexities involved, high order FV schemes, due to their unmached accuracy in compariosn with other second-order schemes, make them partular usefull in multiple applications in Newtonian and relativistic hydrodynamics, (see \textit{e.g.} \cite{Tchekhovskoy:2007zn}) and in unstractured grid applications, using generalized WENO scheme (see \textit{e.g.,} \cite{Dumbser:2007})

\subsection{Central Methods}

A central scheme is a scheme the monotone scheme that does not requrie aa Reimann solver. First such scmee was developed by Lax and Friedrichs (LxF) \cite{Lax:1954,Friedrichs:1954}. It has however a larger numerical dissipation in comparison to Godunov scheme, but it is simpler to implement and less expensive. Its lack of accuracy due to numerical dissipation prevents this scheme from being a popular choice. \\
A second order central scheme was developed by Nessyahu and Tadmor (NT) \cite{Nessyahu:1990}. Kurganov and Tadmor (KT) \cite{Kurganov:2000} have further advanced the scheme, which populirized high-order central schemes. \\
The Lax-Friedrichs scheme can be understood when a \textit{dual grid} is introduced into the Godunov scheme as follows. Consider a time $t_n$, and the numerical solution $U^n$, its local averages are then given by $\{U_{i}^{n}\}_{i\in \mathbb{Z}}$. To obtain a solution at the time $t_{n+1}$, we compute the local averages of the solution on a dual grid $\{x_{i+1/2}\}_{i\in\mathbb{Z}}$, \textit{i.e.,} $\{U_{i+1/2}^{n+1}\}_{i\in\mathbb{Z}}$ in the center. Hence, the name of the scheme. The following steps go through alternating between the \textit{primal} and \textit{dual} grids. \\
In more detains, consider a solution $\{U_{i}^{n}\}_{i\in \mathbb{Z}}$ as a starting point. To obtain $\{U_{i+1/2}^{n+1}\}_{i\in \mathbb{Z}}$, we employ the Godunov scheme for the advection equation \ref{eq:theory:fv:adveq} 

\begin{equation}
    \frac{U_{i+1/2}^{n+1} - U_{i+1/2}^{n}}{\Delta^0} = \frac{1}{\Delta^1}\int_{t_n}^{t_{n+1}}\big\{ f[u(t,x_i)] -f[u(t,x_{i+1})]  \big\}\text{d}t.
    \label{eq:theory:fv:central:intadveq}
\end{equation}

Note,  solution at cell centers remain free of discontinuities for duration of a timestep if the CFL $\leq 1/2 a$, $a$ is the maximum local-characteristic speed. Thus the r.h.s of \ref{eq:theory:fv:central:intadveq} can be computed via a simple quadrature in time, without Reimann solvers. \\

Let us consider a solution, $U^n(x)$, that line in Godunovscheme, is a piecewise constant 

\begin{equation}
    U^{n}(x) = \sum_i U_{i}^{n}\chi_i(x),
\end{equation}

then $u(t,x_i) = U_{i}^{n}$ for $t\in[t_n. t_{n+1}]$ and 

\begin{equation}
    U_{i+1/2}^{n} = \frac{U_i ^n + U_{i+1}^n}{2}
\end{equation}

and the \ref{eq:theory:fv:central:intadveq} reads as

\begin{equation}
    U_{i+1/2}^{n+1} = \frac{U_i ^n + U_{i+1}^n}{2} + \frac{\Delta^0}{\Delta^1}[f(U_i ^n) - f(U_{i+1} ^n)]
\end{equation}

\textcolor{red}{graphical representation -- fig 3.2 -- is it needed?}

Nessyahu and Tadmor \cite{Nessyahu:1990} have introduced the second order central scheme. Instead of piecewise constant reconstruction, they employed a minmod to obtain a piecewise linear approximation of $u^n$, and for time flux quadrature, they used a predictor-corrector approach. \\

The dissipation of of the LxF and NT schemes is time-step depended. For a small $\Delta^0$ it makes schemes very dissipative. To improve the accuracy of the scheme Kurganov and Tadmor \cite{Kurganov:2000} proposed to limit the averaging on the dual grid to the region of spacetime spanned by the largest characteristics of the Riemann problem. Thus, while in the LxF and NT schemes the averging of the solution was supported in the whole grid, in the KT scheme, $U^{n+1}_{i+1/2}$ is only allowed in the region where affected by the results of the local Riemann problem. This reduces the dissipation and also allows the semi-descrete formulation 

\begin{equation}
    \frac{\text{d} U_i}{\text{d} t} = \frac{1}{\Delta^{1}}\big[F(U_{i-1/2}^{-},U_{i-1/2}^{+}) - F(U_{i+1/2}^{-},U_{i+1/2}^{+})\big],
\end{equation}

where $F$ are numerical fluxes \textit{i.e.,}

\begin{equation}
    F(U_L, U_R) = \frac{f(U_L) + f(U_R)}{2} - \frac{a}{2}[U_R - U_L].
    \label{eq:theory:fv:central:fluxes}
\end{equation}

Essentially, the KT scheme is very similar to the Godunov scheme with fluxes evaluated through equation \ref{eq:theory:fv:central:fluxes}, Rusanov fluxes \cite{Kurganov:2000}. This also assures the stability of the system when $1/2 < a \cdot \text{CFL} \leq 1$. However, in that case the interpenetration of the scheme is no longer applicable. \\

Lastly, we revisit the difference between the finite-volume (upwind) and central shemes. The main one lies in the constraction of these schemes, as in their form the latter can alwasy be regarded as a FV scheme with added Rusanov "Reimann solver". In particular, only for KT scheme, the maximum principle without additioanl "maximum-principle enforcing limiters" was proven in multi-dimensional case. \\


\subsection{Finite-Difference Methods}

In the high order upwind finite difference (FD) schemes the approximation is done point-wise instead of averaging over volumes as it is done in FV schemes. Interestingly, these schemes were proposed as better performing alternatives to ENO and WENO schemes of very high order. \cite{Shu:1988,Shu:1989,Jiang:1996}. Especially in the multi-dimensional case, FD schemes are much more superior to the high order FV schemes in turms of efficiency. \cite{Shu:1999,Shu:2003}. \\

Here we aim to discuss finite difference high resolution shock capturing schemes in very brief manner. We refer the reader to specialized literature for a in-depth discussion. In particular, for the discussion of FD ENO/WENO HRSC schemes see \cite{Shu:1999}, and for FD MP5 scheme -- see \cite{Mignone:2010}. \\

Let us start by introducing a system of hyperbolic balance-laws as

\begin{equation}
    \partial_t\boldsymbol{F}^0(\boldsymbol{u}) + \partial_i\boldsymbol{F}^i(\boldsymbol{u}) = \boldsymbol{S}(\boldsymbol{u}).
    \label{eq:theory:fd:hypsys}
\end{equation}

For the grid, we employ a uniform Cartesian for the ease of discussion 

\begin{equation}
    \boldsymbol{x}_{i,j,k} = (i\Delta^1, j\Delta^2, k\Delta^3), \hspace{5mm} i,j,k\in \mathbb{Z}.
\end{equation}

In the following discussion, we will not elaborate on the comparison between different representations of a solution \textit{i.e.,} exact, volume averaged, their approximations, reconstractions. We thus only need, for a quantity $u$, to define the numerical approxiamtion at a given point $x_{i,j,k}$, as $u_{i,j,k}$. 

For the \ref{eq:theory:fd:hypsys} the FD scheme reads

\begin{align}
    \frac{d\boldsymbol{F}^{0}_{i,j,k}}{dt} = \boldsymbol{S}_{i,j,k} &+ \frac{\boldsymbol{F}^{1}_{i-1/2,j,k} - \boldsymbol{F}^{1}_{i+1/2,j,k}}{\Delta^1} \\
    & + \frac{\boldsymbol{F}^{2}_{i,j-1/2,k} - \boldsymbol{F}^{2}_{i,j+1/2,k}}{\Delta^2} + \frac{\boldsymbol{F}^{3}_{i,j,k-1/2} + \boldsymbol{F}^{3}_{i,j,k+1/2}}{\Delta^3},
\end{align}

which is identical to the semi-descrete form of the finite volume scheme introduced before \ref{eq:theory:fv:1storder3dscheme}. However, there terms $\boldsymbol{F}^{1}_{i-1/2,j,k} - \boldsymbol{F}^{1}_{i+1/2,j,k}$ represented integrals of the control volumes along the boundary. If FD scheme, these are direct (high-order, non-oscillatory) approximations of the point-wise value of $-\partial_1 F^1$ at a point $x_{i,j,k}$. \\

Note, that as at the second order the volume averages and point-wise values are the same, the FD and FV schemes read identically. The difference becomes important at higher orders and in many dimensions, especially for the performance of a scheme. Consider the FD high order multi-dimensional scheme, At region boundaries, it does not require any quadrature, Reimann solvers or extra primitive recovery calls \textcolor{gray}{while still requiring reconstruction operators}. This makes a schemem much less computationally expensive. In addition, In general relativistic hydrodynamics, the source term treatment is simpler and more direct, as onlu the point-wise values are requried. This helped popularize the FD for GRHD applications. \\

For the following discussion on the computation of the descrete derivatives on the r.h.s onf the \ref{eq:theory:fd:hypsys}, let us first approach this task in case of a simple equation, the one-deminesiona advection eqiation \ref{eq:theory:fv:adveq}, which is also hyperbolic scalar equation. 

In FV scheme, the reconstraction operators are used to evaluate the left $U_L$ and right $U_R$ from which, via approximate Reimann solvers, the fluxes are computed after. In FD schemes the reconstruction operators are also used, but to evaulate the (non-oscillatory approximation) of $\partial_x f$, referenced above. It is convenient to express $f$ as follows

\begin{equation}
    f\big[u(x_i)\big] = \frac{1}{\Delta}\int_{x-1/2}^{x+1/2}h(\xi)d\xi,
    \label{eq:theory:fd:introd_h}
\end{equation}

where we, following \cite{Shu:1988}, employed function $h(x)$ in a manner that its average between $x_{i+1/2}$ and $x_{i-1/2}$ stands for the value of $f$ at $x_i$. Then, 

\begin{equation}
    \frac{\partial f}{\partial x}\Big|_{x_i} = \frac{h(x_{i+1/2}) - h(x_{i-1/2})}{\Delta}.
    \label{eq:theory:fd:diff_via_h}
\end{equation}

That that there were no approximations done in writing \ref{eq:theory:fd:introd_h} and \ref{eq:theory:fd:diff_via_h}. This allows to obtain an approximation to the derivative $\partial f/\partial x$ at $x_i$ of accuracy $r$, -- the same order as the reconstraction operator $\mathcal{R}$, that was used to recover $h_{i+1/2}$. In addition, as only the values at $x_i$ of $f$ are needed for computation, $h$ is never computed at any time. \\

The stability of the scheme is assured by correctly upwinding the reconstraction. Consider the case of $f'(u)>0$. Then, if we allow

\begin{equation}
    \widetilde{\upsilon}_i = f\big[u(x_i)\big] = \frac{1}{\Delta}\int_{x_{i-1/2}}^{x^{i+1/2}}h(\xi)d \xi
\end{equation}

and 

\begin{equation}
    f_{i+1/2} := \upsilon_{i+1/2} ^{-}, \hspace{10mm} f_{i-1/2} := \upsilon_{i-1/2} ^{-},
\end{equation}

then

\begin{equation}
    \frac{\partial f(u)}{\partial x} = \frac{f_{i+1/2} - f_{i-1/2}}{\Delta} + \mathcal{O}(\Delta^r)
\end{equation}

yields the required high order approximation to $\partial_x$ at $x_i$. \\

In a more general case, where the beahviour $f'(x)>0$ is not assured, the flux $f$ is split in a left-goping $f^{-}$ and right-going $f^{+}$ as $f = f^{+} + f^{-}$, for which separate upwind reconstructions are used to assure the stability. Note that this procedure is rather similar application of a Reimann solver in finite volume schemes. In some cases the direct correspondance between the flux-splitting methods and particualr Reimann solvers can be established. \\

We consider here tow ways, how the flux splitting can be done. The ones that are of relevents to this theiss are: \textit{Roe flux-split}, \textit{i.e.,}

\begin{equation}
    f = f^{\pm}, \text{  if  } [f'(\bar{u})]_{x_{i+1/2}} \lessgtr 0
    \label{eq:theory:fd:roefluxsplit}
\end{equation}

where $\bar{u}_{x+1/2} := (1/2) (u_i + u_{i+1})$, 

and the Rusanov flux-split \cite{Shu:1997} \textit{i.e.,}

\begin{equation}
    f^{\pm} = f(u) \pm \alpha u, \hspace{5mm} a \max[f'(u)],
    \label{eq:theory:fd:laxfluxsplit}
\end{equation}

where the maximum is taken over the reconstruction operator stencil. \\ 

With respect to the efficiency, the Roe flux splittes appears superiour, as it requires only one reconstraction. However, it was shown \cite{LeVeque:1992} that, if the transonic refraction waves are pesnet, this flux splitter can lead to the formation of entropy-violating shocks. In addition, the flux splitter is a odd-even decoupling phenomenon \cite{Quirk:1994}. One of the solutions was suggested in \cite{Radice:2012cu} to replace Roe flux split with Lax-Friedrichs one when $u$ or $f$ are not monotonic within the reconstruction stencil. 

It is important to note that in our discussion an implicit assumption is made that the function $f$ is convex. In case of relativistic hydrodynamics this is indeed so, however, -- not in the case of relativistic-magneto-hydrodynamics. \\

In addition, in \cite{LeVeque:1992}, a more strict condition imposed on the where Roe flux splitter has to be used instead of Lax-Friedrichs. \textcolor{red}{Our implementation is based on the experience, that a more frequent usage of Roe splitter, decreases comutational costs, as $u$ and $f$ are already computed on the grid, while $f'(u)$ is not. This appears to be sufficient to avoid odd-even decoupling in the tests performed. All the results shown in this thesis, obtain using this Roe-split with this "entropy fix" [This is David's work. Not theory. Not sure how to incorporate it.]} \\

Now, let us consider the general system of hyperbolic balance-laws \ref{eq:theory:fd:hypsys}. The spatial derivatives of the fluxes $\partial_{\alpha} F_{i,j,k} ^{\alpha}$ can be evaluated via a component-by-component approach using the algorithm described above. And while, this is sufficient for lower (up to the second) order schemes, in higher order ones it excites spurious numerical oscillations.
This problem can be alleviated if reconstraction is done on local characteristic variables of the ssystem. For simplicity, let us consider a one-dimensional case $\alpha={0,1}$. Then, the we employ Jacobian matrices

\begin{equation}
    \boldsymbol{A}^{\alpha} = \frac{\partial \boldsymbol{F}^{\alpha}}{\partial u}\Big|_{\bar{\boldsymbol{u}}}, \hspace{5mm} \alpha = 0,1,
    \label{eq:theory:fd:jacobreconstr}
\end{equation}

where 

\begin{equation}
    \bar{u} := \frac{1}{2}(u_{i,j,k} + u_{i+1,j,k}),
    \label{eq:theory:fd:aver_ubar}
\end{equation}

to reconstract the fluxes $F^1 _{i+1/2, j, k}$. The $\bar{u}$ is the average state at the point of reconstruction. \\

Notably, the average \ref{eq:theory:fd:aver_ubar} and present in \ref{eq:theory:fd:roefluxsplit} and \ref{eq:theory:fd:jacobreconstr} is considerably less complicated then the ones proposed in \cite{Roe:1981}. It was shown that the use of $\bar{u}$ \ref{eq:theory:fd:aver_ubar} instead of averaging proposed in \cite{Roe:1981} has no significant impact on the quality of the solution in FD schemes, even when relativistic case is concerned. \\

As the \ref{eq:theory:fd:hypsys} is strongly hyprebolic, the matrix $\boldsymbol{A}^0$ can be inverted. Then the generalized eigenvalue problem 

\begin{equation}
    [\boldsymbol{A}^1 - \lambda_{(1)}\boldsymbol{A}^0]r_{(I)} = 0,
\end{equation}

has only real eigenvalues $\lambda_{(I)}$ and $N$ real, independent, right-eigenvectors, $r_{(I)}$ [see, \textit{e.g.,} \cite{Anile:1990}]

Let the matrix of eigenvectors be $R$ as 

\begin{equation}
    R_{J}^{I} = r^{I}_{(J)},
\end{equation}

and $L$ its inverse. Then, the local characteristic variables are 

\begin{equation}
    \omega = Lu, \hspace{10mm} Q = LF^1
\end{equation}

and we perform component-wse reconstraction to obtain $Q_{1+1/2,j,k}$, where $\omega$ is used in palce of $u$ and $Q$ in place of $f$ in the \ref{eq:theory:fd:laxfluxsplit}. 

Then, we set

\begin{equation}
    F^1_{i+1/2, j, k} = RQ_{i+1/2, j, k}.
\end{equation}

The approximate of $\partial_a F^a$ in terms of $x_{i,j,k}$ then is obtained, repeating the procedure for other dimensions. \textcolor{red}{Results shown in this thesis are obtained, via reconstraction of local charactersitic variables.} \\

The major disadvantage of finite differencing schemes, however, is they they are primarely adopted for Cartesian grids (uiform or with Berger-Oliger-style AMR \cite{Berger:1984}). In case of cell-centered AMR with refluxing, \cite{Berger:1989}, the FD scheme reduce to second order at the boundary of refinment levels. \textcolor{red}{These limitations however are of second importance for the results presented in this thesis [Are they really?..]}

\subsection{Discontinuous Galerkin Methods}

For hyperbolic equations, with particular emphasis on neutron-transport, Discontinuous Galerkin (DG) Methods have been prposed by Reed and Hill \cite{Reed:1973}. Cockburn and Shu in a series of works extensively expanded on the topic \cite{Cockburn:1991,Cockburn:1989ii,Cockburn:1989iii,Cockburn:1990iv,Cockburn:1998v}. For the elliptic and parabolic equations, the method was also adopted, (see \textit{e.g.,} \cite{Arnold:2002} and references therein). The popularicy of methods have increases in recent years, as well as number of applications of GC methods to classical hyperbolic, parabolic and elliptic problems \cite{Cockburn:2000,Canuto:2008,Hesthaven:2007}. In addition, Zumbush \cite{Zumbusch:2009fe} and Field et al. \cite{Field:2010} have successfully applied the DG methods to the Einstein system of equations in vacuum. Finally, Radice et al. \cite{Radice:2011qr} proposed the first DG general-relativistic hydrodynamics code. \\

The rapid growth of DG methods popularity is largely attributed to their numerical properties which we are going to briefly outline here. \\

In addition, while DG methods are realtively young with respect to FV and FD, they have majority of mathematically proven properies. In particular, the intrinsic non-linear stability have been shown for DG methods for any order of accuracy. However, this required imposing limiters and/ro filtering, to treat the under-reslved parts inf the solution \textit{i.e.,} shocks. If the solution, however, is not well represented by the truncated expansion, the aliasing instability occures. For example, modes that are not present in the actual solution can be "aliased" into the evolved ones. Thus the high order modes can have thir content transfered to lower-order mdoes imporperly. This leads to a non-linear instabilities (see \textit{e.g.,} \cite{Boyd:2001}). In addiiton, it was shown that the solution obtained via DG methods is always entropic, as DG methods satisfy the cell-entropy inequality (see \textit{e.g.,} \cite{Cockburn:2003}). Recently, a maximum-principle-satisfying DG schemes were obtained by Zhang and Shu \cite{Zhang:2011}, via a special limiting technique. \\

Another important property of DG methods in the regions, where the solution is smooth, a high, spectral accuracy can be achieved. With respect to the accuracy of DG methods, it was shown that the numerical dissipation depends only on the truncation error \cite{Cockburn:2003}. This in turn implies that in regions where the solution is smooth and resolved, the dissipation is effectively turned off. This is of prime importance for problems where numerical diffusivity routinely orders of magnitude higher then the physical diffusivity, \textit{e.g.,} in transport problems, where it can render results completely wrong. \\

With respect to their practical use, the DG methods can be easily adopted to general unstructured grids. They are also belived to be very efficient in massively parallel computing owing to their compact stencils that grand them high stability \cite{Biswas:1994}. \\

A particular property that makes DG methods very compelling for GR applications, that they are covariant. DG (and their derivative finite-element method, FEM) do not depend on the a choice of the cooridnate system. Instead they can be described in terms of push-forward and pull-backs from a given reference element \cite{Meier:1999}. \\

\textcolor{gray}{In figure 3.3, the superiour accurady of DG methods is shown. There, for advection equation, the comparsion is made between 5th order DG scheme, 5th Order WENO5 scheme and 7th order WENO7 scheme. The former reins supreme even though the latter has a higher formal order of accuracy. For the comparison equal number of DF-points/DG-elements was taken, to assure the similar computational cost between schemes. However, it is important to note that formaly, DG method have more degrees of freedom (d.o.f) by 5 times with respect to FD methods. A commonly adopted method of comparions between these methods relies of setting equal number of d.o.f. However, this is David's opinion that it is misleading as FD method is much more expensive numerically then. The approach adopted by David allows to set the computational cost equal between the scheme. By setting equal number of FD points to the number of DG elements motivated by the fact that the computational costs of DG scales with number of elements similarly, as the cost of FV/FD methods scales woth the number of ceslls/points. Thus David's comparisons is govern by the scenario in which the computational time is similar. However, an equal number of d.o.f. for the comparison is useful to set, when the memory requirements are a limiting factor.}

Despite the large number of advantages, DG methods also have certain limitaitons. In particular, in comparison with FV/FD methods, they have larger memory requirements. In addition, with respect to shocks, all the standard flattening techniques ano not sufficintly reliable in redicing oscillations, or/and decrease the accuracy of the scheme in the smooth regions. However, arguably the most important limitation of DG methods, is that the linear stability requires a more strict CFL condition. For example if the strongly-stability preserving (SSP) time-discretization is adopted \textit{e.g.,} Runge-Kutta (RK) schemes \cite{Gottlieb:2009}, and the spatial discritisation of the same order, the stability condition reads

\begin{equation}
    \text{CFL} \leq \frac{1}{c}\frac{1}{2k + 1},
\end{equation}

where the $k+1$ is the order of accuracy of the scheme \cite{Cockburn:2001}. In recent works [259]\cite{Qiu:2005,Qiu:2004} a novel hybrid DG-FV scheme, DG-WENO was developed to combat this limitation. Similarly, a hybrid scheme $P_N P_M$ was introduced in \cite{Dumbser:2009,Dumbser:2008}. Also, advancments in local-spacetime schemes have been made \cite{Gassner:2011} (see also \cite{Hesthaven:2007} for alternative approaches)

\subsubsection{Runge-Kutta Discontinuous-Galerkin Methods}

In this subsection we are going to briefly touch on the fundamentals behind the so-called spectral discontinuous Galerkin methid with numerical integration (SDGM-NI) of the nodal-DG scheme, which is the widely used DR variant. For a more extensive discussion we direct the reader to \cite{Hesthaven:2007}. \\

Once again for simplicity, we shall focus on a one-dimensional hyperbolic equations, the advection equation with no sources \textit{i.e.,}

\begin{equation}
    \partial_t = \nabla\cdot\boldsymbol{f}(u) =0, \hspace{10mm} (t,x)\in \text{I\!R}_{+} \times\Omega,
    \label{eq:theory:dg:adveq}
\end{equation}

where $\Omega\subset \text{I\!R}^{d}$ is a bounded, regular domain. \\

Now, let us introduce a family of diffeomorphisms $\phi_{j}: T\rightarrow\Omega,\:\Omega_{j}=\phi(T)$ which performs the triangulation of $\Omega$. In other words, we introduce a number $N$ of \textit{elements}, that represent a union of images of a base element $T$. In 2D the base element is usually a triangle (or a square), while in 3D it is then a tetrahedron (cube). Then, the union of images satisfies

\begin{equation}
    \cup_{j=1}^{N}\Omega_{j} = \Omega, \hspace{10mm} \Omega_{i}\cap\Omega_{j} = \emptyset, \hspace{5mm} \text{if} i\neq k.
\end{equation}

\textcolor{red}{see example in figure 3.4}. 

The main idea behind the mapping is the following. This mapping allows to "pull-back" the equations from a given element, to a reference one, where all the descrete differential operators have been pre-computed. Thus, even through the spape of elements in the physical space can be arbitrary involved, the operators \textit{e.g.,} derivative,s interpolation and integration, need to be prescribed just one time for a simple geometry. This, for isntance, allows an iimplementation of elements with smooth boundaries. \\

In relativistic case, however, the scheme can be made effectively coordinate-free. This is achieved by utilizing covariance of equations, effectively casting them in coordinate system generated by diffeomorphisms (taking the one, in which the base element is defined and pushing if forward). \\


Next, we proceed with obtaining a condition that resembles a weak formualtion of \ref{eq:theory:dg:adveq}. The classical way of deriving a semi-descrete scheme is to first obtain a form of weak formualtion in which $u$ is supposedly a bounded function in space, to unsure the existance of a normal trace of $\boldsymbol{f}$ and a smooth function in time. \textcolor{gray}{Note, however, that in chapter 6, a somewhat different direction is chosen, when considering a case of general relativistic problems via a space-time approach, which is more native (adequate) for the relativistic equation. In addition, the condition on the solution to be BV is relaxed, as for a system of eqatuins it might appear too restrictive.} Here we limit ourselves to a classical approach, focusing on a simplifed case of a scalar equations. \\

Consider $\upsilon \in C_0 ^1(\Omega)$. Next, we multiply \ref{eq:theory:dg:adveq} by $\upsilon$ and integrate over $\Omega$, 

\begin{equation}
    \sum_{j=1}^{N}\Bigg[\int_{\Omega_j}\partial_{t}u\upsilon\text{d}x - \int_{\Omega_j}\boldsymbol{f}(u)\cdot\nabla\upsilon\text{d}x\Bigg] = -\sum_{j=1}^{N}\langle\boldsymbol{\mathcal{F}}\cdot\boldsymbol{\nu},\upsilon\rangle_{\partial\Omega_j}
    \label{eq:theory:dg:intformadveq}
\end{equation}

where $\boldsymbol{\mathcal{F}}\cdot\boldsymbol{\nu}$ is the normal trace of $\boldsymbol{f}(u)$ on the boundary, \textit{i.e.,} it represents the distribution, that value on any test function $\upsilon$, of which is 

\begin{equation}
    \langle\boldsymbol{\mathcal{F}}\cdot\boldsymbol{\nu},\upsilon\rangle_{\partial\Omega_j} := \langle\text{div}\boldsymbol{f},\upsilon\rangle_{\Omega_j} + \int_{\Omega_j}\boldsymbol{f}(u)\cdot\nabla\upsilon\text{d}x,
\end{equation} 

where $\div\boldsymbol{f}$ is the distributional divergence of $f$. Assuming that $\boldsymbol{f}$ is smooth 

\begin{equation}
    \langle\boldsymbol{\mathcal{F}}\cdot\boldsymbol{\nu},\upsilon\rangle_{\partial\Omega_j} = \int_{\partial\Omega_j}\boldsymbol{f}\cdot\boldsymbol{\nu}\upsilon\text{d}x,
\end{equation}

where $\boldsymbol{\nu}$ is an out-going norm to $\Omega_j$. \\
To obtain a weak formulation of \ref{eq:theory:dg:adveq}, then reduces to obtaning the 

\begin{equation}
    u\in\text{BV}(\Omega), \hspace{5mm} s.t., \ref{eq:theory:dg:intformadveq} \text{holds for any} \upsilon \in C_0 ^1(\Omega).
    \label{eq:theory:dg:weakformrquire}
\end{equation}

The mechanism of DG method, essencially, is to project the \ref{eq:theory:dg:weakformrquire} on a finite-dimensional subspace of BV$(\Omega)$. For isntance, consider a piecewise polynomial function of $\mathcal{T}_{N}$:

\begin{equation}
    V_N = \big\{ \upsilon\in\text{BV}(\Omega):\upsilon\circ\phi\in\text{I\!P}_{D}(T), \: j=1,...,N \big\}
\end{equation}

where $\text{I\!P}_{D}$ is the space of polynomials of degree $D$. Note, we do not actually require the continuity of functions in $V_N$ between elements. Hence the themse of this family of schemes \textit{discontinuous} Galerkin. Then, in order to constract a DG scheme we need to obtain

\begin{equation}
    u(t) = C^1(\text{I\!R}_{+};v_N), \hspace{10mm} s.t., \ref{eq:theory:dg:intformadveq} \text{ holds for any } \upsilon\in V_N.
    \label{eq:theory:dg:weakform}
\end{equation}

Up to now we were considering test functions that belong to the same functional space as the numerical solution, \textit{i.e.,} to the $V_N$. Such schemes, where the test function and numerical the numerical solution $u$ belong to the same functional space are called \textit{Galerkin methods}. Next, computation of the degrees of freedom of $u$ is now simplified since both $u$ and $\upsilon$ are from finite-dimensional space. This is done by assessing the condition \ref{eq:theory:dg:intformadveq} for a large but finite number of test functions $\upsilon$, linearly independent, of course. This finite number of conditions of the form \ref{eq:theory:dg:intformadveq} allows to determined the degrees of freeodom, as it generates a set of ODEs for d.o.f. of $u$. \\

Next, it is important to mention certain aspects of the weak formulation \ref{eq:theory:dg:weakform}. \\

The first point is related to the smoothness of the test function. The formulation \ref{eq:theory:dg:intformadveq} is not well defined for a non-smooth $\upsilon$. Thus, the DG formulations requires a following addition. The test function $\upsilon$ has to appear as smooth $C_0 ^1(\Omega)$ extension $\upsilon$ in $V_N$ for \ref{eq:theory:dg:intformadveq} to be well defined. Doing this, it has to be ensured that the one-sided limit of $\upsilon$ at $\partial\Omega_j$ from the interior of $\Omega_j$. \textcolor{red}{no clue what this means} This can we achieved by setting

\begin{equation}
    \langle\boldsymbol{\mathcal{F}}\cdot\boldsymbol{\nu},\upsilon\rangle_{\partial\Omega_j} := \langle\boldsymbol{\mathcal{F}}\cdot\boldsymbol{\nu},\upsilon_j\rangle_{\partial\Omega_j},
\end{equation}

where $\upsilon_j \in C_0 ^1 (\Omega)$ and $\upsilon|_{\Omega_j}=\upsilon$. \\

Secondly, let us discuss the normal trace. We simplify \ref{eq:theory:dg:intformadveq} by setting the test function $\upsilon = \chi_i$ and obtain

\begin{equation}
    \partial_t\int_{\Omega_j}u\text{d}x = - \langle\boldsymbol{\mathcal{F}}\cdot\boldsymbol{\nu},\upsilon\rangle_{\partial\Omega_j}
\end{equation}

which strikingly resembles the finite volume method. Thus, the normal trace cab be viewed as a flux computed using an approxmiated Riemann solver. \\

Now, let us consider the contraction of the DG scheme. For simplicity and brievity we limit ourselves to the one-dimensional case, reducing \ref{eq:theory:dg:intformadveq} to the 

\begin{equation}
    \sum_{j=1}^{N}\Bigg[\int_{x_j - 1/2}^{x_j +1/2}\partial_t u \upsilon \text{d}x - \int_{x_j - 1/2}^{x_j +1/2}f(u)\partial_x\upsilon\text{d}x\Bigg] = \sum_{j=1}^{N}\big[F^{j-1/2}\upsilon(x_{j-1/2}) - F^{j+1/2}\upsilon(x_{j+1/2})\big],
\end{equation}

where $F$ is now the numerical flux. Next, we peform an expansion on a polynomial basis on $u(t,\cdot)$ and $f[u(t,\cdot)]$ as

\begin{equation}
    u(t,x) = \sum_{i=1}^{D}u_{i}^{j}(t)l_{i}^{j}(x), \hspace{5mm} f(t,x) = \sum_{i=1}^{D}f_{i}^{j}(t)l_{i}^{j}(x), \hspace{5mm} x\in[x_{j-1/2}, x_{j+1/2}],
\end{equation}

where $l_{i}^{j}$ is some polynomial basis over $[x_{j-1/2}, x_{j+1/2}]$. Usually this basis is constracted from orthonormal polynomials, but this is not required. \\
Now, let us chose $\upsilon = l_{k}^{j}\chi_j$, $k=0,...,D$. This would yield a set of evolution equations for the expansion coefficients $u_{i}^{j}(x)$ \textit{i.e.,}

\begin{equation}
    \sum_{i=1}^{D}\Bigg[\int_{x_{j-1/2}}^{x+1/2}l_{i}^{j}(x)l_{k}^{j}(x)\text{d}x\Bigg]\frac{\text{d}u_{i}^{j}(t)}{\text{d}t} - \sum_{i=0}^{D}\Bigg[ \int_{x_{j-1/2}}^{x+1/2}l_{i}^{j}(x)\partial_{x}l_{k}^{j}(x)\text{d}x \Bigg]f_{i}^{j} = F^{j-1/2}l_{k}^{j}(x_{j-1/2}) - F^{j+1/2}l_{k}^{j}(x_{j-1/2}),
\end{equation}

which in more compact form read

\begin{equation}
    \boldsymbol{M}^{j} - \boldsymbol{D}^{j}\boldsymbol{f} = \boldsymbol{F}^{j-1/2} - \boldsymbol{F}^{j+1/2}, \hspace{5mm} \text{for any } j=1,...,N
\end{equation}

where vectors $u^{j}$ contain all the expansion coefficients of $u$ and introduced the following matrixes

\begin{equation}
    \big(\boldsymbol{M}^j\big)_{ki} := \int_{x_{j-1/2}}^{x+1/2}l_{i}^{j}(x)l_{k}^{j}(x)\text{d}x
\end{equation}

which is the mass matrix, 

\begin{equation}
    \big(\boldsymbol{D}^{j}\big)_{ki} := \int_{x_{j-1/2}}^{x+1/2}l_{i}^{j}(x)\partial_{x}l_{k}^{j}(x)\text{d}x
\end{equation}

which is the co-differential matrix, and finally 

\begin{equation}
    \big(F^{j-1/2}\big)_i := \sum_{k} \delta_{ik} F^{j-1/2}l_{k}^{j}(x_{j-1/2})
\end{equation}

which are the flux vectors.\\

It is important to note several things. The mass-matrix $\boldsymbol{M}$ is defined on a single element, \textit{i.e.,} it is local. It can be made diagonal, if the accuracy of integration is sufficient and the orthonormal basis is chosen. This simplifies the time evolution as otherwise, the $\boldsymbol{M}$ has to be inverted. This however, is not expensive doe to locality of $\boldsymbol{M}$. There are also alternative methods on diagonalizing the $\boldsymbol{M}$, \textit{e.g.,} mass-lumping \cite{Canuto:2008}.  \\

next, Owing to the fact that $f(u)$ is a general, non-linear function, expanding the flux requires sufficiently large number of coefficients and an adequate quadrature formula to evaluate $\boldsymbol{D}^j$. Otherwise an aliasing error is being introduced. In most practical application it indeed arises and somwhat metigates the non-linear stability of the scheme. If the solution is smooth however, it can be suppressed via filtering techniques \cite{Hesthaven:2007}. 
Other source of the aliasing error is a shock, \textit{i.e.,} if $u$ has a jump discontinuity. Then, even in a overal linear case, the aliasing error arises, requiring a special techniques to reduce it. One of the commonly adopted methods to do so is to employ a minmod limiter as a non-linear filter to flatten the profile of the solution within the element that contains discontinuities \cite{Cockburn:2001}. \textcolor{gray}{see more in chapter 6}

\chapter{Finite-Differencing Methods: Flat Spacetimes}

\section{Introduction}

\begin{sidenote}
    \textbf{David:}  \\
    
    I've been reading that for high-resolution shock-capturing, the finite differencing techniques are more efficient and simpler in implementation. However, most codes for MHD and radiation MHD that I find are using finite-volume methods. I am very curious why?..
    
    THC actually has both FD and FV schemes implemented
    FV is exactly conservative and there is a better way to do AMR with it
    FD is better because it is much simpler at higher order
    with THC when we want to do high order precision things we use FD
    when we do messy simulations with microphysics, for which robustness and conservation are more important than formal order of convergence, we use FV
    to be more precise we actually use the Kurganov-Tadmor central scheme, not a Godunov-type FV scheme
    but people always mixes the two (see e.g., the discussion in my PhD thesis)
    
    So, our simulations with microphysical eos andneutrinos are performed using the KT FV scheme?
    
    yes KT FV
\end{sidenote}
\textcolor{red}{combine this with 2018 dyn.ejecta paper method section to outline the modern version of THC. This should suffice.}

To increase a formal order of accuracty in a current generation numerical codes,  flux-conservative finite-difference HRSC schemes is the simplest approach. Its direct competitor, high-order finite volume schemes are more computationally expensive, as they require solution of multiple Riemann problems at the interface between regions \cite{Reisswig:2009us,Shu:2001rep}, as well as complex averaging and de-averaging procedures \cite{Tchekhovskoy:2007zn} \\

Here David presents a new code, the Templated-Hydrodynamics Code (THC), developed using Cactus framework \cite{Goodale:2003}. In \texttt{THC}, the state-of-the-art flux-vector splitting scheme are employed. The reconstruction in characteristic fields is avaliable for up to 7th order, as well as, the Roe flux split with a entropy-fix prescription. \\

The "templated" in the code name stands for a modern paradigm in C++ programing, the tempalted progaming, which means, that part of the code can be generated from the prescribed templates at compiling time. This paradigm allows for a creation of complex modular codes avoiding computational costs, that plague classical polymorphism. The "templated" programming allows to inline all the needed functions and classes at compiling time, \cite{Yang:2001}. \\

The following reconstruction schemes are implemented: MP5, classical monotonicity preserving \cite{Suresh:1997,Mignone:2010} the weighted essentially non oscillatory (WENO) schemes WENO5 and WENO7 \cite{Liu:1994,Jiang:1996,Shu:1997} and two bandwidth-optimized WENO schemes WENO3B and WENO4B \cite{Martin:2006,Taylor:2007}, constracted for modelling the conpressible turbulence. Note, that the number in acheme name stands for a forma order of accuracy. \\ 

In this chapter we briefly state the details of the \texttt{THC} algorithm and highlight the results of the comparioson between different reconstraction schemes for modelling relativistic turbulence. \\

The chapter is structured as following. First we overview several detain of \texttt{THC} code, discussing the numerical algorithms, in particular with respect to the equations of Newtonian and special relativistic hydrodynamics. Then we state several results. Then we view the linear and non-linear development of the relativistc Kelvin-Helmholtz instability (KHI) in 3D. 

\subsection{The \texttt{THC} code }

Here the infrastructure of \texttt{THC} is presented in addition to the formulation of Newtonian and special-relativistc HD. 

\subsubsection{Newtonian Hydrodynamics}
\textcolor{red}{this is not needed}

Let us consider the equations of the classical, Newtonian hydrodynamics. These are the coservation of mass, momentum and energy for a perfect fluid. 

They can be written in a from of a stringly hyperbolic system of balance-laws in afrom \ref{eq:theory:fd:hypsys} (copy)

\begin{equation}
\partial_t\boldsymbol{F}^0(\boldsymbol{u}) + \partial_i\boldsymbol{F}^i(\boldsymbol{u}) = \boldsymbol{S}(\boldsymbol{u}).
\end{equation}

Setting source terms $\boldsymbol{S}$ to zero, we have left the primitive variables 

\begin{equation}
    u = [\rho,\: \vec{\upsilon},\: \epsilon],
\end{equation}

where $\rho$ is the density, $\upsilon^{i}$ is the velocity, $\epsilon$ is the specific internal energy. The conserved variables are 

\begin{equation}
    \boldsymbol{F}^{0} = [\rho,\: \rho\vec{\upsilon},\: E] = \big[\rho, \: \rho\vec{\upsilon}, \: \rho\big(\frac{1}{2}\upsilon^2 + \epsilon\big)\big)\big],
\end{equation}

while the fluxes are 

\begin{equation}
    \boldsymbol{F}^i(u) = \big[ \rho\upsilon^i, \: \rho\upsilon^i\vec{\upsilon}+p\boldsymbol{\delta}^i, \: \upsilon^{i}(E+p) \big]
\end{equation}

where $p$ is the pressure, $[\boldsymbol{\delta}^i]^j = \delta^{ij}$ is the Kronecker symbol. To close the system of equations we set the equation of state $p = p(\rho, \epsilon)$. For the ideal fluid we set ideal fluid 

\begin{equation}
    p = (\Gamma -1)\rho\epsilon
\end{equation}

where $\Gamma$ is the adiabatic index of the fluid. \\

More detained description regarding Jacobians and their spectral decomposition for the equations of Newtonian hydrodynamics and for a generic equation of state see in \cite{Kulikovskii:2002}.

\subsubsection{Special-relativistic hydrodynamics}

\textcolor{gray}{[Here david provides the description of the special relativitisc formulation of hydrodynamics and describes many tests here performed with his code. Tests like shock tube, blast waves, studying convergence and resolution effects. This is not very related to my thesis so I read it and I skip it from rewriting here.]}

In relativity it is common to adopt the unit system in which $c=1$. Adopting also the Einstein summation coefficient convention, we aim here to provide a discription of a perfect fluid. 

The $4-$velocity of the fluid is a vector $\vec{u} = (W, W, \vec{\upsilon})$, with $W = (1 - \upsilon^i , \upsilon_i)^{-1/2}$ being the Lorentz Factor. Then, 

\begin{equation}
    \vec{j} = \rho\vec{\upsilon}, \hspace{5mm} T = \rho h \vec{u} \otimes \underline{u} + p\boldsymbol{g},
\end{equation}

are the rest-mass current 4-vector and the stress energy tensor respectively, where $\rho$ is the rest-mass density, $h=1+\epsilon+p/\rho$ is the specific enthalpy and $g$ is the spacetime metric. For a flat spacetime that we consider, it is 4x4 matrix with only non-zero components being diagonals, \textit{i.e.,} $g_{\mu\nu}=(-1,1,1,1)$. \textcolor{red}{David's goal is to study statistical properties of special-relativistic turbulence and of unveiling novel and non-classical features we will consider the fluid not to affect the spacetime geometry.}

Further, recalling the covariant derivative associated with the metric, $\nabla$, the laws of conservation read as a divergence of equals to zero, \textit{i.e.,}

\begin{equation}
    \nabla\cdot\vec{j}=0,\hspace{5mm}\nabla\cdot T = 0,
    \label{eq:theory:specrel:conserv}
\end{equation}

for the rest mass and energy respectively. 

Recalling the general form of the system of hyperbolic balance-laws in the form, (\ref{eq:theory:fd:hypsys}), the equations \ref{eq:theory:specrel:conserv} can written in terms of primitive variables 

\begin{equation}
    u = [\rho, \: \vec{\upsilon}, \: \epsilon],
\end{equation}

conservative variables 

\begin{equation}
    F^0(u) = [D, \: \underline{s}, \: \tau],
\end{equation}

where 

\begin{equation}
    D = \rho W, \hspace{3mm}, \vec{\vec{s}} = \rho h W^2 \vec{\upsilon} \hspace{3mm} \tau = \rho h W^2 - p - \rho W
\end{equation}

with the fluxes given by 

\begin{equation}
    \boldsymbol{F}^i(u) = [D\upsilon^i, \: \vec{s}\upsilon^i, + p\boldsymbol{\delta}^i, \: s^i-D\upsilon^i]
\end{equation}

We consider sources to be zero and hence the r.h.s of eq.(\ref{eq:theory:fd:hypsys}), and the systeb being closed by the equation of state. For simplicity chosen to the that of an ideal fluid.

Note that while in the Newtonian hydrodynamics there exists an inverse transformation $F^0\rightarrow u$, allowing to express primitive variables from conservative ones, this is not achievable in special relativity. A numerical techniques are required to revocer primitive varaibles, \textit{e.g.,} \cite{Kastaun:2006}. \textcolor{gray}{details on how to extract this via rootfinding and physical entropy}. 

\subsection{Numerical Tests}

\textcolor{gray}{Here I will outline some results for my own understanding. This is not to be put in the thesis, as I am not working with the code development.}
\textcolor{red}{NOT REFPHRASED}

\begin{itemize}
    \item \textbf{Strong shock}. Classical one-dimensional shock tube.Even at this fairly low resolution, all the schemes are able to capture well both the shock wave and the rarefaction wave, showing the good behaviour of the entropy fix. The contact discontinuity is resolved, but not without oscillations (due to the high Mach number of the shock wave, i.e., $\mathcal{M}=360$.
    \item \textbf{Blust wave}. larger density contrast at the contact discontinuity. The MP5 scheme is able to properly capture the constant state between the shock wave and the contact discontinuity, while the WENO schemes result in more “rounded” solutions.
    \item \textbf{Rotated Sod test}. Three-dimensional shock-tube test in Newtonian hydrodynamics. All the schemes are able to properly capture the main features of the solution: the discontinuities are captured within 1 or 2 gridpoints and both WENO5 and MP5 are able to capture the plateau in the velocity. Overall, these tests demonstrate the accuracy of the dimensionally unsplit approach that we use to treat the multi-dimensional case.
    \item \textbf{Double Mach reflection test} Our algorithm is able to introduce enough numerical dissipation to avoid the odd-even decoupling. All things considered, we find that the best performance is given by the MP5 scheme.
\end{itemize}

and in special relativity 

\begin{itemize}
    \item \textbf{Adiabatic smooth flow} Test code with the smooth solutions. One-dimensional, large-amplitude, smooth, wave propagating in an isentropic fluid. A good-enough approximation of the exact solution was obtained by computing it on a very fine Lagrangian grid (1e6 points) and interpolated on the Eulerian grid. Instead of the third-order SSP-RK scheme, we adopt here a fourth-order RK time integrator. Our schemes approach the expected convergence order only asymptotically, at very high resolution. The reason for this
    behaviour is in the “kinks” ahead and behind the pulse, where the numerical error is largest. These regions are “misinterpreted” as discontinuities by the shock-detection part of our schemes, unless they are resolved with enough gridpoints. The best performing scheme in this test is the MP5 one. Formation of the shock gradually degrades the overall convergence order to the 1st.
    \item \textbf{Blast wave}. Relativistic fluids can exhibit much stronger shock waves. MP5 scheme requires twice as small CFL as other schemes to prevent large oscillations and yields non-physical values. \textcolor{gray}{there is a finite-volume code \texttt{Whisky}, \cite{Baiotti:2010zf,Baiotti:2004wn} with the HLLE approximate Riemann solver \cite{Toro:1999} and PPM reconstruction \cite{Colella:1984}. } If the timestep is suffciently small, on the other hand, the MP5 algorithm results in very accurate solutions, as in the Newtonian case. \texttt{THC} here performes better then \texttt{Whisky}.
    \item \textbf{Shock-heating} relativistic effects can enhance the density contrasts in shock waves. shocks whose collision compresses the fluid. Kinetic energy into thermal energy, that is, through “shock heating”. For a Lorentz factor of a 1000, $\Gamma=4/3$, for a Newtonian fluid the compression ratio $\approx7$, while for spec. relativ. it is $\approx 4000$. The WENO5 and WENO7 solutions are affected by some small wall-heating effect, slight underdensity. The MP5 scheme, on the other hand, yields a solution which is essentially free
    from oscillations.
    \item \textbf{Transverse shock}. the equations for the momentum in the different directions are coupled together by the Lorentz factor: even in one-dimensional problems the application of a transverse velocity can change completely the solution. This feature was first pointed out by \cite{Pons:2000} and \cite{Rezzolla:2002ra}, and then used by \cite{Rezzolla:2002cc} and \cite{Aloy:2006rd} to discover a new physical effect, see also [\cite{Mignone:2005ns}, \cite{Zhang:2005qy}] for a description of the numerical consequences of this property]. The MP5 scheme overestimates slightly the density contrast, but all of the algorithms are able to capture the correct location of the shock wave.
    \item \textbf{Spherical explosion}. No analytic solution is known in this case. As in the one-dimensional case, a small timestep is necessary in order to avoid numerical oscillations with the MP5 algorithm, while the other schemes appear to be stable even with a timestep which is twice as large.
    \item \textbf{Kelvin-Helmholtz instability in 2D}. The instability is seeded by adding a small perturbation in the transverse component of the velocity. we use periodic boundary conditions in all the directions. Compare first growth rate of the transverse velocity during the linear-growth phase of the KHI. Important to including the contact wave in the approximate Riemann solver in the case of a finite-volume code. We also note the     importance of avoiding excessive dissipation in the contact discontinuity. The behaviour of the MP5 scheme, as well as that of the bandwidthoptimized WENO schemes, is more surprising: all of these schemes overestimate the growth of the RMS transverse velocity at low resolution. Some insight about the numerical viscosity can be gained by looking at the
    topology of the flow during the linear-growth phase of the KHI. These secondary
    instabilities, although only numerical artifacts (see below), appear only
    in schemes able to properly treat the initial contact discontinuity. They are not to be genuine features of the solution and, rather, tend to disappear as the resolution is increased. Conclusion: secondary instabilities are triggered by the non-linear dissipation mechanism of the different schemes, emerge neatly when computed with numerical schemes that treat properly the initial contact discontinuity, but do not have a physical meaning. Solution: adding more numerical viscosity [219] or as David suggests, physical viscosity. A more quantitative way of estimating the numerical viscosity of the code: The one-dimensional power spectrum can be used to quantify the typical scale of structures, such as the secondary vortices discussed above, stretched in the direction of the bulk shear flow. Even more unexpected is the ability of the MP5 scheme to resolve small scales structures and that, on the basis of the argument about the development of the secondary instabilities, should be more dissipative than WENO4B, but which instead appears to yield more small-scale structures in the rest-mass density.
\end{itemize}

\subsubsection{The relativistic Kelvin-Helmholtz instability in 3D}
\textcolor{red}{Important for GRBs}

analysis is meant to assess how the different methods reproduce the same turbulent initial-value problem and to provide some insight on the spectral properties of the different schemes. The relativistic KHI [see, e.g., [51]] is of particular interest because of its relevance for the stability of relativistic jets [see, e.g., [251, 250]], and because of its potential role in the amplification of magnetic fields in gamma-ray bursts [see, e.g., [338]], and binary neutron-star mergers [25, 143, 240, 274].

\begin{itemize}
    \item \textbf{The linear evolution of the instability} Consider the evolution of the instability during its linear-growth phase. As expected, all the numerical schemes, with the exception of MINMOD, are in very good agreement with the 2D solution up to the end of the linear-growth phase, when 3D effects become important and turbulence starts to play an important role in the dynamics. It is interesting to note that MINMOD, which is the most dissipative of the schemes we are using, is actually overestimating the growth of the KHI.
    This suggests that \textbf{some care should be taken when interpreting the results from under-resolved simulations}. secondary vortices are produced in more least dissipative methods.
    \item \textbf{The non-linear evolution of the instability} The linear-growth phase of the KHI instability ends when the primary vortices become unstable to secondary instabilities and the flow starts the transition to turbulence. Three-dimensional effects dominate. Use the tracer scalar field to track the evolution.
    \item \textbf{The non-linear evolution of the instability}. when the primary vortices become unstable to secondary instabilities and the flow starts the transition to turbulence. three-dimensional effects dominate.
    \item \textbf{Fully-Developed turbulence} By far the most interesting quantity to study is the three-dimensional velocity power spectrum. conclusions. importance of the use of high order    schemes (avoid bottle-neck, otherwise power-spectrum shows an excess due to viscous effects.) use of WENO4B over WENO5 is well justified, since WENO4B is roughly twice as expensive as     WENO5 in 3D. Tthe main differences between the bandwidth-optimized schemes and their traditional counterparts seem to lay in the bottleneck region WENO3B and WENO4B have a much less pronounced bottleneck with respect to WENO5, WENO7 and MP5.
\end{itemize} 

\subsection{Driven Relativistic Turbulence}

consider an idealized model of an ultrarelativistic fluid. The fluid is modeled as perfect. We evolve the equations describing conservation of energy and momentum in the presence of an externally imposed Minkowskian force. To solve the equations of relativistic hydrodynamics in 3D we use the THC
code described in this chapter and published in \cite{Radice:2012cu}. In particular, here, we use the MP5 reconstruction in local characteristic variables [165].
\begin{itemize}
    \item \textbf{Basic flow properties} All in all, this is one of our main results: the velocity power spectrum in the inertial range is universal, that is, insensitive to relativistic effects, at least in the subsonic and mildly supersonic cases. Note that this does not mean that
    relativistic effects are absent or can be neglected when modelling relativistic turbulent flows.
    \item \textbf{Intermittency} local appearance of anomalous, short-lived flow features.
    \item \textbf{Conclusion} \textcolor{red}{We have presented THC, a new multi-dimensional, finite-difference, high-resolution shock-capturing code for classical and special-relativistic hydrodynamics... -- [FULL description of THC]}
\end{itemize}

\section{Finite-Differencing Methods: General Spacetimes} 

Goal is to model the inspiral of BNS to produce accurate waveforms. \textcolor{red}{here, we describe our new high order, high-resolution shockcapturing, finite-differencing code: \texttt{WhiskyTHC}, which constitutes the extension to general relativity of the \texttt{THC} code.}

\subsection{WhiskyTHC}
\textcolor{red}{marginally rephrased}
\begin{itemize}
    \item \textbf{Numerical Methods}. \textcolor{gray}{[high order, high-resolution shockcapturing, finite-differencing code]} \texttt{WhiskyTHC} is a result of combination of two \texttt{Whisky} \cite{Baiotti:2004wn} and \texttt{THC} \cite{Radice:2012cu}. High-order flux-vector splitting finite-differencing techniques has come from the former, while the module for the recovery of the primitive quantities as well as the equation of state framework from the latter \cite{Galeazzi:2013mia}. Tabulated temperature and composition dependent equation of states can be used \textcolor{gray}{however David used only polytrops}. Overall, \texttt{WhiskyTHC} solves the equations of general-relativistic hydrodynamics in conservation form \ref{eq:theory:grhdeq_thc}. using a finite difference scheme \textcolor{red}{we however are using FV? Be carefull with which methods are used exactly}. The flux reconstruction is done in
    local-characteristic variables using the MP5 scheme, see \textit{e.g.,} \cite{Rezzolla:2013}. The space-time is evolved using the CCZ4 formulation \ref{eq:theory:ccz4equations}, solved via finite difference code publicly available through \texttt{Einstein Toolkit}, \cite{McLachlan,Loffler:2011ay}. There, the central stencil is used throughout, and only terms associated with the advection along the shift vector are treated using the upwinded by one grid point stencil. The accuracy of the scheme is availalbe at 6th and 8th order, while 4th is commonly employed. In addition, the fifth order Kreiss-Oliger style artificial dissipation \cite{Kreiss:1973} is added to aid with non0linear stability. 
    The code is build on the \texttt{Carpet} AMR driver \cite{Schnetter:2003rb} from the \texttt{Cactus} computational toolkit \cite{Goodale:2003}, incorporating a provided by \texttt{Carpet} Berger-Oliger-style mesh refinement \cite{Berger:1989,Berger:1984} with subcycling in time and refluxing. \textcolor{red}{in Thesis it is said, -- no refluxing was done yet}
    
    
    \item \textbf{Atmosphere Treatment} The atmosphere is referred to an artificial density floor in the simulation domain. It is introduced in order to tackle the challenges arising when considering boundary between the fluid and vacuum in Eulerian (relativistic) hydrodynamics codes \cite{Galeazzi:mThesis:2008,Kastaun:2006,Millmore:2009dk}. The defining property of the atmosphere is that the rest mass density and coordinate velocity are reset to a floor values once the former falls below a certain threshold value during the evolution \cite{Font:2001ew,Baiotti:2004wn}. While showing a reasonable results in second order codes, in higher order ones the numerical oscillations lead to the creation of vacuum nonetheless, that in light of the aforemention atmosphere effect result in the mass and energy violation \cite{Radice:2011qr}. For codes that rely on characteristic variables, the degeneracy in low-density, low-temperature limits also plagues the computation. This problem is the main reason behind the popularity of robust shock capturing codes, even though they are of first order in the general-relativistic hydrodynamics codes. Vacuum treatment for higher order codes is of main challenges to overcome. 
    
    \begin{itemize}
        \item \textit{Standard Atmosphere Treatment} or \textit{"ordinary MP5 approach"} is based on setting density that falls below $(1+\epsilon)\rho_{\text{atmo}}$ to the atmosphere density, velocity to zero and internal energy to the one prescubed by the polytropic EoS. The $\rho_{\text{atmo}}$ is usually related to a certain characteristic density, \textit{e.g.,} maximum density at the beginning of the simulation as $\rho_{\text{atmo}} = 10^{-7,-9}\rho_{\text{max}}$. The tolerance parameter $\epsilon$ is usually set to $10^{-2}$ and accounts for excessive oscillations of the fluid–vacuum interface. 
        
        \item \textit{An Improved Atmosphere Treatment} or \textit{"MP5+LF"} In this approach the component-wise Lax-Friedrichs flux split is turned on when a certain density is reached. This increases the dissipation of the scheme and allows to avoid problems arising in characteristic reconstruction, associated with the degeneracy of the characteristic variables close to vacuum. Unfortunatelly, if the ejection of low velocity and density matter is concered, this approach may yield oscillatory solutions and thus creates artifacts. 
        
        \item \textit{Positivity Preserving Limiter} a novel approach based on the use of PPL proposed in \cite{Hu:2013}. Here we provide a brief overview. 
        Consider a simple scalar conservation law in 1D
        
        \begin{equation}
            \frac{\partial u}{\partial t} + \frac{\partial f(u)}{\partial x} = 0
            \label{eq:theory:whickythc:atmo:conslaw}
        \end{equation}

        Since for a SSP time integrator a time update is convex combination of Euler steps, for which the positivity of $u$ is guaranteed for any scheme, the general discrete from of \ref{eq:theory:whickythc:atmo:conslaw} can be written as 
        
        \begin{equation}
            \frac{u_{i}^{n+1} - u_{i}^{n}}{\Delta^0} = \frac{f_{i-1/2} - f_{i+1/2}}{\Delta^1}
        \end{equation}
        
        And if $\lambda = \Delta^0/\Delta^1$, then 
        
        \begin{equation}
            u_{i}^{n+1} = \frac{1}{2}(u_{i}^{+} + u_{i}^{-}) = \frac{1}{2}\Big[ (u_{i}^{n} + 2\lambda f_{i-1/2}) + (u_{i}^{n} - 2\lambda f_{i+1/2})\Big].
        \end{equation}
        
        where then $u_{i}^{n+1} = u_{i}^{+} + u_{i}^{-}$ and $u_{i}^{n} = u_{i}^{n} - 2\lambda f_{i+1/2}$. Notably, the $u_{i}^{+}$ and $u_{i}^{-}$ as well as $u_{i}^{n+1}$ are positive. In \cite{Hu:2013} it was pointed out that if a first-order Lax-Friedrichs scheme with $\lambda\leq 1/2a$ (with $a$ being the maximum propagation speed) is used for evaluating $f_{i\pm 1/2}$, the $u_{i}^{\pm}\geq \text{min}_i u_{i}^{n}$ \cite{Zhang:2010}. \textcolor{red}{not understand that}. Then the suggested point is ti change the $f_{i+1/2}$ to be 
        
        \begin{equation}
            f_{i+1/2} = \theta f_{i+1/2}^{\text{HO}} + (1-\theta)f_{i+1/2}^{\text{LF}},
        \end{equation}
        
        where $f_{i+1/2}^{\text{HO}}$ is the high-order flux of the original scheme, and $f_{i+1/2}^{\text{LF}}$ is the flux associated with the first order Lax-Friedrichs scheme, and $\theta\in[0,\:1]$. If the spatial location is far from vacuum, then the original high accuracy scheme can be used, so the $\theta$ remains $1$. However, in the vicinity of the vacuum, the $\theta$ decreases, to assure that $u_{i}^{\pm}$ remains positive. This is always possible since the Lax-Friedrichs scheme, used for $f_{i+1/2}^{\text{LF}}$ is positivity preserving.
        
        In a multidimensional case the the component-vise extension is employed. \textcolor{red}{formula that I will not used for $u_{i,j,k}^{n+1}$}.
        
        In \cite{Hu:2013} the extension of the method to the system of conservation laws was also proposed. 
        
        The complications however are present when the source terms are treated. While for a simplified case of classical gas dynamics it might require a lower timestep, in the general relativistic case and general tabulated EOS, the positivist of pressure is difficult to assure due to complexity of the energy source terms. It can be mitigated by enforcing a floor value on the pressure.
        
        Note, that adopting a positivity preserving limiter to treat the transition between matter and vacuum, still implies replacing the vacuum with low density fluid at rest, is not a physically accurate approach. That would rely on treating the transition as a free boundary (see \textit{e.g.,} \cite{Kastaun:2006}) The advantage of positivity preserving limiter with respect to a classical atmosphere treatment, is that it allows to have a value of $\rho_{\text{atmo}}$ that does not require further tuning and can be arbitrary small, and assure that the solution is locally conserved. 
            
        \textcolor{red}{In our models} we employ this approach as follows, at the meginning of the simulations we set the floor density, relying in the subsequent evolution on a positivity preserving limiters to ensure the atmosphere well behaviour. Due to negligeble density of the atmosphere its accretion has a negligeble effect on the evolved object. 
        
        
    \end{itemize}

    \item \textbf{Single Neutron Stars: Fixed space-time} here the atmosphere test showed that using the standard atmosphere leads to the mass conservation violation on a small degree, however, it also shows an appearance of a "jet+-like structure along the axes where grid points are aligned with the star's surface. These aritifical outflowes are driven by the numerical oscillations creating an imbalance at the surface MP5+LF on the other hand shows no artificial matter streams due to its conservative nature
    \item \textbf{Single Neutron Stars: Full-GR}
    \item \textbf{Non-linear Oscillations: the Migration Test} Here the setup is the following, a neutrono star in Full GR is set with an initial oscullation, that formces a star to fiurst contract and then rebounce. This rebounce creates and ejecta. Different prescitpions show this ejecta, but the MP5-LF is inadequate in this test, introducing the structure in the ourflow (numerical osculllations/fragmentations). Origin: component-wise reconstruction in low density regions.
    \item \textbf{Gravitational Collapse to Black-Hole}
    \item \textcolor{red}{\textbf{Binary Neutron Stars} [Copied. Not rephrased]} Models having an initial small separation of 45 km. Compare it to \texttt{Whisky} code, that is a second-order finite-volume code, with high-order primitive reconstruction and implements several different approximate Riemann solvers, \textcolor{red}{David used PPM reconstruction [95] and of the HLLE Riemann solver \cite{Harten:1983}, \cite{Einfeldt:1988}].} 
    The initial data we consider describes two neutron stars in quasi-circular orbit. It is computed in the conformally-flat approximation using the Lorene pseudo-spectral code \cite{Gourgoulhon:2000nn} and has been made publicly available by the Meudon group \cite{Lorene}. The EoS assumed for the initial data is polytropic. \textcolor{gray}{In our case it is cold EOS} while the evolution is performed using the ideal-gas EoS to allow for thermal effects in the merger phase. \textcolor{gray}{In our case it is finite temperature EOS}. Discussion on baryonic masses and compactness $c=M_{\infty}/R_{\infty}$, where $R_{\infty}]$ is the areal radius.
    \begin{itemize}
        \item \textit{Small separation} Grid discussion: extend, symmetries \textit{e.g.,} we assume reflection
        symmetry across the $xy$ plane and $\pi$ symmetry across the yz plane. Number of refinement levels. Static grid or AMR. 
        Evolution via CCZ4 with damping constants $\kappa_1=?$, $\kappa_2=?$ and $\kappa_3=?$ and with beta-driver $\eta=?$. The space time evolved. Space-time is evolved via fourth order finite-differencing and with fifth order Kreiss-Oliger artificial dissipation \textcolor{red}{I need to find what is used in our runs}.
        Study the graviational radiation via looking ad the $l=2$ $m=2$ mode of the $Weyl$ scalar $\Psi_4$ extracted at a fixed coordiante radius of $r=450M_{\odot}$. Strain is not computed as it involves other uncertanties \cite{Boyle:2009vi,Reisswig:2009us,Reisswig:2009rx,Reisswig:2010di}. 
        The dynamics of the inspiral and merger of BNS has been described many times and in great detail in the literature \textit{e.g.,} \cite{Baiotti:2008ra}. We only mention that the two neutron stars
        inspiral for about 2:5 orbits, touch and quickly merge into a single black-hole. For this particular model no significant disk is left behind. The gravitationalwave signal consists.
        For GW plot 22 mode of $\Psi_4$ as extracted at $r=450M_{\odot}$ and as a function of the retarted time $t-r_*$ where $r_* = r + 2M_{\text{ADM}}\log(r/(2M_{\text{ADM}})-1)$.
        Results: 1. treatment of the neutron star surface is not a leading source of error in binary neutron star simulations, as far as the inspiral GW signal is concerned. \texttt{WhiskyTHC} shows a smaller dephasing significantly smaller de-phasing: the difference between the low and the high resolution is about 0.6 radians, which is a factor four smaller than the one shown by \texttt{Whisky}. Observation: merger happens earlier as we increase the resolution. For each run we compute the phase, $\phi$, of the 22 mode of $\Psi_4$ from its definition, $(\Psi_4)_{22} = A e^{i\phi}$. 
        We should stress that this error estimate only reflects the numerical truncation error. Other systematic errors and, in particular, finite extraction radius effects and inaccuracies in the initial data, are also present and might be relevant (especially for WhiskyTHC). On the other hand, here we are interested only in evaluating the accuracy of the two numerical methods.
        \item \textit{Large separation} [mostly skipped]
        Notice that  contact happens before the bare contact angular frequency \cite{Damour:2012yf} 
        \begin{equation}
            0.15276 = M\omega_{\text{contact}} := 2C^{3/2}, \hspace{5mm} \omega:=\dot{\phi}
        \end{equation}
        is reached. This is in any case expected because this approximation of the contact frequency does not take tidal deformations into account.
    \end{itemize}
    
    \item \textbf{Conclusion}
     
\end{itemize}

\section{Discontinuous Galerkin methods for general relativistic hydrodynamics}
\textcolor{red}{Skipped}


%% ==========================================
%%
%% Radiation
%%
%%
%% ==========================================


\part{Relativistic Radiation Transport}
\chapter{The Filtered Spherical Harmonics Method}
\section{Introduction}

Radiation-transport equations are complex and expensive to solve numerically. The main reasons for that are the following. First, is the dimensionality of the problem. Radiation carriers are described by their location in space, which generally is a 3D sapce, momenta, which requires 2 additional components, angles, and finally one component for energy of the carrier. Thus, with an addition of time there are $(6+1)$ dimensions for the problem. The second main difficulty stems from in the different regimes of the radiation transport with respect to the optical depth. If the latter is high, \textit{i.e.,} the opacity is very large, the radiation transport can be described by the diffusion of carriers, having the parabolic character. However, if the opacity is small and thus the optical depth, the radiation can stream freely, displaying a hyperbolic character of the transport \cite{Mihalas:1984}. A particular difficulty is presented by the intermediate regime, when the radiation transport mechanism transitions from diffusion to free streaming. 

Most commonly used approaches to simplify the radiation transport equation relies on reducing the sentimentality of the problem. The most approach is to reduce the number of spatial dimensions by assuming a certain symmetry, for example, an axial or spherical symmetry. And while this have shown to be a reasonable approximation for many astrophysical models, in cases where the system itself does not exhibit any spatial symmetries, such simplification cannot be done. Anther approach to reduce the dimensional of the problem, is to simplify the momentum space. An example of such approach, that allows to reduce the computational cost considerably, is the approximation of transport equations with diffusion equations \cite{Pomraning:1973,Roe:1981}. And while in the opaque regions (with high optical depth) this approximation is reasonable, it becomes much less so in the transparent regions where the anisotropy of radiation has to be taking into account \cite{Ott:2008jb}. There, the free-streaming approximation is usually adopted. The solution in the intermediate region then obtained by the interpolating between the two. This approach can be augmented by using a two-moment schemes with analytic closures (\textit{e.g.,} \cite{Brunner:2002}), however in many applications a solution in actual $(6+1)$ dimensions is a requirement.

The main source of complexity in the radiation transport stems from the scattering integral over all 4$\pi$ steradian. By dividing the solid angle into a number of discrete angular intervals, the integral can be replaced with a finite sum, converting the integrodifferential equation into a linear system of equations for a multi-index object. This method of solving transport equation along several directions in each spatial zone the is called discrete-ordinate (SN) method \cite{Castor:2004,Ott:2008jb,Sumiyoshi:2012za,Godoy:2012}. The discretization, however, comes with a a serious drawback, as it introduces regions which the radiation cannot reach between the grid directions. This is so-called "ray-effect" \cite{Morel:2003}, that causes large spatial oscillations in the transport variables.

Another way to solve the multidimensional radiation transport is to employ Monte-Carlo methods \cite{Fleck:1971,Gentile:2009,Abdikamalov:2012zi}. They can achieve very high accuracy, however, plague by the statistical noise (due to finite sampling of the phase space), they require very large number of Monte Carlo particles, and thus, they are computationally expensive. 

A more physically motivated approximation to the radiation transport can be achieved by expending the radiation intensity in angles using spherical harmonics as basis functions. This is so-called $P_N$ method. It allows to convert the integrodifferential equation into a hyperbolic system of partial differential equations for the expansion coefficients. Coefficients stand for angular moments in the basis of spherical harmonic functions. An advantage of this method with respect to the diffusion approximation is that if in the latter the radaition propagation speed is unbound, in $P_N$ method, that approximates the radiation as a set of elementary waves, the propagation speed is limited by the speed of light \cite{McClarren:2008b}. This approach is also numerically more favorably, exhibiting the formal spectral convergence to the true solution and requiring less memory, a factor of two with respect to $S_N$ method for an equivalent angular distribution and accuracy. Preserving the rotational invariance, the $P_N$ method allows to avoid the "ray effect" of $S_N$ method. However, approximation of the radiation field with smooth basis functions, while being sufficiently accurate in the optically thick regime, displays non-physical oscillations in the transparent one \footnote{This is related to the so-called Gibbs phenomenon \cite{Boyd:2001}}. If radiation is coupled to matter, these oscillations can produce regions with negative radiation intensity and thus negative temperature \cite{McClarren:2008b,Olson:2009} (see also \cite{Olson:2000,Olson:2009,Brunner:2001,McClarren:2010,Olson:2012,Hauck:2010}). On of the solutions to this problem is to filter out (using \textit{e.g.,} spherical spline expansion \cite{Boyd:2001}) these oscillations from the radiation intensity \cite{McClarren:2010}. However, the exact choice of the filtering technique, extension to full 3D and absence of the clear continuum limit\footnote{Which does not allow to study the spatial convergence of the solution} present additional challenge in implementing the filtering approach. \\
%
In this chapter we overview the method by McClarren \& Hauck \cite{McClarren:2010}, \textcolor{red}{folllowing David  who achieved the clear continuum limit by reformulating the original method, investigated the range of filter types and studied 2D and 3D radiation transfer with his trasport code \texttt{Charon}} 
\textcolor{red}{more description of Charon}
\textcolor{red}{The chapter is organized as following}


\section{The relativistic Boltzmann equation}
\textcolor{red}{If I am to use this seriously, I must augment this section. It is barely rephrased and copied.}

Here we aim to introduce the equation for radiation transport in special relativity. For the purpose of being in-line with corresponding literature we slightly change the notation from the section \textcolor{red}{sec:[Boltzmann GR]} and given an independent discussion

\subsection{The distribution function for radiation}

Let us start by introducing the specific radiation intensity $I$

\begin{equation}
    d\mathcal{E} = I \cos\theta\text{d}A\text{d}\nu\text{d}\Omega\text{d}t
    \label{eq:theory:boltz:specIntes}
\end{equation}

that describes the radiation energy in freqquency range $\nu\pm\text{d}\nu$ propagating in the direction $\Omega$, within the solid angle $\text{d}\Omega$, crossing the surface are $\text{d}A$, whose norm makes and angle $\theta$ with respect fo $\Omega$, withing time interval $\text{d}t$. (see \textit{e.g.,} \cite{Pomraning:1973}). 

Note, that in this definition, the intensity is set per unit frequency interval, while it is common in neutrino-transport literature to define it per frequency interval (\textit{e.g.,} \cite{Burrows:1999es})

In order to approach the number density and energy density of the radiation, we introduce the distribution function, $F$, describing the density of radiation carriers in phase space. This is of particular usefullness for the relativistic case as $F$ is Lorentz invariant \cite{Mihalas:1984}. For that we first focus on a single relativisitc particle phase space. Let $x^{\mu}$ and $p^{\mu}$ be particles positions and momenta, measured in \textcolor{red}{fiducual internal frame}. Assuming that the radiation carriers are massless, with their energy being related to frequency as $\epsilon = h\nu$, the momentum four-vector $p^{\mu}$ can be written in terms of angles describing the direction of the particles movement, $\theta$ and $\phi$, and the frequency $\nu$ as 

\begin{equation}
    p^{\mu} = \frac{h\nu}{c}(1, \: \cos\phi\sin\theta,\:\sin\phi\sin\theta,\:\cos\theta),
    \label{eq:theory:boltz:pmu}
\end{equation}

where we also made use of the normalization condition for timelike vectors, that imply that $p^{\mu}$ has only $3$ independent components.

Next we introduce a quantities

\begin{align}
    dN &= F p^{\mu}t_{\mu}\text{d}^3 x\text{d}\Pi = \frac{h^3\nu^2}{c^2}F\text{d}^3x\text{d}\nu\text{d}\Omega, \text{ where } \\
    \text{d}\Pi &= \frac{\text{d}p^1\text{d}p^2\text{d}p^3}{-p_0} =\frac{h^2\nu}{c^2}\text{d}\nu\text{d}\Omega
    \label{eq:theory:boltz:dN}
\end{align}

where $dN$ stands for the total number density of radiation carriers within the phase-space volume element $\text{d}\Pi$, spatial volume element $\text{d}^3 x$, that pass a $t=\text{const}$ hyperserface with normal $\vec{t} = \partial_t$. The $\text{d}\Pi$ is Loretnz-invariant by contraction volume element over the manifold of allowed $p^{\mu}$ \cite{Cercignani:2002}.

Thus we introduced a distribution function in terms of $p^i$, that can ultimatelly be expressed in terms of $\nu$, $\phi$ and $\theta$. This function thus contain the information regarding the type of the radiation carrier. However, in the neutrino-transport literature factor $g$ is usually introduced that represents the statistical weight of the particle, $1$ for neutrinos and $2$ for photons (assuming both are mass-less). 

Note, that the spatial volume element can be written as $\text{d}^3x = \text{d}\cos\theta\text{d}t$ and the energy per particle is $h\nu$. Then from definition of $\text{d}\mathcal{E}$ we can write that $\text{d}\mathcal{E} = h\nu\text{d}N$. This, combined with equations \ref{eq:theory:boltz:dN} and \ref{eq:theory:boltz:specIntes} results in 

\begin{equation}
    I = \frac{h^4} \nu^3{c^2}F,
    \label{eq:theory:boltz:IasF}
\end{equation}

which imply that both $\text{d}N$ and $F$ are scalars.

Note, that instead of momentum phase space $\text{d}^p$ we used here the Lorentz-invarient volume element $\text{d}\Pi$ from equation \ref{eq:theory:boltz:dN}. 

\subsubsection{The relativistic Boltzmann equation}

Following \cite{Mihalas:1984} we write the special-relativistc Boltzmann equation as

\begin{equation}
    p^{\mu}\frac{\partial F}{\partial x^{\mu}} = \mathbb{C}[F],
    \label{eq:theory:boltz:boltz}
\end{equation}

where $\mathbb{C}$ is the collisional term that describes the radiation-matter interaction. The l.h.s. of the \ref{eq:theory:boltz:boltz} contains the description of radiation. 

Following the commonly adopted approach in the radiation transport literature, we express $\mathbb{C}$ in terms of the absorption, emission, and scattering coefficients. Then, the evolution equation for intensity reads \cite{Pomraning:1973}

\begin{equation}
    \frac{1}{c} \frac{\partial I}{\partial t} + n^i\frac{\partial I}{\partial x^i} = \eta - \kappa I + \frac{\kappa_s}{4\pi}\int\frac{\nu}{\nu'}K(\nu',\vec{n}'\rightarrow\nu,\vec{n}) I(\nu',\vec{n}')\text{d}\Omega'\text{d}\nu'
    \label{eq:theory:boltz:def_dIdt}
\end{equation}

where $\eta$ stands for the matter radiative emissivity, $\kappa$ is the total extinction coefficient that incorporates the absorption $\kappa_a$ and scattering $\kappa_s$ as $\kappa = \kappa_a + \kappa_s$, $K$ is the scattering \textcolor{red}{kernel}, that describes the statistical probability of a scattering to occur from one specified angle and frequency into anther \cite{Pomraning:1973}.

Combining the equations \ref{eq:theory:boltz:pmu}, \ref{eq:theory:boltz:IasF}, and \ref{eq:theory:boltz:def_dIdt}, we write 

\begin{equation}
    p^{\mu}\frac{\partial F}{\partial x^{\mu}} = \frac{c^2}{h^3}\frac{\eta}{\nu^2} - h\nu\kappa F + \frac{h \nu \kappa_s}{4\pi}\int\Bigg(\frac{\nu'}{\nu}\Bigg)^2 K(\vec{p}\:'\rightarrow\vec{p})F(\vec{p}\:')h\text{d}\nu'\text{d}\Omega',
\end{equation}

where as $\mathbb{C}$ is a scalar\footnote{as $F$ was shown to be a scalar}, the $\eta/\nu^2$ and $\nu\kappa$ are invariant quantities, as also pointed out in \cite{Mihalas:1984}. 

For simplicity we shall consider only elastic scattering, in which the energy of the radiation carriers is preserved. Then the scattering kernel reads 

\begin{equation}
    K(\nu'\vec{n}\: '\rightarrow\nu,\vec{n}) = [1 + \sigma_a\vec{n}\cdot\vec{n}\: ']\delta(\nu-\nu')
\end{equation}

where the coefficient $\sigma_a$ accounts for scattering anisotropy. 

\subsection{The \texttt{Charon} Code}
\textcolor{red}{Here I summarize how the charon code was made}

The distribution fuction $F$ depends in $1+3+1+2$ variables of time, $t$, space, $x^{\mu}$, frequency, $\nu$, and angles of propagation $\theta$, $\phi$. If the matter within which the radiation is propagating is moving, there two main frames in which these variables are defined: Eulerian (inertial) frame and comoving with the fluid frame (a collection of frames for every fluid element that have an instantaneous velocity, same as the fluid, \cite{Mihalas:1984,Hubeny:2006wm}). If the fluid id stationary these frames coincide.

In charon code the distribution function is expanded in the spatial cooridnates using the linear DG basis and in the angular variables using spherical harmonics, the frequency is treated using a multi-group approach. Thus the radiation transfer takes from a large set of ordinary differential equations that can be solved via an semi-implicit time integrator. 

\subsubsection{Frequency Discretization}
Chosen a maximum frequency $\nu_{\text{max}}$ and a uniform grid in a frequency space. Associated intervales of frequency are called frequnecy or energy groups. 

End.


%[2]\cite{Cactus}
%[3]\cite{Lorene}
%[4]\cite{McLachlan} % BSSN
%[5]\cite{Abdikamalov:2012zi}
%[6]\cite{Abramowitz:1968}
%[7]\cite{Adams:2002}
%[13]\cite{Alic:2013xsa}
%[15]\cite{Aloy:2006rd}
%[16]\cite{Luigi:2002}
%[18]\cite{Anile:1990}
%[22]\cite{Arnold:2002}
%[25]\cite{Baiotti:2008ra}
%[28]\cite{Baiotti:2010zf}
%[29]\cite{Baiotti:2004wn} [same 1]
%[31]\cite{Baiotti:2009gk}
%[33]\cite{Baiotti:2004wn} [same 1]
%[34]\cite{Balsara:2012}
%[35]\cite{Balsara:2000}
%[36]\cite{Banyuls:1997}
%[40]\cite{Benartzi:2007}
%[44]\cite{Berger:1989}
%[45]\cite{Berger:1984}
%[50]\cite{Biswas:1994}
%[59]\cite{Borges:2008}
%[60]\cite{Boris:1971}
%[61]\cite{Boyd.:1996}
%[62]\cite{Boyd:2001}
%[63]\cite{Boyle:2009vi}
%[66]\cite{Borges:2008}
%[67]\cite{Bruenn:1985}
%[68]\cite{Brunner:2002}
%[69]\cite{Brunner:2001}
%[70]\cite{Brunner:2005}
%[72]\cite{Burrows:1999es}
%[74]\cite{Canuto:2006}
%[75]\cite{Canuto:2008}
%[76]\cite{Canuto:1988}
%[77]\cite{Cardall:2002bp}
%[78]\cite{Castor:2004}
%[79]\cite{Cercignani:2002}
%[81]\cite{Chen:2006}
%[82]\cite{Chen:2003}
%[83]\cite{Chen:2009}
%[84]\cite{Chernikov:1962}
%[85]\cite{Chiu:1973}
%[86]\cite{Cockburn:1998v}
%[87]\cite{Cockburn:2003}
%[88]\cite{Cockburn:1989ii}
%[89]\cite{Cockburn:1989iii}
%[90]\cite{Cockburn:1990iv}
%[91]\cite{Cockburn:2000}
%[92]\cite{Cockburn:1991}
%[93]\cite{Cockburn:2001}
%[94]\cite{Colella:2008}
%[95]\cite{Colella:1984}
%[96]\cite{Colella:1984}
%[97]\cite{Conway:1966}
%[100]\cite{Crandall:1980}
%[101]\cite{Crandall:1980proc}
%[102]\cite{Damour:2012yf}
%[108]\cite{Debbasch:2009a}
%[109]\cite{Debbasch:2009b}
%[114]\cite{DiPerna:1985}
%[115]\cite{Donat:1996}
%[116]\cite{Donat:1998}
%[121]\cite{Dumbser:2009}
%[122]\cite{Dumbser:2007}
%[123]\cite{Dumbser:2008}
%[124]\cite{Ehlers:1971}
%[125]\cite{Einfeldt:1988}
%[128]\cite{Field:2010}
%[129]\cite{Fleck:1971}
%[130]\cite{Font:2008fka}
%[131]\cite{Font:2001ew}
%[134]\cite{Friedrichs:1954}
%[137]\cite{Galeazzi:mThesis:2008}
%[136]\cite{Galeazzi:2013mia}
%[139]\cite{Gassner:2011}
%[140]\cite{Gentile:2009}
%[141]\cite{Gerolymos:2009}
%[144]\cite{Giacomazzo:2005jy}
%[146]\cite{Giacomazzo:2010bx}
%[147]\cite{Glimm:1965}
%[148]\cite{Godoy:2012}
%[149]\cite{Godunov:1959}
%[150]\cite{Goodale:2003}
%[151]\cite{Goodman:1985}
%[152]\cite{Gottlieb:1997}
%[154]\cite{Gottlieb:2009}
%[157]\cite{Gourgoulhon:2000nn}
%[161]\cite{Harten:1976}
%[162]\cite{Harten:1983}
%[163]\cite{Harten:1987}
%[164]\cite{Hauck:2010}
%[166]\cite{Henrick:2005}
%[167]\cite{Hesthaven:2008}
%[168]\cite{Hesthaven:2007}
%[169]\cite{Hotokezaka:2013mm}
%[170]\cite{Hu:2013}
%[171]\cite{Hubeny:2006wm}
%[173]\cite{Israel:1979wp}
%[174]\cite{Israel:1963}
%[176]\cite{Jiang:1996}
%[179]\cite{Kastaun:2006}
%[180]\cite{Kastaun:2007phd} % bugged
%[185]\cite{Konigl:1980}
%[186]\cite{Kreiss:1973}
%[188]\cite{Kruzkov:1970}
%[189]\cite{Kulikovskii:2002}
%[190]\cite{Kurganov:2000}
%[192]\cite{Lax:1957}
%[193]\cite{Lax:1956}
%[194]\cite{Lax:1960}
%[196]\cite{LeVeque:1992}
%[195]\cite{Lax:1954}
%[197]\cite{LeVeque:2002}
%[201]\cite{Lindquist:1966}
%[202]\cite{Liu:1994}
%[203]\cite{Loffler:2011ay}
%[204]\cite{Lowrie:2002}
%[207]\cite{Marquina:1994}
%[208]\cite{Marti:1994}
%[210]\cite{Marti:1991wi}
%[211]\cite{Martin:2006}
%[212]\cite{Maselli:2012zq}
%[215]\cite{McClarren:2010}
%[216]\cite{McClarren:2008}
%[217]\cite{McClarren:2008a}
%[218]\cite{McClarren:2008b}
%[220]\cite{Meier:1999}
%[221]\cite{Meister:2009}
%[222]\cite{Curtis:1972}
%[224]\cite{Mignone:2005ns}
%[226]\cite{Mignone:2010}
%[227]\cite{Mihalas:1984}
%[228]\cite{Millmore:2009dk}
%[230]\cite{Morel:2003}
%[235]\cite{Nessyahu:1990}
%[242]\cite{Olson:2009}
%[243]\cite{Olson:2012}
%[244]\cite{Olson:2000}
%[245]\cite{Ott:2008jb}
%[247]\cite{Pannarale:2011pk}
%[248]\cite{Papadopoulos:1999kt}
%[255]\cite{Pomraning:1973}
%[257]\cite{Pons:2000}
%[259]\cite{Qiu:2005}
%[260]\cite{Qiu:2004}
%[262]\cite{Quirk:1994}
%[263]\cite{Radice:2011qr}
%[264]\cite{Radice:2012cu}
%[267]\cite{Reed:1973}
%[268]\cite{Reisswig:2010di}
%[269]\cite{Reisswig:2009us}
%[270]\cite{Reisswig:2009rx}
%[272]\cite{Rezzolla:2002ra} 
%[273]\cite{Rezzolla:2002cc}
%[274]\cite{Rezzolla:2011da}
%[275]\cite{Rezzolla:2013}
%[277]\cite{Roe:1981}
%[283]\cite{Santamaria:2010yb}
%[284]\cite{Sasaki:1958}
%[285]\cite{Sasaki:1962}
%[289]\cite{Schnetter:2003rb}
%[295]\cite{Shu:1997}
%[296]\cite{Shu:1999}
%[297]\cite{Shu:2001rep}
%[298]\cite{Shu:1989}
%[299]\cite{Shu:1988}
%[300]\cite{Shu:2003}
%[304]\cite{Smoller:1993}
%[305]\cite{Sod:1978}
%[309]\cite{Sumiyoshi:2012za}
%[310]\cite{Suresh:1997}
%[311]\cite{Synge:1957}
%[312]\cite{Tadmor1998}
%[314]\cite{Tanner:2006}
%[315]\cite{Tauber:1961}
%[316]\cite{Taylor:2007}
%[317]\cite{Tchekhovskoy:2007zn} [copy 2]
%[318]\cite{Tchekhovskoy:2007zn} [copy 2]
%[321]\cite{Thorne:1981}
%[323]\cite{Toro:1999}
%[325]\cite{vanLeer:1973}
%[327]\cite{Vandeven:1991}
%[329]\cite{Vines:2011ud}
%[334]\cite{Woodward:1984}
%[335]\cite{Yang:2001}
%[339]\cite{Zhang:2005qy}
%[340]\cite{Zhang:2010}
%[341]\cite{Zhang:2011}
%[342]\cite{Zhang:2011a}
%[347]\cite{Zumbusch:2009fe}

%% ---------------------------------
%% ---------------------------------
%%
%%      NUCLESYNTHESIS 
%%
%% ---------------------------------
%% ---------------------------------

\chapter{Nucleosynthesis}
\textcolor{red}{Based on the Jones Lippuner PhD thesis}

%%
%%
%%

\section{Introduction}

For a long time it was believed that all elements in the universe were synthesized during the Big Bang \cite{Alpher:1948}. Later studies have, however, proven that only elements with atomic number $A<8$ were produced (see \textit{e.g.,} \cite{Alpher:1950,Shaviv:2012}). Thus, only the lightest elements, hydrogen and helium primarily, were produced during in the Big Band \cite{Burbidge:1957}. Heavier elements are synthesized in a plethora of processes outlined in that work, understanding of which has gradually grown over the years. 

In this chapter we discuss the rapid neutron capture process ($r$-process) responsible for creation of heavy elements \cite{Burbidge:1957}. We introduce conditions and cites of $r$-process, its contribution to the Universe' chemical evolution. We briefly discuss how numerous nuclear species, participating in thousands on nuclear reactions, are evolved using the \texttt{SkyNet} nuclear reaction network code. Then, we consider binary neutron star mergers as a source of $r$-process elements. We compute the nucleosynthetic yields from mergers, using the numeral relativity simulations performed with \texttt{WhiskyTHC} code (see sec. \ref{sec:code:WhiskyTHC}). 

In section... we discuss...

%%
%%
%%

\subsection{Solar system abundances}

It is of paramount importance for testing nucleosynthesis theories and models to have an accurate measurements of relative abundances in the Universe. For Humanity, confined still to just one planet, this is not a trivial task. And while automatic spacecrafts, such as Luna, Apollo and Hayabusa have delivered samples from asteroids, most of the studies are performed using the naturally falling meteorites. The composition of these guests from space is then thoroughly studied via absorption and emission spectroscopy \cite{Shaviv:2012}.

From the pioneering works is study absorption lines and evaluate relative abundances \cite{Suess:1956}, a great progress has been made in study the solar system isotope and element abundances (\textit{e.g.,} \cite{Cameron:1973,Anders:1989,Grevesse:1998,Lodders:2003}) \textcolor{red}{add last papper you used for soalr A.}

\begin{sidenote}
    \textcolor{red}{Figure with solar observed abundances, showing $r$-elements and $s$-elements}
    The observed obundacnes as a function of atomic mass number $A$ are shown in figure Fig. (XXX). (data from \cite{Lodders:2003}). The plot shows nuclei that are more bound (due to spin pairing of nucleons \cite{Moller:1993ed}), \textit{i.e,} those with an even $A$ number, are more abundant. The lowest binding energy of nuclei with odd number of neutrons and protons (but even $A$) are largely unstable or short-lived. A few exceptions are \textit{e.g.,} $^{40}$K, $^{50}$V, $^{138}$La and $^{176}$Lu, that have a half-life of at least $10^{9}$ years.
\end{sidenote}

In different mass ranges the dominant nuclearsynthesis process varies. For instance, light elements $A<8$ were synthesized right after the Big Bang. Nuclides before the iron peak, $12\leq A\leq 56$ come from stellar hydrostatic nuclear burning, elements at the iron peak $50\leq A \leq 62$ produces mostly during the type Ia supernova or explosive silicon burning in core-collapse supernova (CCSN). The conditions at these sights are such that the dense material at the nuclear statistical equilibrium (NSE), expands and cools down. Most of the material beyond iron peak are produced via neutron capture processes \cite{Burbidge:1957}.

%%
%%
%%

\subsection{Nucleosynthesis up to the iron peak}

\subsubsection{Big Bang nucleosynthesis}

Light elements in the Universe, like hydrogen ($\sim 75\%$ by mass) and helium ($\sim 25\%$ by mass) alongside trace amounts of $^{3}$He and $^{7}$Li were created during the Big Bang nucleosynthesis (see \textit{e.g.,} \cite{Tytler:2000qf} and references therein). And while only a small number of nuclides were involved in BBN, there are large discrepancies between BBN models and observations. For instance, the "lithium problem" \cite{Coc:2013eha}, which origin is not well understood \cite{Fields:2011zzb}.

%%
%%
%%

\subsubsection{Low-mass stellar burning}

In order to fuse massive nuclides and overcome the strong Coulomb barrier, high temperatures, ($\geq 10^6$ K) are required. Thus production of heavy elements from hydrogen and helium is possible only in special environments, in particular, in the interior of stars \cite{Bethe:1939}. 
For the most of their lives stars burn hydrogen into helium. The process releases the binding energy and maintains the hydrostatic stability of a star. After the hydrogen is exhausted in the core, a core (atmosphere) contracts (expands), heats up (cools), and the shell hydrogen burning is initiated, slowly depositing ashes, \textit{e.g.,} helium, into the inert core. 
The star's subsequent evolution depends primarily on its mass. If $M>0.5M_{\odot}$, at some point the helium in the core starts to fuse into $^{12}$C and $^{16}$O and small amounts of $^{24}$Mg, $^{28}$Si. These elements are called \textit{alpha elements}  \cite{Rolfs:1988,Hasen:2004}. 
The end of the core helium burning phase leads to another core contraction phase. A star of a mass $\sim 8M_{\odot}$ would be able to ignite carbon and oxygen producing heavier elements afterwards. A less massive star looses its outer layers and becomes a slowly cooling degenerate core, a white dwarf.

%%
%%
%%

\subsubsection{nuclear burning is massive stars}

For a star that has $M\geq 8M_{\odot}$, a sequence of burning stages follows, each of which leads to the exhaustion of a respective fuel, contraction of the core and rise of its temperature \cite{Woosley:2002}. Carbon burning leads to the production of $^{20}Ne$, $^{23}$Na and free protons that contribute to the synthesis of non-alpha elements. As temperature increases, the photodisintegration of $^{20}$Ne becomes possible and a small amount of $^{24}$Mg is formed. Next, the oxygen burning occures that produces $^{28}$Si $^{31}$P, and $^{28}$Si and $^{32}$S, that become dominant nuclides in the core by the end of oxygen burning \cite{Rolfs:1988}.

The subsequent silicon burning proceeds at $T\sim3.5\times10^9$K through photodissociation of some of the $^{28}$Si and a sequence of alpha particle captures, "alpha ladder", on the remaining $^{28}$Si to form $^{32}$S, $^{36}$Ar, $^{40}$Ca, $^{44}$Ti, $^{48}$Cr, $^{52}$Fe and $^{56}$Ni. This process lasts around a day. \cite{Rolfs:1988,Hasen:2004}. Due to high temperatures preset at silicon burning, nuclides with $A\in[28, 62]$ fall into quasi-equilibrium, meaning that these nuclides (with exception of $^{12}$C, $^{16}$O $^{20}$Ne and $^{24}$Mg), alpha particles and protons participating in reactions, are in equilibrium with each other. 

As the maximum binding energy per nucleon appears at $A=56$, the silicon burning cannot produce heavier nuclides with the release of energy. As the fraction of $^{56}$Ni in the core of a star increases, the support that nuclear burning has provided against gravitational contraction falls. Meanwhile the mass of the degenerate core still increases as burning proceed in shells. However, when the electron degeneracy pressure can no longer counteract gravity, \textit{i.e.,} when mass of the core exceeds the effective Chandrasekhar mass \footnote{The collapse however occur before the core reaches Chandrasekhar mass, and the pressure support that rests on the avaialbility of free eletrons drops when electrons capture on the nuclides becomes possible. To account for this, the effective Chandrasekhar mass was introduced.}, the core collapses leading to a supernova (CCSN) \cite{Woosley:2002}.

Thus, the origin of more abundant alpha elements in the Universe is stellar fusion.

%%
%%
%%

\subsubsection{Iron Peak}

At temperatures higher then $T\sim 5\times10^{9}$K nuclear statistical equilibrium (NSE) establishes. This is a balance between the fusion reactions froming a ($N,Z$) nuclide from $N$ neutrons and $Z$ protons and photodissociation reactions, splitting it back. At NSE three parameters describe the composition: density, temperature and electron fraction $Y_e = n_p/(n_p + n_e)$, where $n_e$ and $n_p$ are the (total) number density of electrons and protons respectively \cite{Seitenzahl:2009}. 

The composition at NSE favors more tightly bound nuclides, as they are more difficult to photodissociate. Thus, if the conditions allow, \textit{i.e.,} temperature, density and electron fraction of the mater, $Y_e \sim 0.46$ (an electron fraction of iron), nuclides at $A\sim56$, \textit{i.e.,} iron peak elements, dominate \cite{Seitenzahl:2009}. 

For example, NSE establishes during the type Ia supernova, when a thermonuclear explosion of a white dwarf allows for sufficiently high temperatures and densities. After the explosion, newly synthesized elements of the iron peak cools, and being stable, remain in the expanding medium \cite{Iwamoto:2000as}. 

%%
%%
%%

\subsubsection{Nucleosynthesis beyond the iron peak}

Nuclides with $A\geq 56$ cannot be synthesized via standard cycles due to their strong Coulomb barriers. Thus, processes that do not involve charged particles become dominant. These are the neutron capture processes.
As nuclides absorb neutrons and grow larger, their binding energy $Q_n$ decreases. This process is stopped when $Q_n\sim1$MeV and energetic photons start to knock out neutrons from a nucleus. This process is called photodisintegration and a location in parameter space where it occurs, that in turn dependents on temperature and density, is called the \magenta{neutron drip line} \cite{Rolfs:1988}.

Nuclides produced via neutron capture are generally unstable to a $\beta$-decay, with a timescale $\tau_{\beta}$, that can be larger or smaller than a neutron capture timescale, $\tau_n$. 
In case when $\tau_{\beta}\ll\tau_n$, \textit{i.e.,} when a $\beta$-decay occures faster then the next neutron capture, the process is called \textit{slow} or $s$-process. 
Thus, by definition, the $s$-process moves along the valley of stability\footnote{a region of stable nuclides in the nuclides chart, a chart in terms of number of neutrons $n_n$ and number of protons $n_p$.}, departing no further than by a few nuclides away.
On the other hand, if $\tau_{\beta}\gg\tau_n$, \textit{i.e,} when a neutron capture occures much faster then a $\beta$-decay, the process is called \textit{rapid} or $r$-process. This nucleosynthesis generates nuclides near (but not crossing) the neutron drip line \textcolor{red}{Here a plot, Fig.1.6 from Source would have been usefull.} \cite{Rolfs:1988}. 

Notably, the trajectory of $r$-process is interrupted when the neutrons within nuclide can arrange themselves in a closed shell. Such configuration is energetically very favorable and thus the cross section for a subsequent neutron capture reduces. Only after several $\beta$-decays does the $r$-process continue. Thus, nuclides located at points where neutron drip line and closed neutron shell overlap is more abundant. These unstable nuclides will decay back to the valley of stability and some of the neutrons within them turn to protons, reducing the total mass of the nuclide. The indication of this "overproduction" are the peaks in the abundance patterns at a mass, $A$, slightly lower then the one corresponding to a closed shell nuclide. \textcolor{red}{here a ref to a fig with Abundances is needed.}. 
A similar "overproduction" of nuclides with a closed neutron shell occurs when an $s$-process is considered. However, in that case it is caused by the cross section of these nuclides being 1-2 order of magnitude smaller then of neighboring ones \cite{Rolfs:1988}. Thus, in the case of $s$-process, the peaks in abundance pattern will be at $A$, corresponding to the closed shell  exactly, and thus higher than $A$ of $r$-process. 
Closed shell nuclides is located at $N=50,\: 82, \: 126$ and thus corresponding abundance peaks for $s$-process are at $A=88, \: 138, \: 208$ and at $A=80,\:130,\:194$ for $r$-process (see \textit{e.g.,} \cite{Arnould:2007gh}) \textcolor{red}{abundance peaks with 'r' and 's' elements is needed.}

It was found, that the solar $r$-process abundance pattern of $r$-process is consistent, and can be found anywhere in the Universe. Im particular, in stars that are formed very early on a galactic evolution timescale, the metal poor halo stars, one would expect to observe less $s$- and $r$-process elements as there might have been not enough time for the enrichment to take place. However recent studies showed that the solar $r$-process abundances are present in these stars as well \cite{Sneden:2008,Roederer:2010}, see also Figure 8 in \cite{Sneden:2009} \textcolor{red}{and section 1.9}. Thus, modelling the $r$-process nucleosynthesis it is expected to reproduce the solar abundances. \textcolor{red}{once again, a figure can be good}

It is important to note that in addition to $s$- and $r$-process, a possible $i$-process, (intermidate) is widely discussed. The process operates further from the valley of stability than $s$-process, but not reaching neutron drip line \cite{Cowan:1977,Bertolli:2013gka}. Slow and intermediate neutron capture processes operate within the low-mass asymptotic giant branch (AGB) with mass $M\in[1.5,3]M_{\odot}$ and more massive stars, that enrich the interstellar medium with heavy elements via strong winds (\textit{e.g.,} \cite{Peters:1968,Couch:1974,Kaeppeler:1994K,Woosley:2002,Straniero:2005hc,Herwig:2011}). The possible cites for the $r$-process we discuss in the following subsections.

%%
%%
%%

\subsubsection{Possible $r$-process sites}

Study of possible cites of $r$-process is a wide and rapidly developing field. 
The general requirement for $r$-process is a low electron fraction, or, in other words, neutron-rich conditions. These can be found in a various places for certain models of Big Bang, that include inhomogeneities to neutron star merger and supernova ejecta (see \cite{Mathews:1990} and references therein). Spectral study of metal-poor (MP) stars, combined with models of galactic chemical evolution sheds light on possible dominant cite of $r$-process material. \textcolor{red}{add more sources/models}. It is now believed that certain types of supernavie and neutron star mergers are the most likely sources on $r$-process material. \cite{Mathews:1990,Thielemann:2011} \textcolor{red}{add more sources}

%%

\paragraph{Core-collapse supernovae}

Collapse of a massive star produces a hot neutron core, that undergoes deleptonization, releasing $\sim10^{53}$~erg of binding energy in form of strong neutrino flux. These neutrinos, irradiating dense medium around the core, can produce a neutrino-driven wind \cite{Qian:1996xt}, that was suggested to be a promising cite for $r$-process \cite{Woosley:2002,Wanajo:2006mq}. Later, it was shown however, that the electron fraction in the wind would be too high for a full $r$-process, and only "light" heavy nuclide up to $A\sim130$ can be synthesized. \cite{Qian:1996xt,Thompson:2001ys,Fischer:2010,Roberts:2010,MartinezPinedo:2012rb,Wanajo:2013} \textcolor{red}{add Perego:2017} 

It is important to note, in the proton-rich neutrino-driven wind nuclides with $A\sim 100$ can be produces via so-called $\nu p$-process. The process relies on a creation of a free neutron from proton by an antineutrino capture. This free neutron can then be captured by a seed nuclide, $^{64}$Ge seed nuclide and thus nuclides heavier then $^{64}$Ge can be created \cite{Frohlich:2006,Pruet:2005qd,Wanajo:2010mc,Arcones:2012}

A full $r$-process can be achieved in so-called magnetorotationally driven core collapse supernovae. This is a rare type of CCSNe, where a core of a progenitor is rotating rapidly and strongly magnetized. Induced by a magnetorotational processes \textit{e.g.,} magnetorotational instability, collapse is accompanied by a formation of a collocated bipolar jet \cite{Wheeler:2000,Akiyama:2003,Burrows:2007yx,Mosta:2014jaa,Mosta:2015} \textcolor{red}{add Siegel2019?.}. Materiel in these jets is predicted to be sufficiently neutron rich to allow for a full $r$-process nucleosynthesis \cite{Winteler:2012,Nishimura:2015nca}. The rarity of this type of supernovae, however, might not allow it to be the dominant source or $r$-process material \cite{Nishimura:2015nca}. 

%%

\paragraph{Neutron star mergers}

Mergers of two neutron stars or a neutron star and a black hole are regarded as one of the main cites of $r$-process material. Formed in a evolution of a binary of massive stars, compact objects orbit each other for gigayears, before slow loss of energy from the system due to gravitational waves reduces their orbit and they merge \cite{Hulse:1975,Lattimer:2004sa,Price:2006fi}. 
The late inspiral and merger of binary neutron stars (BNS) or a neutron star and a black hole (NSBH) have been studied extensively via smooth particle similation \red{[REFS]} hydrodynamic simulations \red{[REFS]} and numerical relativity simulations with simplifed gravity \red{[REFS]} or full general relativity \red{[REFS]}. The compoisition of a neuttron star in these simulations have been modelled with simplifed polytropic \red{[REFS]} or pice-wise polytropic \red{[REFS]} or microphsyiccal equaiton of states (EOS) \red{[REFS]}. The physical setup of these simulations have also evolved to eventually include effects of neutrino radiaiton transport \red{[REFS]} and magnetic fields \red{[REFS]}. 
These studies have shown that shortly before and during the merger, the neutron star(s) undergo(s) a tidal deformation and disruption. Formed streams of neutron reach matter are then ejected into the circombinary with enough energy to be not graviationally bound to the system \cite{Price:2006fi,Foucart:2014nda,Sekiguchi:2015dma,Kyutoku:2015gda,Radice:2016dwd}. 
In addition, in case of a BNS, when neutron stars collide, material at the collision interface, heated by shocks, gets 'squeezed' and launched in the directions perpendicular to the plane of the binary \cite{Bauswein:2013,Hotokezaka:2013b} \red{[REFS]}. 
Generally, these tow components, tidal and shocked, constitute the \textit{dynamical ejecta}. Where the term ejecta referrers to the material that has enough energy to leave the system. 
The properties of the dynamical ejecta from BNS have a broad distribution, especailly in terms of mass and ejectron fraction [\red{[REFS]} \& myFitPaper], where the former lies in range $(10^{-4},10^{-2})M_{\odot}$ and the latter $(0.05,0.45)$. We discuss dynamical ejecta properties of BNS in more details in section \red{sec:results:dyn\_ej:prop} and nucleosynthesis in it in \red{sec:results:dyn\_ej:nucleo}. In case of NSBH the ejecta mass was shown be larger, reaching $0.1M_{\odot}$ with low electron fraction, $\leq0.2$ but it requires that masses of BH and NS are comparable and BH is rapidly spinning \cite{Foucart:2014nda}\red{[REFS]}. If the BH is much more massive then NS, the latter would be 'swallowed' with no ejecta \red{[REFS]}.

After the merger, there are expected to be additional ejecta. For general postmerger configuration consists of a remnant, massive neutron star (MNS) or a black hole sorrounded by a disk (torus) of bounded matter. In the first case,  a strong neutrino flux from cooling MNS and disk can drive an outflow in the direction, perpendicular to the plane of the binary, the so-called \textit{neutrino-driven wind} (see Figure 1 from \cite{Perego:2014fma}) \red{[REFS],Jujibayashi+20}. This ejecta is expected to occure on a timescales of $\sim100$ms postmerger, be not very neutron rich $Y_e\sim(0.2-0.45)$ due to neutrino irradiation and have a mass of $(10^{-4}-10^{-3})M_{\odot}$ \red{[REFS]}. 

The massive nutron star born in a merger exhibit dynamical oscillations \red{[REFS]}. The $m=1$ mode, so-called "one-armed spiral instability" especially can persisit on a $\sim100$ms powtmerger timescale and become a dominant mode \red{[REFS], MainPaper}. This oscillations can inject energy within the disk, where via angular momentum transport it leads to an outer part of the disk to become unbound. This ejecta, the \textit{\swind{}} was shown to occur in all cases where the MNS is present. It has high electron fraction and its mass depends on a lifetime of the remnant, and for a $\sim100$ms it can amount to a few $\times\sim10^{-2}M_{\odot}$ \red{[Letter, MainPaper]}. We discuss the mechanism that drives the \swind{} in the section \red{sec:results:swind:mechanism} and the ejecta properties in \red{sec:results:swind:prop} and corresponding nucleosynthesis in \red{sec:results:swind:nucleo}.

On a longer timescales, the viscous processes and alpha recombination in the disk, surrounding MNS or a BH are expected to unbind additional material. This is so-called \textit{secular ejecta}. It is expected to be massive and neutron rich. However, due to long timescales involved, it is very difficult model \red{[REFS]}.

The BNS and NSBH mergers thus ejecta a neutron rich material in which $r$-process can take place, producing nuclides beyond $A=300$. However thous are unsubtle to fission and decay. But before they reach the valey of stability they capture more free neutrons and grow up to $A=300$ and the cycle repeats. This is so-called fission cycle. It is maintained as long as there are free neutrons. After then the nuclides decay to the valley of stability for the last time, forming the remarkably robust abundance pattern, independent of the number of cycles \cite{Korobkin:2012uy,Bauswein:2013,Mendoza-Temis:2014mja}, (see also Figure 4 from \cite{Korobkin:2012uy}).\cite{Korobkin:2012uy}

%% 

\subsubsection{Galactic chemical evolution}
\red{bad structure in the source.}

Numerical models have shown that the final $r$-process abundances in the BNS and NSBH mergers ejecta are robust and reproduce the solar ones robustly. \cite{Freiburghaus:1999,Goriely:2011vg,Goriely:2015fqa,Wanajo:2014,Just:2014,Radice:2016dwd}\red{[Refs]}. Recent observations of the one and only detected so far merger have confirmed that BNS is a cite of $r$-process \red{[Refs]}.
However, the question of weather BNS (NSBH) is a dominant cite or $r$-process nucleosynthesis remains open \cite{Qian:2000bh,Argast:2003he,Matteucci:2014}. 
In particular as it is difficult to explain the observed enrichment of very metal-poor stars (MP) with $r$-process elements. In addition, the observed rather uniform $r$-process abundances in the Galaxy is not very well explained.

%% Delay
After the Big Bang nucleosynthesis, the Universe consisting of hydrogen and helium, with traces of lithium, have expanded and cooled. Under the influence of dark matter, the primordial gas fragmented, clumped and first stars, galaxies and galaxy clusters have formed. During their lifetime the first stars (population III stars) converted light elements into heavier ones and then ejected them into the interstellar medium (ISM) during supernova events. Future populations of stars were born of gas enriched with heavy elements, in particular, iron. Thus, the amount of elements heavier then hydrogen and helium is stars (\textit{i.e.,} metallicty) increased with each stellar generation and an there exists an age-metallicity relation  \cite{Matteucci:2012}. Important to note, that multiple dark matter sub-halos contributed to the formation of the galaxy and there might not be a unique age-metallicty relation (see \textit{e.g.,} \cite{Ishimaru:2015} and references therein).
%% Delay
The enrichment of interstellar medium with heavy elements from stellar interior occurs immediately after stars die. However, the $r$-process elements, produced in BNS (NSBH) mergers can only enrich ISM when compact objects inspiral and merge which on average takes $(0.1-1)\times10^{9}$ years \cite{DeDonder:2004cx,Dominik:2012kk}. The exact delay time is however highly uncertain and depends on a poorly understood common envelop evolution phase of the binary (progenitors). And it was shown, that a small percentage of compact binaries might form with a time delay before merger as small as $10^{6}$ years \cite{Dominik:2012kk}. 

%% Study observations
To study the chemical evolution of stars in the galaxy, the Spectroscopic surveys \footnote{And indicative quantity of metallicty measured in such iron-to-hydrogen ration, [Fe/H],that reads as a $\log_{10}$ of the abundance of a element $X$ to hydrogen, normalized to solar ration, \textit{i.e.,} in the sun for every X, [X/H]$= 0$. If a stars has [Fe/H]$=-2$, it is said that this star ahs a 100 times less iron compared to hydrogen then sun.} are conducted \cite{Edvardsson:1993,Suda:2008na}. 
%% Scatter
Mergers of compact objects are rare events and thus expected to introduce a considerable scatter into the $r$-process elements distribution in the Galaxy. However, observations show that the distribution is more uniform than expected \cite{Argast:2003he}.
%% Scatter
However, recent advances population synthesis models, have indicated that with a contribution from magnetorotationallydriven CCSNe the compact object mergers can account for the observed scatter of heavy elements \cite{Ishimaru:2015,Cescutti:2015,Wehmeyer:2015,VanDeVoort:2015}.

%% 244Pu
Comparison between the solar system and earth crust abundances of $^{244}$Pu have indicated that this nuclide might have been produced rare events with high yield \cite{Wallner:2015}. This statement was confirmed via models of galactic mixing \cite{Hotokezaka:2015zea}, that also showed that there appears to be no degeneracy between rare high yields events (BNS/NSBH) and frequent low yield ones (CCSN). Similar conclusion was draws from studying $^{244}Pu$ abundances in meteorites \cite{Tsujimoto:2017}.

%% UFDG
Study on untrafaint dwarf galaxies (UFG) also point towards a rare high yield events for $r$-process nucleosynthesis. In particular, the UFG Reticulum II was shown to have a solar $r$-process abundances, while UFG of similar type tend to have 2-3 times less $r$-process elements. This suggests that a rare high-yield event has modified Reticulum II chemical composition and high europium abundances indicate that it was a compact object merger \cite{Ji:2016}. 

Thus it is still unclear whether observed degree of homogeneity in $r$-process elements distribution in the galaxy and $r$-process elements enrichment of VMP stars can be explained by object mergers only. 
%% Scatter

\red{Mention Actinide abundaces problem.}

%%

\subsubsection{Kilonova}

Nuclides, synthesized in $r$-process ave very neutron rich and unstable. After the last fission cycle, they decay to the valley of stability. This process takes from hours to days and released energy can power an electromagnetic transient, called Kilonova or Macronova (\textit{e.g.,} \cite{Li:1998bw,Kulkarni:2005jw,Metzger:2010,Roberts:2011,Metzger:2016pju,Wollaeger:2017ahm})
%
In 2017 such electromagnetic signal, AT2017gfo after the gravitational wave, GW170817, one 
confirmed that indeed, $r$-process nucleosynthesis takes place in the ejecta from merger of compact objects, in that case of two neutron stars \red{[Refs]}. 
% [RED]
Kilonova has a complex observational signature due to different ejecta components with various compositions contribution to it. In particular, it was shown that if produced, lanthanides $(58\leq Z \leq71)$ and actinides $(90\leq Z \leq 103)$, that have an open $f$-shell and hence a multitude of absorption lines, increases the opacity of emitting region by a factor of $10$. These elements are produced only if electron fraction of the ejecta was sufficiently low, $Y_e\leq 0.25$ \cite{Lippuner:2015gwa}, present for example in a tidal component of dynamical ejecta.High opacity would imply that a transient is dim $(\sim10^{40})$ erg s$^{-1}$ and peak around a weak after merger in the red/infrared band \cite{Barnes:2013wka,Grossman:2013lqa,Lippuner:2015gwa}. 
% [BLUE]
If the ejecta electron fraction is high, $Y_e \geq 0.25$, weak $r$-process would produce a small amount of lanthanides and the opacity of the emitting region would thus be lower. The kilonova signal originating from such ejecta would be bright and peak on a timescale of a few days in blue band \cite{Kasen:2014toa,Martin:2015hxa}. 
%
Indeed, both blue and red components were observed in the case of AT2017gfo, that confirmed the general picture \red{[Villar2017]}. However, estimated mass of the ejecta required to explain the red component is larger then what is predicted by numerical relativity simulations \red{refs}. It is believed that the most contribution to this component comes from the low $Y_e$, slow but massive outflow from the degenrate disk on a secular timescale \red{refs}.
%
In a high velocity tale of the dynamical ejecta, the neutrons might avoid being captuired on a seed nuclide and freely decay, producing a short, a few hours, and bright ultraviolet precursor \cite{Metzger:2014yda}. Unfortunately, in case of AT2017gfo, the electromagnetic followup started \red{11} hours after the gravitation wave detection.
%
See Figure 6 from \cite{Metzger:2016pju} for an examples of lightcurves of Kilonova and precursor.
%
A very high energy emission from the non-thermalized radiation is weak and can be detected only for a sufficiently close event \cite{Hotokezaka:2015cma}.
% 
Prior to AT2017gfo, the were candidates to based onm the detection of short gamma ray bursts, \textit{e.g.,} GRB130603B, \cite{Berger:2013wna,Tanvir:2013pia}, GRB060614 \cite{Jin:2015txa,Yang:2015pha}, GRB050709 \cite{Jin:2016pnm}. However the exact nature of the observed signals were not strongly confirmed. 
%
The search for electromagnetic counterparts to mergers continues, involving observatories around the world \cite{Law:2009,Singer:2014qca,Bellm:2014,Kasliwal:2016uhu}.
%
\red{motivation why it is important to model nucleosynthesis}


\red{Subsection of GRBs}
\cite{Lee:2007js,Nakar:2007yr,Gehrels:2009,Fernandez:2015use}

%%
%%
%%

%%
%%
%%

\section{SkyNet: A modular nuclear reaction network library}

%%

\subsubsection{Introduction}

Nuclear reactions play an important role in numerous astrophysical cites, where sufficiently high temperatures and densities are achieved. In particular, in main sequence stars nuclear fusion prevents stars from collapsing, releasing the binding energy, counteracting gravity \cite{Bethe:1939}. When stars do undergo a core collapse, the nuclear and weak reactions govern the explosion by introducing and removing energy (\textit{e.g.,} via neutrino cooling and heating). 
%
A small amount of the material synthesized in the explosive burning is ejected into the ISM  \cite{Nomoto:1997if,Woosley:2002}. It is composed mostly of iron-group elements, enriched with heavier elements synthesized in weak $r$-process \cite{Wanajo:2013} and $s$-process \cite{Burbidge:1957}. 
Enrichment of ISM with even heavier elements, synthesized in full $r$-process, is believed to occur manly in compact binary mergers \cite{Freiburghaus:1999}, \red{where the material with ifferent composition is ejected on a prolonged timescale via numerous physical processes [ManyRefs]}. \gray{In addition, nuclear burning in a form of thermonuclear explosion occurs when matter is accreted onto a white dwarf or a neutron star, producing electromagnetic transients Novae and X-ray bursts \cite{Boyd:2008,Freiburghaus:1999}}
%
To study the aforementioned phenomena, it is necessary to model nuclear reactions and not only focusing on the total energy generation (\textit{e.g.,}  \cite{Weaver:1978,Mueller:1986,Timmes:1999}) but tracking the evolution of the whole ensebmle of nuclear species \red{[Refs]}. A mathematical and/or numerical model that describes coupled nuclear reactions, tracking the evolution of abundances of various species, coupled by a non-liear reaction, rates is called \textit{nuclear reaction network}.
% BBN
The complexity of networks, \textit{i.e.,} the number of species and reactions evolved, varies depending on the intended implementation. For instance, for a Big Band nucleosynthesis rarely more then a dozen nuclear species are evolved 
\cite{Wagoner:1973,Orlov:2000,Nollett:2000fh,Coc:2013eha,Cyburt:2015mya}. 
% STellar
Stellar evolution models also included reaction networks starting from a few species and reactions (\textit{e.g.,} \cite{Hayashi:1962,Hofmeister:1964}) and incorporating tens or even hudred of them in a more recent and advanced models \cite{Arnett:1977,Weaver:1978,Paxton:2011,Bressan:2012,Jones:2015}
% SN
Networks designed for explosive nuclear burning and supernovae evolved from including a few tens of species (\textit{e.g.,} \cite{Truran:1966,Truran:1967,Arnett:1969,Woosley:1973}) to including hundreds of species and reactions, (\textit{e.g.,} in type Ia SN \cite{Thielemann:1986,Hillebrandt:2013gna,Seitenzahl:2013,Leung:2015fxa}, in core-collapse supernovae \cite{Thielemann:1986,Limongi:2003ui,Heger:2008td,Harris:2014}, in novae \cite{Weiss:1990,Jose:1997vf,Iliadis:2002zz,Starrfield:2016} and X-ray bursts \cite{Schatz:2001xx,Woosley:2003cd,Cyburt:2010,Parikh:2012hx}) 
% r-process 
Neutron captrue results in a creation of unstable nuclides, that decay via complex chains of reaction, involving hundreds and thousands steps. Thus, nuclear reaction networks has to adequately complex for modelling the $s$-process, (\textit{e.g,} \cite{Prantzos:1990,Kaeppeler:2011,Nishimura:2017zdi}) and even more so to model $r$-process. In particualr, in CCSN neutrino driven winds (\textit{e.g.,} \cite{Woosley:1992,Arcones:2010,Wanajo:2013}) in outflows from magnetorotational CCSN (\textit{e.g., \cite{Winteler:2012,Nishimura:2015nca}}. In compact object mergers several cites with different conditions require modelling. In particular, in dynamical ejecta \cite{Goriely:2011vg,Bauswein:2013,Wanajo:2014,Just:2014,Fernandez:2016sbf}\red{[Refs]}, in the disk (torus) formed after merger around the remnant \cite{Surman:2008qf,Perego:2014fma,Martin:2015hxa,Lippuner:2017tyn}\red{[Refs]}. This however does not limit the scope of where modeling of neutron capture nucleosynthesis is done. See \cite{Blinnikov:1996,Panov:1995,Panov:2001,Mumpower:2011ar}\red{[Refs]} for overview.
% modelling the network
The key component of a nuclear reaction network is the interaction between two and more nuclides, that are cahraterized by reaction rates (RR) as well as single particle reactions, such as $\beta$-decay.
Charged particle reactions require that the Coulomb barrier to be overcome. Thus, in general, RR depend strongly on the particle energy as well as on resonances in compound nuclear systems (\textit{e.g.,} \cite{Clayton:1968}, \textsection{4}). 
In a the single-particle reactions, \textit{e.g.,} $\beta$-decay, it is common to assumed a constant RR, which is strictly speaking is true only in vacuum. Another common approach to treating complex reactions, is to assume that reactants from a single nuclide that undergoes a chain of decays into reaction products. This is called a Hauser-Feshbach approach. It is particulalrly useful for reactions far from the valley of stability with a long chains of decay \cite{Hauser:1952,Rauscher:2000fx,Goriely:2008zu}. 
In addition, in order to model the nuclear statistical equilibrium (NSE) and $\beta$-decay rates, nuclide masses and partition functions (\cite{Arcones:2010dz,Brett:2012jn,Mendoza-Temis:2014mja,Mumpower:2015ova}) have to be prescribed. This data is not fully available and have large uncertainties (see \textit{e.g.,} \cite{Lunney:2003,Schatz:2013,Mumpower:2015ova} and references therein). Thus, often theoretical models for numcear masses and $\beta$-decay properties are used for many species \cite{Lunney:2003,Moller:2003,Mumpower:2015ova}. 
% Other NRN codes
Several nuclear reaction networks are available in the literature. In particular: a set of networks by 
\cite{Timmes:1999} \footnote{\url{http://cococubed.asu.edu/code_pages/burn.shtml}}, 
\texttt{XNet} by \cite{Hix:1999} \footnote{\url{http://eagle.phys.utk.edu/xnet/trac}} , 
\texttt{NucNet} by \cite{Meyer:2007} and a  \footnote{\url{https://sourceforge.net/projects/nucnet-tools}}
\texttt{SkyNet} by \cite{Lippuner:2015gwa} \footnote{\url{https://bitbucket.org/jlippuner/skynet}}.
% 
In this thesis we employ \texttt{SkyNet} to compute nuclesosynthetic yilds in binary neutron star mergers ejecta. Thus we discuss this network in more details. \texttt{Skynet} is a modular and versatile nuclear reaction network designed initially for $r$-process nucleosynthesis calculations. It is capable of evolving arbitrary set of nuclear
species in NSE as well as switching to a full network self-consistently. The network incorporates screening corrections and equation of state that consideres an entire composition. 
% 
\texttt{SkyNet} is a popular choice for studying nucleosynthesis in neutron rich environment \cite{Lippuner:2015gwa,Radice:2016dwd,Roberts:2016igt,Lippuner:2017tyn,Siegel:2017nub,Vlasov:2017nou,Fernandez:2016sbf} \red{[Refs(us,david)]}, .

%%
%%
%%

\subsubsection{Nuclear reaction network basics}

Nuclear reaction networks allow to track the evolution of a system of many species that undergo nuclear transmutations in prescribed reactions given the reaction rates. The rate equations describe microscopic processes. They are based on the kinetic theory. For instance, a system of non-correlated particles, involved in interactions, some of which can change particle type, can be described in terms of  ad distribution function. The evolution of this function is given by Kinetic theory \red{Danielewicz and Bertsch, 1991; Buss et al., 2012}. 
Generally, only a subset of particles is evolved by a reaction network, while for some the conditions are assumed to be unchanged. For instance, photons are assumed to be in equilibrium at all times, while electron and positron densities are set by the charge neutrality. 
% Abundances
Describing particle interaction and nuclide transmutation it is common to introduce the entrance and exit channels representing reactants and products. Then, reaction rate is defined as a speed at which a reaction proceeds per particle in the entrance channel. Thus, if there is no change between particles in entrance and exit channels, the RR is zero. A particular usefull quantity is \textit{abundance} $Y_i$, defined as 

\begin{equation}
    \label{eq:theory:nuc:abundance}
    Y_i = \frac{n_i}{n_B} = \frac{N_i/V}{N_B/V} = \frac{N_i}{N_B},
\end{equation}

where $V$ is the volume of the fluid element, $N_i$ and $N_B$ are the total numbers of particles of species $i$ and baryons respectively. It is usually abundances of $Y_i$ that are evolved in nuclear reaction networks instead of particle number densities $n_i$ as the latter depend on the volume that often is not constant. 
The abundance evolution equation reads

\begin{equation}
    \frac{\text{d}Y_i}{\text{d}t} = \sum\lambda_{\alpha}(-R_{i}^{\alpha}+P_{i}^{\alpha})N_{i}^{\alpha}\prod_{m\in\mathcal{R}_{\alpha}}Y_m^{N_{m}^{\alpha}}
\end{equation}

where $\lambda_{\alpha}$ is the reaction rate of the forward process, $R_{i}^{\alpha}$ are reactants, $P_{i}^{\alpha}$ are products, $N_{i}^{\alpha}$ number of particles of the species $i$ involved and $Y_m^{N_{m}^{\alpha}}$ are the abundances of the particles of psecies $i$ involved (see e.g., \cite{Hix:1999}).
The \textit{SkyNet} solves a coupled, first-order, non-linear system of equation, \eqref{eq:theory:nuc:abundance} for a given set of reaction rates $\lambda_{\alpha}$.
The Eq.~\eqref{eq:theory:nuc:abundance} can be understood as following. The time derivative of the species $i$ abundances is given by the sum over all reactions, in which the species in question particiepate. Each reaction contribution consists of multiplies: reaction rate, a factor describing creation or destruction of particles (of species $i$), \textit{i.e.,} number of particles, and an abundances of reactants. 
A representative example is carbon burning $^{12}\text{C} + ^{4}\text{He} \leftrightarrow ^{16}\text{O}$:

\begin{eqnarray*}
    \frac{\text{d}Y^{12}\text{C}}{\text{d}t} &= -\lambda_{\alpha}Y_{^{12}\text{C}} Y_{^{4}\text{He}} + \lambda_{\alpha'}Y_{^{16}\text{O}} + \dots , \\
    \frac{\text{d}Y^{4}\text{He}}{\text{d}t} &= -\lambda_{\alpha}Y_{^{12}\text{C}} Y_{^{4}\text{He}} + \lambda_{\alpha'}Y_{^{16}\text{O}} + \dots , \\
    \frac{\text{d}Y^{16}\text{O}}{\text{d}t} &= \lambda_{\alpha}Y_{^{12}\text{C}} Y_{^{4}\text{He}} + \lambda_{\alpha'}Y_{^{16}\text{O}} + \dots . \\
\end{eqnarray*}

\paragraph{Reaction rates and velocity averaged cross-sections}

% On the particle distribution function: GENRAL
In general, the particle distribution function has a complex dependency on the macro- and micro- physical parameters. However, scattering processes, redistribution energy between nuclear species, bring particles into thermal equilibrium. Then, the distribution function and reaction rates, simplify considerably and depend only on temperature and chemical potential. 
% On the Dist.Func. of Boltzmann particle
In addition, in most applications the distribution function for Boltzmann particles is assumed to be given by thermal Maxwell-Boltzmann distribution. Then the reactions in which nuclei and photons participate depend only on temperature and density.
% On the neturinos 
Leptons, \textit{e.g.,} neutrinos, are generally not included into a nuclear reaction network explicitly, as their distribution is often non-thermal and difficult to treat. The electron-positron chemical potential is set to obey the charge neutrality. Then, the weak reaction rates depend, besides temperature, number density of baryons and electron fraction, on the predefined neutrino distribution function (see e.g.,Reddy et al., 1998 for two-particle charged currentweak interaction description and \red{section 2C})). 
% Cross section
An important quantity entering the nuclear networks is the reaction cross-section. Considering incoming particles, projectiles, $j$, scattering on stationary targets $i$, it the $\sigma_{\alpha}$ can be deined as a ration of the number of reactions per target, $i$, over the flux of incoming projectiles, $j$. For Boltzmann particles, then, the reaction rate reads 

\begin{equation}
    \lambda_{\alpha} = N_{\alpha}n_{B}\langle\sigma_{\alpha}\nu_{\text{rel}}\rangle
\end{equation}

where $N_{\alpha}$ accounts for not double counting particles entering reactions, $\nu_{\text{rel}}$ is the relative velocity between particles and $n_B$ is the number density of baryons. The expression in $\langle\cdot\rangle$ states the cross-section averaged ober the relative velocities between two given particles. 
The relaction between reaction rate and cross-section then reads 

\begin{equation}
    r_{i,j} = n_1 n_2 N_{\alpha} \langle\sigma_{\alpha}\nu_{\text{rel}}\rangle.
\end{equation}

See \textit{e.g.,} \cite{Clayton:1968}, \paragraphmark{4}; \cite{Rolfs:1988}, \paragraphmark{3}

\paragraph{Nuclear Statistical Equilibrium (NSE) and inverse reaction rates}

In addition to forward reactions, inverse ones if are not too unlikely, play a significant role in nucleasynthesis calculations. Considering the strong reactions. For instance, the inverse reaction to the neutron capture can be even more frequent than the forward process. At high enough temperatures the photodissociation reaction becomes as frequent the fusion. 
When the rate of a forward reaction becomes equal to that of the inverse, the reaction is said to be in equilibrium. 
If all reactions considered are in equilibrium, it is said that a nuclear statistical equilibrium (NSE) is established. This condition can also be thought of as, a mixture of free protons and neutrons creating a nucleus which is dissociated back into free neutrons and protons. In other words, when composition enters the NSE, forward and inverse reactions become in equilibrium. The NSE composition can benerally be computed from the equality of chemical potentials. 
The abundance of species $i$ in chemical equilibrium, assuming Boltzmann gas, is given 

\begin{equation}
    Y_{i;\text{eq}} = \frac{G_i(T)}{n_B}\Bigg(\frac{m_i T}{2\pi}\Bigg)^{3/2}\exp\Bigg(\frac{\mu_i - m_i}{T}\Bigg),
\end{equation}

where $G(T)$ is the internal partition function of species $i$, $m_i$ is its rest mass, $\mu_i$ is its chemical potential, $T$ is the temperature.

%%
%%
%%

\subsubsection{Network Evolution}
\red{physics from previouse subsection implemented in SkyNet}
In essence, a nuclear reaction network evolves a large system of coupled non-linear ordinary differential equations (ODEs) given in a form of \eqref{eq:theory:nuc:abundance}. As reaction rates can span order of magnitude, the system of ODEs is very stiff \cite{Timmes:1999,Hix:2005pf}, and implicit ODE solvers are mode suited for it \cite{Timmes:1999,Winteler:2012,Longland:2014}. \gray{or specifically adapted explicit methods \cite{Feger:2011,Guidry:2011,Guidry:2012,Guidry:2011a,Guidry:2011b,Brock:2015}}. 
\texttt{SkyNet} relies on the first-order implicit backward Euler method \cite{Hix:1999}. \red{Check}

\begin{sidenote}
    For temperature and density, functions of time, the network evolves the composition vector $Y(t)$ via implicit backward Euler method, obtaining the abundances at the end of the timestep as well as $\dot{Y}(Y,T,\rho)$, time derivative of the abundances, from known abundances at the beginning of it. 
    The problem amounts to a multi-dimensional root-finding problem. \texttt{SkyNet} uses Newton–Raphson (NR) method. The Jacobian matrix is $N\times N$ for $N$ nuclear species and can be expensive to invert and store. But most of its entries are zero, as not all species are directly related via a single reaction. 
\end{sidenote}
\begin{sidenote}
    \textbf{Self-heating evolution}
    The density as a function of time is usually given, \textit{e.g.,} from hydrodynamc simulations. Temperature, however, in most cases does not include heating from nucleosynthesis. On the other hand, nucleosynthesis itself depends on the fluid thermodynamics, as Kinetic theory and balance equations state. Simultaneously, binding energy released in nuclear reactions affects fluid state. Thus a \textit{self-heating} network evolution is requited. This is done following the First Law of thermodynamics, in which system entropy, volume and composition are treated as independent variables.
    The system takes into account external heating (per baryon) $\dot{\epsilon}_{\text{ext}}$ and neutrino heating cooling $\dot{\epsilon}_{\nu}$.
    In \textit{SkyNet} the implementation is such that while abundances are evolved implicitly,
    the entropy is computed explicitly (at the end of the timestep, from the one at the beginning of the imte step, using NR root-finder, after that the temperature is evalated from EOS)
    \red{[it was mentioned that fullly implicit scheme was planned]}
    The total heating rate, $\dot{\epsilon}_{\text{tot}}$ is computed as following:
    \begin{equation}
        \dot{\epsilon}_{\text{tot}} = \frac{\Delta q}{\Delta t} + \dot{\epsilon}_{\text{nuc}} = -\dot{\epsilon}_{\nu} + \dot{\epsilon}_{\text{ext}} - \frac{1}{\Delta t}\sum_i \Delta Y_i \mathcal{M}_i
    \end{equation}
    where $\Delta q$ is the infinitesimal heat added to the system from the surroundings per baryion, \textit{i.e.,} $\delta Q/N$ and $Y_i=N_i/N_B$ are the abundances for which $\sum_i Y_i A_i = 1$, $\epsilon_{\text{ext}}$ is the external heating rate (per baryon), $\mathcal{M}_i=m_i - A_i m_u$ is the mass excess, $\epsilon_{\nu}$ is the neutrino heating/cooling rate of the system on the environment, $\epsilon_{\text{nuc}} = (-1/\Delta t)\sum_i\Delta Y_i \mathcal{M}_i$ is the rate of release of binding energy in nuclear reactions.
    Notably, $\dot{\epsilon}_{\text{tot}}$ is generally composed of different components \cite{Barnes:2016umi}, depending on thermolisation efficiency. For instance, kinetic energy of massive fission fragments thermilizes very efficiently, while light neutrinos do not. \red{this is planned to be taken into account for Kilonca calculations} 
\end{sidenote}

\begin{sidenote}
    \textbf{Convergence criteria and time stepping}
    Timestep depends on the convergence of the NR rootfinder. 
    Convergence criterion for a NR:
    \begin{equation}
        \sum_{x_i^{n+1}\geq Y_{\text{thr}}}\Bigg|\frac{x_{i}^{(n+1)} - x_{i}^{(n)}}{x_{i}^{(n+1)}}\Bigg| < \epsilon_{\text{tol},\Delta x}
    \end{equation}
    where $x_{i}^{(n+1)}$ is the $i$-th vompoennt of the vector $x_{n+1}$. Here $x$ is the unknown abundances at the end of the timestep. Usually, $\epsilon_{\text{tol},\Delta x}$ is set to $10^{-6}$.
    Another convergence criterion: assureance of the total baryon number conservation 
    \begin{equation}
        \Bigg| 1- \sum_i x_i^{(n+1)}A_i \Bigg| < \epsilon_{\text{tol},\text{mass}}
    \end{equation}
    where $x_{i}^{(n+1)}$ represents abundances $Y_i(t+\Delta t)$
    After a successfull timestep a cheme tries to increase timestep, if not limited by a rapid growth of a given abundances. 
    \gray{A proper way to renormalize abundances $Y_{i,\text{new}} = Y_i/\sum_i A_i Y_i$
    to assure that $\sum_i A_i Y_i = 1$ exactly}
\end{sidenote}

\begin{sidenote}
    \textbf{NSE evolution mode} 
    \texttt{SkyNet} tracks the composition evolved, and if it approaches NSE, it automatically switches from full network evolutiom (where jacobian approaches numerical singumlarity as partial derivates become zero), to the NSE scheme. 
    In NSE mode, \texttt{SkyNet} evolves only the termodynamic quantites, \textit{i.e.,} entropy $s$ and electron fraction $Y_e$ of the composition, following the weak reactions ($\beta$-decay, neutrino reactions). The temperature is computed from EOS. 
    Two coupled ODEs are obtained for $Y_e$ and $s$ that are evolved via adaptive step-size Runge-Kutta method, an explicit integration method, that is allowed for a not-too-stiff equations.
\end{sidenote}

\begin{sidenote}
    \textbf{Electron screening}
    A Coulomb (electron) screening occurs when the nuclear reactions proceed at sufficiently low temperatures and high densities where electron gas to becomes non-uniform, centered around nuclei, creating an electron charge. This reduces the repulsion of two positively charged nuclei, enhancing the RR. (Note that for nuutron capture screening does not affect RR).
    Naively, the effectivness of the screening depends on the ratio between Coulumb interaction energy (between nucleus and nearby electrons) to the thermal energy. If the latter is large, the electron gas is diffuse and screening is ineffective.
    \gray{SkyNet assumes that the ions (nuclides) are non-degenerate and non-relativistic.
        Therefore, they obey Boltzmann statistics}
    \gray{Many subsections on the theory of screening.}
\end{sidenote}

%%
%%
%%

%%
%%
%%

\section{r-Process lanthanide production and heating rates in kilonovae}
\red{On the lanthanaid production, opacities and kilonova of BNS ejecta MODIFIED[]}

Neutron star mergers (BNS) or neutron star-black hole mergers (NSBH) emit gravitational waves \cite{TheLIGOScientific:2014jea} and eject a small amount of mass via dynamical processes \cite{Lattimer:1977}. 

%% GRB
%They also power short gamma-ray bursts (sGRB) (\textit{e.g.,} Lee and Ramirez-Ruiz, 2007; Nakar, 2007; Gehrels et al.,
%2009). An example of such merger is the GW170817 event, a merger of two neutron stars that was observed as a source of graitatial waves, electromagnetic transient, Kilonova, and unusually dimm short, off-axis, sGRB. \cite{NEW PEOPLE}.

%% RADIO EMISSION (Ejecta)
%\gray{The ejecta material, interaction with interstellar medium (ISM) results in an additional electromagnetic counterpart in primaraly, radio bnand, on a timescale of a few weeks. \cite{Nakar:2011cw} }

Neutron-rich material ejected during the merger, undergoes the $r$-process nucleosynthesis, in which heavy nuclides, up to $A\sim 300$, are created. Subsequently, these unstable nuclides decay into the valley of stability after the neutron capture ceases, (see, \textit{e.g.,} \cite{Burbidge:1957} for the process description). 
%
Thus, BNS and NSBH mergers are one of the prime sources of the $r$-process elements in the Universe \cite{Argast:2003he,Shen:2015,VanDeVoort:2015,Ramirez-Ruiz:2014fsa}, 
%
The radioactive decay of unstable nuclides formed during decompression of the ejecta releases enough energy to power an electromagnetic transient in optical and near infrared, that peaks on a timescale from hours to a week \cite{Li:1998bw,Kulkarni:2005jw,Metzger:2010,Kasen:2013xka,Tanaka:2013ana}. 
This transient is called "kilonova" \cite{Metzger:2010} or "macronova" \cite{Kulkarni:2005jw}. 
Such transients were suspected for the events of gamma-ray bursts, GRB130603B \cite{Tanvir:2013pia,Berger:2013wna} and GRB060614 \cite{Yang:2015pha,Jin:2015txa} as an excess in infrared afterglow, that is an indication of a $r$-process material decay. \red{In 2017 the Kilonova was directly observed, following the gravitational wave signal from merging neutron stars}. \red{[REFS]} 
%
Properties of the electromagnetic counterpart to mergers sensibly depend on the ejecta geometry, mass, velocity and composition. These parameters differ for various types of ejecta. In case of dynamical ejecta, two sub-components are usually considered with a cold one ejected tidally near the plane of the binary and \cite{Lattimer:1977,Freiburghaus:1999}, and a fast shock-heated one, squeezed out at the collision interface in polar direction \cite{Bauswein:2013,Hotokezaka:2013b}. 
On a longer timescale if the merger results in the formation of a disk around the remnant, additional mass ejection from it continues via winds \cite{Fernandez:2013tya,Perego:2014fma,Just:2014}.
Generally, the electron fraction of the ejecta depends on the weak interactions, in particular on the weak interaction timescale relative to the dynamical timescale.
Thus, the tidal component of the dynamical ejecta would have low electron fraction \cite{Korobkin:2012uy}, the wind from the disk, irradiated by the neutrinos and heated up by shocks would have high $Y_e$ \cite{Just:2014,Richers:2015lma} and the shocked component of the dynamical ejecta would take an intermediate value, depending on the energetic of the collision \cite{Wanajo:2014,Goriely:2015fqa}. 
%
For the material with initial electron fraction $Y_e\leq 0.2$ the fission cycling in a strong $r$-process makes final composition rather independent of the exact value of $Y_e$ \cite{Metzger:2010,Roberts:2011,Goriely:2011vg}. 
If the $Y_e\in[0.2,0.3]$, however, the final composition of the outflow depends on the details of the incomplete $r$-process nucleosynthesis \cite{Korobkin:2012uy,Grossman:2013lqa,Kasen:2014toa}
%
As an adiabatic expansion of the outflow removes most of the initial energy of it, the electromagnetic emission is powered by the decay of heavy elements \cite{Metzger:2010}. 
Properties of this emission, besides mass, velocity and heating rate of the outflow, depend also on its opacity. 

% Properties of the electromagnetic emission also depends on the opacity of the emitting material, which in turn is set by its composition. 

% This the composition of the ouflow at $\sim 1$~day, as well as mass and velocity, sets the nueclar heating rate and opacity of the material \cite{Li:1998bw}. 
%
It was shown that the presence of elements synthesized in $r$-process, \textit{e.g.,} lanthanides and actinides can significantly increase it \cite{Kasen:2013xka,Tanaka:2013ana}. 
This is due to their large atomic complexity, that allow for a number of transitions far in excess of those found in iron-group elements. 
As the time of the luminosity peak of transients depend on when the material becomes transparent, the presence and the exact amount of lanthanides and actinides defines the observe properties of the kilonva \cite{Barnes:2013wka,Tanaka:2013ana}. 
%
Thus, accurate modeling of the composition of the outflow is of prime importance for predicting the electromagnetic counterparts to mergers. 

%%
%%
%%

\subsection{Parameterized ejecta nucleosynthesis}
\red{This is how the nucleo is computed for our BNS models}

Note: the exact $r$-process abudnance pattern(at a 3rd peak in particualr) depends on the nuclear mass model, reaction rate and fission fragment distribution 
(\textit{e.g,}, \cite{Goriely et al., 2005; Arcones and Martinez-Pinedo, 2011; Mumpower et al., 2012; Mendoza-Temis et al., 2015; Eichler et al., 2015})
Having a hydrodynamic model, implies a need for a full post-processing with nuclear reaction network. This was done in \textit{e.g.,} 
\cite{Goriely et al. (2011), Korobkin et al. (2012), Grossman et al. (2014),Wanajo et al. (2014), Just et al. (2015), and Martin et al. (2015)}.

Here a parametrized approach is adopted that allows for a systematic study of the effects of ejecta on the nycleosynthetic abundances and possible implication for kilonova signals. 
The exapding outflow is described with three parameters:
\begin{itemize}
    \item Ejecta electron fraction $Y_e=N_p/N_B$, where $N_p$ and $N_B$ are the numbers of protons and baryons. The $Y_e$ is sampled uniformly between \red{$0.01$} and \red{$0.5$}. $Y_e$ above that is not favourable for $r$-process.
    \item Ejecta specific entropy $s$. It is sampled lorarithmically between $1$ and $100$ $k_B/\text{baryon}$.
    \item Ejecta expansion timescale $\tau$. This quantity describes the ejecta expansion while it undergoes the nucleosynthesis. $\tau$ is ampled between \red{$0.1$ms and $500$ms}
    \gray{WE assume homologous expansion where $r\propto t^{-3}$, \textit{i.e.,} $\rho(t) = \rho_0(3\tau / 2.72t)$,
    where $\rho_0$ is the intiial density} \red{Check with David2018 paper}. This choice is motivated by the recent studies of the density histroies of Lagrangian fluid elements in the ejecta \cite{Duez, 2015; Foucart et al., 2014}
\end{itemize}

\red{The $\rho_0$ is taken from the ejecta profile. Is it? Or is it giben by the NSE for a given Ye and temperatute $T=6*10^9$?}.
\red{Entropy is not given by the NSE. THe EOS we use does not give entropy for every species seaprately. It is given for an averagre mass and charge numbers?}
The initial abundances (and $\rho_0$) are determined by the NSE for a given $Y_e$, $s$ and $\tau$. \gray{We use \texttt{SkyNet}, \cite{Lippuner and Roberts, 2017}} for abundance evolution. 

The nuclear reaction rates for computations are taken fron JINA REACLIB database \cite{Cyburt et al., 2010}.

\red{Details on what was used for setting up the skynet}

%%

\subsubsection{Parameter Space}

\red{Check what is the grid in s, rho and Ye for the precomputed nucleosynthesis we have}
\red{Fig.3.1 shows the effect of Ye and s on the final abundances. High Entropy -- high neutron-to-seed ratio, even though seed nuclei are light, -- full $r$-process. Low entropy, $r$-process is jugged as there are not many free neutrons available. However, it still preceeds and the seed nuclei are very heavy.}.
\red{At high Ye the, where ejecta becoms less neutron rich, the full $r$-process is no longer produced as there are not enough neutrons per seed nucleus to rich the third peak.}
\red{High enstropy and low Ye allows for 3rd peak reproduciton still as there are few seed nuclei and neutron to seed ration is high.}

\subsubsection{Lanthanide turnoff and heating rate as a function of $Y_e$}

A turn off is a porint in aprameter space, when the mass fraction of synthesiszed lanthanaids (actinides) $X_{\text{La}}$ ($X_{\text{Ac}}$) drops below $10^{-3}$. For example, a maximum $Y_e$ (and minimum $\bar{A}_{\text{fin}}$) for which the $X_i \geq 10^{-3}$ is a turn off point. 

The turnoff prevents a large amount of lanthanides (actinides) from being produced and suppresses the fission cycling.

Here $\bar{A}_{\text{fin}}$ is the final average mass number,

\begin{equation}
    \bar{A}_{\text{fin}} = \frac{1}{Y_{\text{seed}}(0) + Y_{\alpha}/18},
\end{equation}

where $Y_{\alpha}(0)$ is the initial abundance of alpha particles, $Y_{\text{seed}}(0)$ is the initial abundance of the seed nuclides ($A\geq12$). The $Y_{\text{seed}}(0) + Y_{\alpha}/18$ thus denotes the number abundance of heavy nuclei at the end of the $r$-process, assuming that it takes $\sim 18$ $\alpha$-particles to create a seed nuclei \cite{Woosley and Hoffman, 1992}. Further, assuming that the number of seed nuclides by the end of the $r$-porcess equals the initial number of seeds plos those produced in $\alpha$-process, \textit{i.e.,} neglecting the effects of fission cycling, the total mass-fraction of heavy nuclei at the end of the nucleosynthesis is $1$. Note that $\bar{A}_{\text{fin}}$ is a function of initial abundances only and a good indicator of the direction that nucleosynthesis would take. 

Considering a range in $s\in\{1, 10, 30, 100 \}$, $\tau\in\{0.1, 1, 10\}$~ms and $Y_e\in(0,0.5)$,
the nucleosynthesis calculations state, nuclear heating rates, $M\varepsilon$, ($1$day, $10^{-2}M_{\odot}$) are rather constant up to $Y_e\approx 0.4$, when a particualr nuclides start dominate the heating. The number of fission cycles is increased for high $s$ models, due to increased production of seed nuclides by triple-$\alpha$ process. 

Neutron-rich freeze-out occurs at high $s$ and small $\tau$, where ejecta high temperature and rapid expansion allows for a fewer capture events on seed nuclei and free neutrons escape. An ejecta compoent with such conditions was suggested to produce a ultraviolet precoursour to the kilonova on a timescale of hours, powered by the decay of frozen-out neutrons \cite{Metzger et al. (2015)}.

\gray{See if that can be modelled easily for our BNS models}

The heating rate, $M\varepsilon$, are roughly constant with $Y_e$, at $1$ day, as long as the fraction $X_{\text{La+Ac}}$ produced is constant. 
After lanthanides production ceases and their fraction falls, the instantaneous heating rate (at $1$ day) decreases only slightly (up to $1$ order of magnitude). 
Considering the integrated nuclear heating amount as a function of $Y_e$ ($s=\text{const}$, $\tau=\text{const}$), it does decreases by $1-2$ orders of magnitude, when the $r$-process stops producing unstable heavy lanthanides, and stable nuclides are produced directly.

%%

\subsubsection{Fission cycling}

Fission cycling is a process where freshly synthesized via strong $r$-process heavy nuclides with $A\sim 300$ undergo fission just to become seed nuclides for a similar $r$-process leading to an $A\sim 300$ nuclide. This process eliminates the dependency of the final abundances on the initial conditions and it also limits the maximum nuclide mass that ca be achieved. 

The number of fission cycles can be estimated via a ration of the seed nuclides at time zero and the number of seeds at the time when there are no more free neutrons available. This is motivated by the fact that neutron capture itself does not create new seeds, only increases the mass of them, while fission, splitting heavy nuclide in two, generates additional seeds. 

The number of fission cycles is tight to how much lanthanides and actinides are produced. In particular, as fission cycling limits the maximum mass of the nuclide that can be created, the fraction of lanthanides, actinides as well as heating $\varepsilon$ are insensitive to the initial $Y_e$. The number of cycles is thus tight to the initial $Y_e$. The lower it is, the more free neutrons available and thus more cycles would occur. After the fission cycling stops the $r$-process maximum mass drops to $A\sim 250$. Thus the amount of actinides drops as only the litest of them can be produced that do not fission immediately. 

%%

\subsubsection{Lanthanide production and heating rate in the full parameter space}

The properties of the electromagnetic signal, kilonova, are determined by the energy available, \textit{i.e.,} nuclear heating rate, and how easy this energy can escape the medium, \textit{i.e.,} photon opacity. Both, in turn, depend on the amount of lanthanides (and actinides) available. 

Consider a parameter space in $Y_e$, $s$ and $\tau$. The mass fraction $X_{\text{La}+\text{Ac}}$ have the following general dependency on the parameters. 

For a very low ($Y_e \sim 0.01$) the lanthanides production is mostly assured with exception of two cases: high $s$ - low $\tau$ and very large $\tau$ low $s$. In case of the former, the reason is the increased timescale for the neutron capture and lowered mass of the seed nuclei, which might lead to the neutron-rich freeze-out. In case of the latter, the long expansion timescale results in the newly-synthesized heavy elements decaying under the high density and temperature conditions sufficient for photodissociation, binning the composition back to NSE. When the material does cool down enough for the $r$-process to produce heavy elements agian, the electron fraction has been already increased through $\beta$-decays, allowing for the weak $r$-process only. 

For an intermediate $Y_e \sim 0.25$, where the boundary between lanthanides-right and lanthanides-free composition lies. At the $Y_e=0.25$, lanthanides are produced if the $s \sim 1$ is very low or very high $s \sim 100$. In case of the former this is due to seed nuclei being initially very heavy and few captures are needed to produces lanthanides, while in case of the latter, a large number of free neutrons is available as triple-$\alpha$ process is suppressed \cite{Hoffman et al., 1997}, Meanwhile, high-$\tau$ low-$s$ and low-$\tau$ high-$s$ (symmetric corners of distribution) are lanthanides-free. 

Considering the heating rate, $M\varepsilon$, at $1$~day for $M=10^{-2}M_{\odot}$ the following dependencies observed.
For $Y_e\in[0.04,0.35]$ the heating rates (at $1$~day) remain almost independent of $s$ and $\tau$. The dependency on $Y_e$ becomes prominent at $Y_e>0.35$, however the dependency on $s$ and $\tau$ are rather weak. In partuclar, there heating rates peak at $Y_e=0.425$ due to the decay of $^{66}$Cu that has a rather large $Q$-value.
Notably, no significant correlation found between $X_{\text{La}+\text{Ac}}$ and heating rate at $1$~day. This independence from composition results from the fact that at $1$~day the main contributions to the heating rate would come from nuclides with corresponding half-life (decay energy depends on half-life). And thus, even though the composition of the material does depend on the $Y_e$, $s$ and $\tau$, the same nuclides are considered when heating rates are estimated. At very high $Y_e$, however, the heating rate can be dominated by a selected nuclides and thus it depends on the composition.

%%

\subsubsection{Fitted nuclear heating rates}

Decay of newly synthesized heavy elements powers kilonova. There are however, prticular nuclei decay of which has the largest controbution. To determine these, the fractional heating contributions of all nuclides are integrated. The time window is between $0.1$ and $100$ days, $s\in[1,100]$ $k_B/\text{baryon}$ and $\tau\in[0.1,500]$~ms. 

The single most important nuclide in the aforementioned window is $^{132}$I. The origing of this nuclide is the decay of double magic $^{132}$Sn, that is produced in large quntities and then decays within minutes to $^{132}$Te (through $^{132}$Sb). The decay of $^{132}$Te is not very energetic, but it results in $^{132}$I, which decays with $Q=3.6$~MeV. This determines its large contribution to the heating. 

Overall, at very low $Y_e < 0.25$, nuclei from the second and third peaks ($A\sim 130$ and $A\sim 200$) domiante the heating (and few $A\sim 250$ ones), while at higher $Y_e$, most of the heating comes from the second peak. This is in agreement with dominant $\beta$-decay nuclei found in \cite{Metzger et al., 2010}. In addition, heating comes from the spontaneous fission of heavy elements, produced alongside actinides.

At higher $Y_e\in(0.25,0.375)$, the first $r$-process peak, $A\sim 88$ makes a significant contribution as well as neutron-rich side of the iron-peak. 

At even higher $Y_e>0.375$, the proton-rich side of stability around the iron peak exerts a contribution.

%%
%%
%%

\subsection{Lightcurves}

Consider a rather simplified way of modeling a kilonova. 
The expansion is assumed to be homologous, \textit{i.e.,} $r = \upsilon t$ with the density structure being

\begin{equation}
    \rho(t, r) = \rho_0(r/t)\Big(\frac{t}{t_0}\Big),
\end{equation}

instead of uniform density of the expanding ball, as the latter might show superluminal expansion velocities.

The nuclear reaction network gives the heating as a fucntion of time, $\varepsilon(t)$, but only a fraction of released energy (in form of neutrinos and gamma-rays) will be passed into the material, thermolized in the material. This fraction is difficult to calculate. It requries gamma ray transport schemes. One can assume it to be constant, \textit{i.e.,} $f=0.3$ \cite{Barnes and Kasen, 2013}. Thus, only $f\varepsilon$ of the released in nucleosynthesis energy is deposited into the material.
Assuming the homologous outflow, the velocity can be considered as a Lagrangian coordinate and the Lagrangian radiative transport equations can be writted to the forst order in $\upsilon/c$ \cite{Mihalas and Weibel-Mihalas, 1999}
The equation of state can be assumed to be the one of an ideal gas, $u=3T/(2\mu)$ and the Eddington factor entering the Lagrange equations can be obtained from the static Boltzmann transport equation. 
This method is similar to \cite{Ensman, 1994}. 

Calculating the exact opacities of a mixture of atoms with complex line structure, such as iron-group elements, and even more so, lanthanides and actinides is a difficult task. \red{it was adressed though by Tanaka et al}.
However, if the detailed opacity calculations are beyond the scope, one can assume an approximate opacities given by 

\begin{equation}
    \kappa = \kappa_{\text{Fe}}(T) + \sum_i \max[\kappa_{\text{Nd}}(T,\: X_i) - \kappa_{\text{Fe}}(T),\: 0],
\end{equation}

where $\kappa_{\text{Fe}}(T)$ and $\kappa_{\text{Nd}}(T,\: X_i)$ are the iron and neodymium opacities given in \cite{Kasen et al., 2013}, $X_i$ is the mass fraction lanthanides or actinides species. 
For the gray opacity calculations, the Plank mean opacity can be assumed (it is a good approximation when wavelength dependent opacity is calculated in the Sobolev approximation (Kasen, 2015))

%%

\subsubsection{Dependence of kilonova light curves on the outflow properties}

In case of a lanthanides-rich ejecta, $Y_e=0.01$ $(Y_e=0.19)$ the different in heating rates is small. However, the difference in opacity results in the peak time difference of about a weak and magnitude difference of about $1$ magnitude. In addition, the effective temperature at peak is also much lower for the $Y_e=0.01$. 
In case of a lanthanides-poor ejecta, $Y_e=0.25$ $(Y_e=0.5)$, the difference in heating rates is more prominent as in the latter, mostly stable nuclei are produced.The difference in peak time of the latter 1-2 days later and its peak magnitude about $1$ order dimmer. This is the effect of the reduced heating.
Thus difference in opacity cominates the lightcurve properties at low $Y_e$ while effect of different heating rates on the lightcurve is prominent at high $Y_e$. 
The interplay between opacity in heating can be seen from the fact that more heating leads to the brighter late part, because hotter ejecta maintins its high opacity (more populated excited levels) for longer \cite{Kasen et al., 2013}.. 

The effect of different entropy of the ejecta is the most prominent at $Y_e=0.25$, where the $s=1k_B/$baryon and $s=10k_B$/baryon give lanthanides-righ and -poor ligtcurves.
Thus, if lanthanides are produced the transient is expected to be longer, dimmer and redder, while of the lanthanides are not produced, it is brighter, bluer and shorter.
The peak times and magnitudes also assessed by \cite{e.g. Roberts et al., 2011; Barnes and Kasen, 2013; Tanaka and Hotokezaka, 2013}. 

A particular exception is the case of the neutron-rich freeze-out that leads to a neutron-powered transient and occurs at high entropy and low expansion timescale. It results in a very bright,$L\sim10^{41-42}$, and very early, $\sim0.5$~hour, hot, $T\sim 10^{4-5}$, ultraviolet transient. Previously this transient was also reported by \cite{Metzger et al. (2015)}.

%%

\subsubsection{Mass estimates of potential kilonova observations}

A particular example for the investigation of the depenedency of a lightcurve on ejecta mass can be done for the GRB130603B, for which observations is infrared and optical \cite{Berger et al., 2013; Tanvir et al., 2013}

The lwoer limit on the ejected mass can be computed. The method is the following. Consider a set of parameters for ejects, velocity and mass, compute the AB magnitude for the transients produced bu this ejecta. Account for the cosmology and instrument filter responds for the observed data. Compare the observed magnitudes with model ones and interpolated the minimum that reproduces observations.

%%

\subsection{Conclusion}

The final amount of lamthinides and actinides depends mostly on $Y_e$. Ejecta is free of these elements for $Y_e > 0.25$. Notably, lanthinides-free ejecta can be achieved also for low $Y_e$ under special conditions in terms of entropy and expansion timescale. High $s$ low $\tau$ ang low $Y_e$, for isntance, give a neutron rich freeze-our. Contrary, at low $s$, high $\tau$, there is a late time heating that resets the composition to the NSE corresponding to a higher $Y_e$. 

High opacity of lanthinides (a factor of a 100) results in the peak of a Kilonova occurring a weak later and at $1$ magnitude magnitude lower. Similar results were found in \cite{Roberts et al. (2011), Kasen et al. (2013), Tanaka and Hotokezaka (2013), and Grossman et al. (2014).}. Heating rates however, at $1$ day depend weakly on the lanthinides abundances, and thus on $Y_e$. When ejecta is initially low $Y_e$ the dependency is small, as the same ensemble of nuclides dominating the heating is produced. If the $Y_e$ is high, however, the specific nuclides dominate the heating and its rate changes strongly depending on composition. 

%%
%%
%%

\section{NEUTRINOS AND BHNS R-PROCESS NUCLEOSYNTHESIS}

The influence of neutrinos on r-process nucleosynthesis
in the ejecta of black hole–neutron star mergers

Condiering the output of the general relativistic simulation of a NSBH merger, mapping it onto the newtonian smoothed particle hydrodynamics (SPH) code and investigate the evolution of ejecta before the outflow is homologous. The nucleosynthesis is added in postprocessing, considering different levels of neutrino irradiation from the disk. The neutrinio irradaition can affect the final abundances as neutrino absorption on free neutrinos turn them into protons, changing the $Y_e$, adding to the seed nuclei budget. 

%%

\subsection{Introduction}

NSBH were expected to be detected by LIGO/VIRGO \cite{LIGO Scientific Collaboration et al., 2015; Acernese et al., 2015}, (\gray{and they were but with no counterpart}), possibly hinting on the origin of short Gamma ray Bursts \cite{e.g. Lee and Ramirez-Ruiz, 2007} and $r$-process nucleosynthesis \cite{(e.g. Lattimer and Schramm, 1976; Korobkin et al., 2012; Bauswein et al., 2014b}.

While galactic chemical evololution hinted that SN might be largely responsible for $r$-process elements enrichment \cite{e.g. Qian, 2000; Argast et al., 2004}, the conditions within SNe are not perfect for this porcess \cite{e.g. Arcones and Thielemann, 2013)}. These conditions however, are present when neutron stars or NS and BH merger \cite{Freiburghaus et al., 1999}. Modelling galactic chemical evolution is difficult, as these are high yeild rare events that requrie a significant time between projenitor formation and merger. In parcticular, discepancies between such models and observations of $r$-process elements abundances in low metallicity hallow \cite{Qian, 2000; Argast et al., 2004}. More complex models of galactic chemical evolution show however a reconciliation of the disagreement \cite{Matteucci et al., 2014; Shen et al., 2015; van de Voort et al., 2015; Ishimaru et al., 2015} \gray{And recent observations, GW170817, indicate the mergers are one of the main sources of $r$-process material.}

Weak interactions were shown to be important in mergers of two neutron stars, affecting the properties of the outflow \cite{Wanajo et al., 2014; Goriely et al., 2015; Sekiguchi et al., 2015; Foucart et al., 2016a; Palenzuela et al., 2015; Radice et al., 2016} \gray{Can Add long paper in the list}. The outcome of the merger has a complex dependency on EOS, electromagnetic field and neutrino effects \cite{e.g., Neilsen et al., 2014; Palenzuela et al., 2015}. 

In case of NS and BH merger, weak interactions are expected to play much lesser role. Low entropy in the tidal part of the dynamical ejecta supresses the $e^{-}$ and $e^{+}$ captures. Neutrino interactions are supressed by the high outflow speeds and low neutrino luminosity \cite{e.g., Foucart et al., 2014, 2015}. Thus, strong $r$-process is likely to occure in the NSBH outflows \cite{Lattimer and Schramm, 1976; Lattimer et al., 1977; Korobkin et al., 2012; Bauswein et al., 2014a}.

The merger rates for NSBH is quite uncertain (\gray{Check the LIGO papers}). The ejecta mass from the mergers depends on the binary parameters, BH spin and mass in particular \cite{Foucart et al., 2013; Hotokezaka et al., 2013c; Bauswein et al., 2014a; Kyutoku et al., 2015}. Increasing the former (in the direction of the orbital angular momentum), increase the amount of ejecta, while increasing the latte leads to a complex effect on the ejecta mass \cite{Kyutoku et al., 2015}.

To asses the importance of NSBH for the galactic enrichment with $r$-process material requires an accurate estimate of merger rates and predictions for the amount of unbound mass, both of which are not very well known. 

%%

\subsection{Methods}

\paragraph{NSBH} Code: Spectral Einstein Code (SpEC) with neutrino leackage scheme. EOS: LS220

\paragraph{SPH evolution of ejecta}

\subsubsection{Nuclear reaction network and weak interactions}

The nucleosynthesis calculation requires the density history of the ejecta, its initial composition and entropy. This can be done performing a hydrodynamics simulation of the expanding ejecta (which allows to account for the feedback the nucleosynthesis has on ejecta, \textit{e.g.,} generated entropy) or assuming an expansion mode, \textit{e.g.,} homologous expansion. Change in electron fraction during the expansion can be included via weak interactions. Otherwise, it can remain constant.

Notably, a single EOS may not extend to the temperature-density region where $r$-process is still possible in the expanding ejecta. As the nuclear reactions affect the EOS only marginally below $\rho\sim 10^{12}$g cm$^{-3}$, a switch to another, multi-species non-degenerate ideal gas EOS, \textit{e.g.,}\cite{Timmes and Arnett, 1999}. However, caution must be exercised in this procedure, as a single nucleus approximation of the LS220 eos predicts nuclei different from thos from full NSE calculation and thus different internal energy. 

For a given density history (of a Lagrangian particles), the nucleosytnehes can be performed via NNR \texttt{SkyNet} \cite{Lippuner and Roberts, 2015}. If it is required to track the evolution of the electron fraction, (which can be the case for high densities and temperatures where weak interactions are not in equilibrium), the NSE evolution mode can be used. Below $T\sim 7$GK, however, a full NNR evolution is required.
The inclusion of weak reactions, -- absorption of neutrinos and atin-neutrinos by electrons and positrons, and electrons and positions by free nucleons. The expression for neutrino capture rate is complex, which requires different distribution functions for electrons and positrons. In its applied form it does not include momentum transfer to the nucleons and neglects weak magnetism corrections (note however, that might be of importance for the neutrino-driven with \cite{Horowitz, 2002}). Neutrino-distribution is usually assumed to the be of the Fermi-Dirac sahpe in energy sapce. While the simple spherical emitting surface is a common approximation, the geometry might be of importance as the NSBH (BNS) merges form a disk-shape emitting region.

%%
%%
%%

\subsection{Results and discussion}

\subsubsection{The electron fraction of the ejecta}

The prime quantity defining the nucleosynthesis in the ejecta is its electron fraction \cite{(e.g., Lippuner and Roberts, 2015)}. Due to absence of neutrino emitting central object, shocks at collision and due to short timescales, the electron fraction of the ejecta from NSBH mergers is generally expected by the that of the initial beta-equilibrium \cite{Just et al., 2015}. Such low  $Y_e < 0.1$, the full $r$-process with a large number of fission cycles is expected  \cite{(e.g. Kasen et al., 2015; Lippuner and Roberts, 2015)}. On the other hand, in case of the BNS, the significant neutrino irradiation is expected raising the ejecta electron fraction through weak interactions \cite{Wanajo et al., 2014; Goriely et al., 2015; Foucart et al., 2016a; Palenzuela et al., 2015; Radice et al., 2016}. If the $Y_e$ is raised to $0.25$, the nucleosynthesis becomes dependent on other ejecta properties, such as expansion timescale and entropy. 

In case where neutrino absorption is not 'ab-initio' included into the simulation, the effect of it can be estimated from the total neutrino luminosity and neutrino processing timescale. 

\begin{equation}
    \tau_{\nu} \approx 67.8 \text{ms} \: \Big(\frac{r}{250\text{km}}\Big)^2 L_{\nu_e, 53}^{-1} T_{\nu_e, 5}^{-1}
\end{equation}

where $r$ is the radius of the fluid element, $L_{\nu_e, 53}$ is the electron neutrino luminosity in units of $10^{53}$erg s$^{-1}$ and $T_{\nu_e,5}$ is the electron neutrino spectral temperature in units of $5$MeV. The spectral temperature can be estimated for a gray scheme by considering the temperature of the emission region, and adopting that the average neutrino energy is $\varepsilon_{\nu} \approx 3.15T$ \cite{(e.g., Foucart et al., 2015).}. \red{[Quote]Electron antineutrinos are unlikely to contribute significantly to the neutrino interaction timescale[Why?]} \red{[Answer]This is because in the low entropy outflows of BHNS mergers almost all protons are locked in heavy nuclei and thus have very low neutrino capture cross-sections.[EndQuote]}.

Next, assuming that all protons are incorporated into the heavy nuclei, neutrino luminocity and ejecta velocity remain constant, and that neutrino irradiation from the disk start at after merger, also neglecting $e^{-}$, $e^{+}$ captures, 

\begin{equation}
    \frac{d Y_e}{dr} = \frac{\theta(r - \nu t_{\nu, on})}{\nu \tau_{\nu} Y_{e; eq}}(Y_{e; eq} - Y_e)
\end{equation}

where $Y_{e; eq} = \langle Z \rangle_{\text{nuclei}} / \langle A \rangle_{\Omega_j}\text{nuclei}$ and $t_{on}$ is the time when neutrino luminosities reach their saturation value.
The result of this analysis states taht the the neutrino irradiation is unlikely to play a significant role in changing the composition of the ejecta from NSBH mergers.
Running with the full NNR results in very little changes in ejecta composition during the first $\sim10$~ms after while the changes at $\sim20$~ms are driven by beta-decays in $r$-process.

%%

\subsubsection{Nucleosynthesis and neutrino induced production of the first r-process peak}

Consider in more details, the effect neutrinos have on the isotropic abundances in ejecta. 
Note that luminosity of the electron anti-neutrino is higher then the one of the electron neutrino due to \magenta{re-leptonization} of the neutrino emitting disk \cite{Foucart et al., 2016a}. However, nucleosynthesis yilds are unsensitive to the chosen electron antineutrino luminosity because of the \magenta{$\alpha$-effect}.

The result is, again, a confirmation, that the NSBH ejects, dynamically, a large amount of $r$-process rich material \cite{(e.g. Roberts et al., 2011; Korobkin et al., 2012; Just et al., 2015)}, with both second and third $r$-process peaks reproduced robustly at all considered neutrino luminosities which is to be expected for the outflow with initially very low $Y_e$, that passes several fission cycles \cite{Lippuner and Roberts, 2015}. 

Notably, the abundance of the first r-process peak at mass number $A=78$ depends on the neutrino luminosity but in all cases it is under-produced relative to the solar abundance when normalizing to the second and third peaks.

The origin of seed nuclei for the first peak production is the following. After falling from NSe, and the strong eqilibrium ceases to hold, the material is composed of heavi nuclei and free neutrons, which, while still being close to the torus, turns into proton. Then it caputres another neutron forming $D$. Then, two deutrons form an $\alpha$-particles, two of which then undergo a neutron-catalyzed triple-alpha reaction, forming a low-mass seed nuclei \cite{Meyer et al., 1998}. A similar process proceeds in neutron rich neutrino driven winds \cite{Delano and Cameron, 1971; Hoffman et al., 1997}.
These low-mass seed nuclei capture neutrons forming heavier elements, but cannot generally ass the $N=50$ ($Z=28$) in the $r$-process path due to exhaustion of free neutrons. In the end these seeds end up as a first-peak elements. 

%%

\subsubsection{Details of the first peak production mechanism}

Consider the indirect production of the first peak nuclei. This relies on the neutrino induced seed production, \textit{e.g.,} neutrino catalyzed triple alpha process, that converts $6$ protons into one seed nucleus. For a $L_{\nu_e}\sim 10^{53}$~erg s$^{-1}$, about $\sim10\%$ of the $r$-process material stops at the first peak. 

A time needed for an indirectly produced seed nuclei to be processed to its final $N=50$, the $\tau$ timescale, is $70-600$~ms postmerger for $L_{\nu_e;52}\in[1,20]$. The time it takes to go from chrage $Z_1$ to $Z_2$ can be estimated considering the waiting point apporxiamtion (c.f., \cite{Kratz et al., 1993}) and beta-decay timescales of intermediate nuclei. Notably at high temperatures, where  $(n,\gamma)$ reaction is in equilibrium with its inverse reaction, the $\tau$ timescale depend only on hydrodynamics $\rho$, $T$, $Y_n$. 
This explains the dependency of the first $r$-process abundances on the ejecta parameters. 

The combined timescale that is required for a seed necleus to go to the start of the first $r$-process peak and get processed through the first peak is $\sim500$~ms and $\sim100$~ms respectively for the aforementioned range of neutrino luminosities. The final first peak abundances then scales linearly with neutrino luminosity. After the $\sim600$ms postmerger the free neutrinos are exhausted.
Thus, overall, the neutrino flux in a time-frame of the charge conversion, $[70, 70+100]$~ms, determines the amount of neutrino-induced first peak production. 

Notably, while up to $L_{\nu_e}5\times10^{52}$erg s$^{-1}$, the first peak element production scales linearly with $L_{\nu_e}$, at this value, the saturation occurs. In addition, for a certain slow moving fluid elements with low initial electron fraction and entropy, the neutrino luminosity can have a non-monotonic behavior of the first peak abundance with the neutrino luminosity.

Regarding the effects of thermodynamic conditions and their effect on the first pear elements production the following can be stated. Increased neutrino luminosity raises the fluid electron fraction and temperature. Hence, the total number of seed nuclei is increases. Higher entropy also reduces the ability of the synthesized material to pass the $A=8$ stability gap. Notably at very low temperatures (lower $L_{\nu_e}$) the induced seed production changes considerably and is able to pass the $N=50$ maxiumum. 
The first peak elements are produced even when there no neutrinos through fission of heavy nuclei.


\subsubsection{Isotopic and elemental abundances, galactic chemical evolution, and low metallicity halo stars}
\red{Brief}
It was shown that for a set of BHNS models, the nucleosynthesis in the ejecta differs only a little. The reason for this is twofold. On one side, abundances at the second and third peak are insensitive to the binary parameters due to fission cycling. On the other, for a given neutrino luminosity, the neutrino produced first peak is insensitive to binary parameters. Notably, larger difference is expected if the ejecta velocity varies more, which in turn changes the time when the neutrino exhaustion the ejected material.

Comparison between solar $r$-process abundance pattern and models yields show an overall qualitative agreement but considerable quantitative disagreement. In particular, an overproduction of the third peak elements. This disagreement can be attribution to uncertainties in ejecta properties and uncertainties in nuclear data. The latter, in particular can amount to a factor of ten \cite{Mumpower et al., 2015}.
\footnote{In addition, the neutrino-induced fission, that can affect the abundances of the first peak elements, especially in ejecta that undergoes strong neutrino irradiation, \cite{Qian, 2002; Kolbe et al., 2004}, is not included in the NRR} 
Overall, it is unlikely that NSBH is a dominant source of $r$-process elements. 

\magenta{Elemental abundance pattern} $Y_Z = \sum_i \delta_{Z_i,Z}Y_i$ is commonly used in studyng the abundance patterns in metal poor stars. The comparison is meaningfull for high $Z$ elements, $(51,81)$. Elements below Mo can be synthesiszed in MP stars via multiple other channels \cite{Travaglio et al., 2004; Montes et al., 2007; Qian andWasserburg, 2008; Arcones and Montes, 2011; Hansen et al., 2014}.
As in stars with high enough metallicyt the $s$-process can contribute to the abundance pattern, present in \textit{e.g.,} sun, the comparison can also be done with low metallicity halo stars. Data is available in \cite{Westin et al., 2000, Roederer et al., 2012, and Roederer et al., 2014}

\subsubsection{Conclusion}
Considered are the BHNS mergers with neutrino irradiation, ejecta follo-up via SPH code and nucleosynthesis in postprocessing. 
Findings: 
\begin{itemize}
    \item Second and thirds $r$-process peaks are robustly reproduced. (This is not always the case for BNS \cite{Wanajo et al., 2014; Goriely et al., 2015; Sekiguchi et al., 2015} )
    \item Neutrinos do not change the pre-$r$-$Y_e$ considerably, and full $r$-process occures in the entire ejecta. 
    \item Weak dependency of $r$-process final abundances and binary parameters.
    \item Neutrino irradaition affect nucleosythesis in a way besides changing $Y_e$, -- inducing low mass seed nuclei production via triple-$\alpha$ process \cite{Fuller and Meyer, 1995, Meyer et al., 1998}. This enhances the synthesis of first peak elements. This is relevant for both BHNS and BNS. 
    \item Final abundnaces qualitatively agree with solar for second and third peak. The third peak is however overproduced. 
    \item Uncertanties in ejecta parameters and nuclear data input for NRR can affects the result significantly.
\end{itemize}


%%
%%
%%

\section{Signatures of hypermassive neutron star lifetimes on rprocess nucleosynthesis in the disk ejecta from neutron star mergers}

\red{We investigate the nucleosynthesis of heavy elements in the winds ejected by accretion disks formed in neutron star mergers.}

\subsection{Introduction}

%% Motivation
Origin of $r$-process material in the universe is uncertain. Observations of MP stars in the galactic halo suggests that a mechanism that produces robust abuncnace patters, that of the Solar system, for $A>130$ \cite{(e.g., Sneden et al., 2008).}, while observations of old VMP stars suggest that this mechanism has operated since the early universe \cite{(e.g., Cowan et al., 1999; Ji et al., 2016)}. Less robust abundance patters is found for light $r$-process elements, as diviations observed between the one of the Solar System and MP stars \cite{(e.g., Montes et al., 2007).}. Similarly, meteoritic abundances show difference in the timescales on which light and heavy $r$-process elements formed. This suggests more than one main source of $r$-process nucleosynthesis \cite{Wasserburg et al., 1996}.

%% Prodocution cites 
While the CCSN can contribute to the budget of light $r$-process elements ($A\leq 130$), e.g., \cite{Roberts et al., 2010; Fischer et al., 2010, Hrdepohl et al., 2010}, \cite{Martinez-Pinedo et al., 2012; Wanajo, 2013}, most heavy elements are produced in compact object mergers, BNS, NSBH, as conditions in their ejecta is highly neutron-rich \cite{(Lattimer and Schramm, 1974)}. These mergers are candidates for gravitational wave telescopes \cite{(e.g., LIGO Scientific Collaboration, 2010; IGO Scientific Collaboration et al., 2015)}, are a focus of numerical relativity modelling \cite{(e.g., Lehner and Pretorius, 2014; Paschalidis, 2017)} and search for electromagnetic counterparts \cite{(e.g., Rosswog, 2015; Fernandez and Metzger, 2016; Tanaka, 2016).}. 
\red{Collapsars are missing}.

%% Dyn. Ej. as a production cite
The dynamical ejecta of BNS/NSBH mergers is favorable site for the $r$-process nucleosythesis. Its low initial electron fraction assures the robust $r$-process abundance pattern for $A>130$ due to \magenta{fission cycling} \cite{(e.g., Goriely et al., 2005),}, that depends weakly on the binary parameters \cite{(e.g., Goriely et al., 2011; Korobkin et al., 2012; Bauswein et al., 2013)}. Additionally, neutrino absorption within the ejecta allows for the production of lighter $A<130$ elements \cite{(e.g., Wanajo et al., 2014; Goriely et al., 2015; Sekiguchi et al., 2015; Radice et al., 2016; Foucart et al., 2016a,b; Roberts et al., 2017)}

%% Disk outflow
A disk formed in BNS or NSBH mergers can ejecta a fraction of its mass of a longer timescales \cite{(e.g., Ruffert et al., 1997; Lee et al., 2009; Metzger et al., 2009b}. Such ejecta has large $Y_e$, raised by weak interactions \cite{(e.g., Dessart et al., 2009; Fernandez and Metzger, 2013; Perego et al., 2014)}. The mechanisms contributing to mass ejection are weak interactions with a thermal timescale $\sim30$~ms, angular momentum transport processes and nuclear recombination with timescales $\sim1$~s. Mass of the ejecta can exceed that of the dynamical ejecta \cite{e.g., Fernandez and Metzger, 2016} with the neutrino-dorven compoennt accounting for only a small fraction, present if massive neutron star was formed behore the collapse \cite{(Dessart et al., 2009; Fernandez and Metzger, 2013; Metzger and Fernandez, 2014; Just et al., 2015}. 

%% Previous works 
The disk outflows has been shown be the cite of light and heavy $r$-process elements production in early works, that made use of parametric treatment to obtain thermodynamic trajectories for composition analysis \cite{(e.g., Surman et al., 2008; Wanajo and Janka, 2012)}. A robust pattern in produced light elelements with variability in heavy elements was found in ourflowed for disks surrounding the BH \cite{Just et al., 2015; Wu et al., 2016)}. The neutrino driven (polar) outflows from the disks surrounding a massive neutron star, however, were found to the the cite of primaraly light elemnt production $A<130$ \cite{Martin et al., 2015, Perego et al., 2014}
 
%% This work 
Here the nucleosynthesis in the ejecta from disks surrounding massive NS is considered, with varying lifetimes of the remnant. The parametrized approach mimics the effect that different EOS, angular momentum traspt mechanisms and neutrino cooling would have on the remnant \cite{(e.g., Paschalidis et al., 2012; Kaplan et al., 2014).}. Nucleosytheissi is computed in postprocessing on tracer particles. 

%%
%%
%%

\subsection{Method}
\subsubsection{Disk outflow simulations and thermodynamic trajectories}
\red{brief}
Use \texttt{FLASH} code, that solves newtonian hydro (with $\alpha$ viscosity) and lepton number (including neutrino absorption) consiervation in axisymmetry. 

Neutrinos are treated via leackage scheme for cooling and 'light-bulb' approximation for self-absorption, considering only charged-particle interactions. The surface of the NS is streated as a reflecting surface, that also has an outward neutrino flux.

No feedbacl from NNR on the hydro evolution. 

NNR starts at $T\leq 10^9$~K, or at maxiumim, if lower. Thus, it starts at NSE. \texttt{SkyNet} includes viscouse heating and neutrino heating effects from therodynamic trajectories. 

Important timescales:
The orbital time at the initial density peak $t_{orb}=f(R_d, Mc)\sim3$~ms, with $R_d$ being the radius of the diensity peak, $M_c$ -- the mass of the central object. 
The initial thermal time of the disk $t_{th}=f(M_d, e_{i,d}, L_{\nu,52})\sim30$~ms, where $e_{i,d}$ is the initial specific internal energy of the disk, $L_{\nu,52}$ is a typical neutrino luminosity from the disk.
The initial viscous timescale of the disk $t_{vis}=f(\alpha, H/R, R_d, M_c)\sim200$~ms, where
$\alpha$ is the viscosity parameter, $H/R$ is the height-to-radius ration of the disk.

Neutrino driven outflow are launched on the thermal timescale. Long-term outflows are launched on a viscous timescale. 

\magenta{dimensionless spin parameter} $\chi = J_c / (G M_c ^2)$, where $J$ is the BH angular momentum, $c$ is the speed of light, $G$ is the graviational constant. 

%%

\subsection{Results and discussion}
\red{very brief}

When a MNS collpases to a BH a \magenta{rarefaction wave} moves outward from the inner boundary, quenching the thermal outflow \cite{(cf. Figure 3 of Metzger and Fernandez, 2014)}. After this disks re-adjusts and settles in on the path of a viscous evolution of a disk around a BH. 

Densities of $\sim 10^11$g cm$^{-3}$ are sufficient to trap neutrinos locally, creating a local emission hotspot. 
High density in the disk's midplane shadows the outer disk from irradiation by the remnant. This is also supported by monte-carlo simulations \cite{(Richers et al., 2015).}. Hence, the large weak interatcion timescale leads to the $Y_e$ on the inner disk, near the plane, being low $Y_e\sim0.1$. Away from the midplane, the timescale for the weak interactions is shorted and the electron fraction is higher. In particular, material at $\sim45^{\circ}$ displays high electron fraction.
With time, the density of the inner disk decreases, the $Y_e$ rises (as the degree of matter degeneracy decreases), the intensity of weak interaction decreases. 

\subsubsection{Neutrino emission}

Due to disk re-absortion of the $\nu_e$ and anti-$\nu_e$, the correspoding luminocities between the remnant and the disk remain very simialr. 
After the collapse and disk re-adjustment, the $L_{\bar{\nu}_e}$ dominates over $L_{\nu_e}$, as weak interactions \magenta{leptonize} the initially low $Y_e$ disk (positron capture rates are higher then the electron ones), and the continuous decrease in $\rho$, leads to increase in equilibrium $Y_e$.

\subsubsection{Ejecta properties}

To connect the disk and ejecta properties, consider trace particles conditions when they last pass the point of $Y_{e,\:T=5\text{GK}}$ and $s_{T=5\text{GK}}$. At lower temperatures, the composition moves out of NSE, requiring the full nuclear reactrion network -- hence conditions at $T=5$GK are the initial conditions for the nucleosynthesis.

The correlation between the remnant lifetime and both the amount of the ejecta and its electron fraction is noted \cite{(Metzger and Fernandez, 2014)}. The matter is ejected doe to the strong neutrino heating, viscouse heating and nuclear recombination. Matter ejected from the innder accretion disk reaches the $\beta$-equilibrium and ejects with higher electron fraction (?). 

The mas of low-$Y_e$ ejecta depends weakly on the MNS lifetime as long as it is longer then $30$~ms. This is because the longer MNS leaves the higher the average $Y_e$ of the ejecta, more ejecta is ejected. 

Ejecta components identified. $\nu$-wind, it shows a tight correlation between $Y_{e,\:T=5\text{GK}}$ and $s_{T=5\text{GK}}$. with $Y_e$ reaching $0.5$. 
Second ejecta, shows weak correlation between electron fraction and entropy, and associated with the disk reaching the \magenta{advective state} with little to none neutrino heating or cooling and is dreiven by the energy injection due to angualr momentum transport and nuclear recombination \cite{Metzger and Fernandez, 2014}. Associated with the convection within the disk.

To qualitatibely asses the amount of the $\nu$-driven wind, consider the contribution of neutrinos to changing the $s_{T=5\text{GK}}$ of a tracer, \textit{i.e.,} the increase in entropy deu to neutrino absorption, neglecting cooling. A tight 'boomegang' shape between $Y_{e,\:T=5\text{GK}}$ and $s_{T=5\text{GK}}$ is an indicative of such wind. \red{See that this can be added to the long paper investigation of the $\nu$-wind}.
Note taht this component is present if $t>30$~ms, lifetime. 

%%

\subsubsection{Nucleosynthesis}

Standard techniques: consider mass-averaged composition, abundances, multiplied by the ejecta mass to highlight their relative contributions to the different $r$-process regions. 
Usually, solar abundances are scaled to match the second peak of the models.
\magenta{Rare-Earth peak} located at $A\sim165$.
\magenta{First peak} $A\sim80$
\magenta{Second peak} $A\sim130$
Regaring other sources or $r$-process material. The expected abundance pattern is weighted toward the $3$rd peak for the dynamical ejecta \cite{(e.g., Goriely et al., 2011;  Wanajo et al., 2014; Roberts et al., 2017)} and towards $1$st peak for core-collapse supernovae \cite{(e.g., Wanajo, 2013; Shibagaki et al., 2016; Vlasov et al., 2017)}. The solar $r$-process abundances then, is a combination of all the sources, weighted by their rate and yield per event (taking into account production delays).
Shit to the higher $A$ for the third peak, for models, might be die to well-known shortcoming of the FRDM mass model \cite{(e.g., Mendoza-Temis et al.,2015; Mumpower et al., 2016)}
Peak at $A\sim132$ might be attributed to "late-time heating that photodissociates neutrons from synthesized heavy elements" which leads to "additional neutron capture and a pile up of material at the doubly magic nucleus" (see \cite{e.g., Wu et al. (2016) })
For the long-lived models, if the lifetime $\geq 100$~ms, the first peak $A\in[70,90]$ is overproduced with respect to the solar values, then the abundances are normalzied to the second peak. 

A way to quanify the \textbf{relative contribution} of model's abundances to the different regions of the $r$-process distribution, the average abundances around those peaks can be considerd, normalized to the solar values. 
For instance, assume that $A_1 \in [70,90]$, $A_2 \in [125,135]$, $A\in[186,203]$ and $A\in[160,166]$ for the $1$st, $2$nd $3$rd and rare-Earth peaks respectively. Let $Y_{i}$ be the summed final abundances within each peak. Then, the realtive contribution can be evaluated as 

\begin{equation}
    \frac{Y_{1st}}{Y_{2nd}} = \log_{10}\frac{Y_{1st}}{Y_{2nd}} - \log_{10}\frac{Y_{1st,\odot}}{Y_{2nd,\odot}},
\end{equation}

with those denoted with $\odot$ correspodong to the solar system abundances peaks. 
These values would generally go between $-2$ and $2$ 

%%

\subsubsection{BH spin mimicking HMNS lifetime}

high-Ye material also correlates with BH spin (\cite{Fernandez et al., 2015a}).
While a larger BH spin has a similar overall effect on the disk ejecta composition as a longer HMNS lifetime.
At best, a rapidly spinning BH can mimic a HMNS of modest lifetime.

%%

\subsubsection{Lanthanides, actinides, and heating rates}

The light curve and spectrum of the radioactive decay powered electromagnetic transient are determiend by two propetiesL the opacity (of up to $\times10$ of iron-group elements) of the material and the radioactive heating rate. largest opacities are found in lathanides $Z\in[58,71]$ and actinides $z\in[90,103]$, that have open $f$-shells, and thus complex atomic line structures \cite{(Kasen et al., 2013; Tanaka and Hotokezaka, 2013; Fontes et al., 2015)}

One considers the trajectory-averaged lanthanide $\langle X_{La} \rangle$ and actinide $\langle X_{Ac} \rangle$ mass fraction of the ejecta. Respectively, in dynamical ejecta $\langle X_{La} \rangle$ and $\langle X_{Ac} \rangle$ are usually $10^{-2}$ and $10^{-3}$. For along lived remnants, the $\langle X_{La} \rangle$ drops to $10^{-3}$. This for the dynamical ejecta the increase in opacity might reach $10$, in disk ejecta it is more of a factor of a few. 

In addition to the disk outflow, dynamical ejecta, its spatial distribution and compostion defines the color of a Kilonova. 

There is a correlation between $Y_{e, 5GK}$ and $s_{5GK}$, is increase in former requires either stron neutrino heating, that increases $Y_e$ or long timescales, thermal timescales, which are sufficient for weak interactions to raise $Y_e$ to its equilibrium value.

A material with high entropy $S\geq30$ $k_B$baryon$^{-1}$ and electron fraction $Y_{e,5GK}\in(0.25,030)$ also produces a significant fraction of lanthnides and actinides. This is due the dependency of the $La$, $Ac$ abundances on the neutron-to-seed ratio, $Y_n/Y_{seed}$. If it is $\sim40$ the $La$ and $Ac$ are produced. The $Y_n/Y_{seed}=35$ represents the boundary. In the ejecta, at high entropies and $Y_e>0.25$, such $Y_n/Y_{seed}$ can be reached and exceeded. Hence, the production of $La$ and $Ac$. Note that extremely high $s_{5GR}$, \textit{e.g.,} $\geq 200$, can allow $La$ and $Ac$ production even at $Y_{e,5GK}>0.4$. 

For the Kilonova modelling, the quantity of importance is the total total radioactive heating rate $\varepsilon_{tot}$ at a given time after merger, which depends on the ejecta mass. Note, that it is not addicitve between different ejecta components. note, that in case of the BNS, the high opacity, quasi-spherical ejecta \cite{(e.g., Hotokezaka et al.,2013b)} can obstruct the disk ekecta. However, during the high-temperature phase, the lanthanides-rich material allows phitons to escape \cite{(Barnes and Kasen, 2013; Fernandez et al., 2017)}, and thus the short-lived blue component must be seen.

To properly account to the emergy deposition by the radiactive heating, the energy converstion from the decay products into thermal energy, that has a limited efficiently \cite{(e.g., Metzger et al., 2010; Hotokezaka et al., 2016; Barnes et al., 2016)}, ought to be considered. Taking the total heating rates woould provide the upper limits to the bolometric luminosity estimation. 

The heating rate falls with increassing $Y_e$, until it starts to oscillate. The oscillations are given gy the NSE being domianted by nuclides with matching $Y_e$ and the half-life matching the time of the heating rate computation. If there is no such nuclei, the heating is drastically reduced. 

%%

\subsubsection{Comparison with r-process abundances in metal-poor halo stars}
\red{very very brief}

It is believed that MP stars in the galactic halo have been enriched with $r$-process elements, which they show in the specra, by a few events \cite{(Cowan et al., 1999)}

The abundances (with respect to $Z$) are scaled to match solar values in $Z\in[56,75]$, as models predict similar abundances in this range and match solar ans MP abundances well. 
Lower $Z$ elements are oversporduced by models, however the qualitative agreemnt at lower $Z$ found in MP stars.
Overall, the disk outflowes for $Z=44$ and $A=47$ display abundances incosistent with pure disk outflows. Thus suggests that an additioanl source contributed to the enrichment of the MP stars.
Notably, that when only the dynamical ejecta is considered, producing elements with $Z>55$, the combined final abundance pattern is consistent with MP halo stars \cite{(cf. Just et al., 2015).}. For $Z<42$, it might me that SN contributed. 

%%

\subsubsection{Impact of angular momentum transport}

While energy deposition by neutrinos can unbind a significant amount of the material, \cite{Ruffert et al., 1997}, so can the transport of angular momentum. The latter modifies disk hydrostatic balance, changing the centrifugal force and thus inducing a disk spread, while it is being accreted. In addition, it increases the local entropy of the fluid. The quntatice description of the mechanisms depends on the exact way the process is modelled. Similarly, MHD turbulence heats up the fluid \cite{Hirose et al., 2006}. 

Notably, here the inclusion of the viscosity increases the disk mass by a factor of $6$, allowing almost all disk mass to be ejected. 

In late-time ejecta, large dispersion in $s_{5GK}$ for agiven $Y_{e,5GK}$ might be attributed to the convective motions in the advective phase of the disk. 

Notably, an inclusion of the viscosity can enhance the $\nu$-driven wind, as the former produces disk spreading, and contributing to the energy gain by the ejecta from neutrinos. 

\subsection{Conclusion}

A note on nucleosynthesis. Since dynamical ejecta tends to be neutron-rich, producing heavy elements between $2$nd and $3$rd peak, if the abundances of the latter are consistent with the solar abundances, it implies, that the short-lived remnats are dominant source or $r$-process material. However, if the third peak is over-produced with respect to the second one, then a source of lighter elements is required, \textit{i.e.,} long-lived remnants and their outflows. 



% \pagebreak

%% A A A A A A A 

% Alpher et al. (1948) \cite{Alpher:1948}
% Alpher and Herman, 1950 \cite{Alpher:1950}
% Arnould et al., 2007 \cite{Arnould:2007gh}
% Arnould and Goriely (2003) \cite{Arnould:2003}
% Anders and Grevesse, 1989 \cite{Anders:1989}
% Arcones et al., 2012 \cite{Arcones:2012}
% Akiyama et al., 2003 \cite{Akiyama:2003}
% Argast et al., 2004 \cite{Argast:2003he}
% Abbott et al., 2016a \cite{Abbott:2016blz}
% Abbott et al., 2016b \cite{Aasi:2013wya}
% Arnett and Truran, 1969 \cite{Arnett:1969}
% Arnett, 1977 \cite{Arnett:1977}
% Arcones et al., 2010 \cite{Arcones:2010}
% Arcones and Martínez-Pinedo, 2011 \cite{Arcones:2010dz}


%% B B B B B B B 

% Burbidge et al. (1957) \cite{Burbidge:1957}
% Bertolli et al., 2013 \ctie{Bertolli:2013gka}
% Burrows et al., 2007 \cite{Burrows:2007yx}
% Bauswein et al., 2013 \cite{Bauswein:2013}
% Barnes and Kasen, 2013 \cite{Barnes:2013wka}
% Barnes et al., 2016 \cite{Barnes:2016umi}
% Berger et al., 2013 \cite{Berger:2013wna}
% Bellm, 2014 \cite{Bellm:2014}
% Bethe, 1939 \cite{Bethe:1939}
% Boyd, 2008 \cite{Boyd:2008}
% Bressan et al., 2012 \cite{Bressan:2012}
% Blinnikov and Panov, 1996 \cite{Blinnikov:1996}
% Brett et al., 2012 \cite{Brett:2012jn}
% Brock et al., 2015 \cite{Brock:2015}
% Buss et al., 2012 ------------------------------

%% C C C C C C C 

% Coc et al. (2013). \cite{Coc:2013eha}
% Cameron, 1973 \cite{Cameron:1973}
% Cowan and Rose, 1977 \cite{Cowan:1977}
% Couch et al.,1974 \cite{Couch:1974}
% Cescutti et al., 2015 \cite{Cescutti:2015}
% Coc et al., 2012 \cite{Coc:2011az}
% Cyburt et al., 2016 \cite{Cyburt:2015mya}
% Cyburt et al., 2010 \cite{Cyburt:2010}
% Clayton, 1968, \cite{Clayton:1968}

%% D D D D D D D 

% Dominik et al., 2012 \cite{Dominik:2012kk}
% De Donder and Vanbeveren, 2004 \cite{DeDonder:2004cx}
% Danielewicz and Bertsch, 1991 --------------------------

%% E E E E E E E 

% Edvardsson et al., 1993 \cite{Edvardsson:1993}

%% F F F F F F F 

% Fischer et al., 2010 \cite{Fischer:2010}
% Fröhlich et al., 2006 \cite{Frohlich:2006}
% Foucart et al., 2014 \cite{Foucart:2014nda}
% Foucart et al., 2015 \cite{Foucart:2015vpa}
% Fernández and Metzger, 2013 \cite{Fernandez:2013tya}
% Freiburghaus et al., 1999 \cite{Freiburghaus:1999}
% Fernández and Metzger, 2016 \cite{Fernandez:2015use}
% Fernández et al., 2017 \cite{Fernandez:2016sbf}
% Feger, 2011 \cite{Feger:2011} 	 # Phd thesis


%% G G G G G G G 

% Grevesse and Sauval, 1998 \cite{Grevesse:1998}
% Goriely et al., 2011 \cite{Goriely:2011vg}
% Goriely et al., 2015 \cite{Goriely:2015fqa}
% Gehrels et al., 2009 \cite{Gehrels:2009}
% Grossman et al., 2014 \cite{Grossman:2013lqa}
% Goriely et al., 2008 \cite{Goriely:2008zu}
% Guidry and Harris, 2013 	 same as \cite{Guidry:2011}
% Guidry and Harris 2011 \cite{Guidry:2011}
% Guidry, 2012 \cite{Guidry:2012}
% Guidry et al., 2013a,b \cite{Guidry:2011a} \cite{Guidry:2011b}


%% H H H H H H H 

% Hansen et al., 2004 \cite{Hasen:2004}
% Herwig et al., 2011 \cite{Herwig:2011}
% Hulse and Taylor, 1975 \cite{Hulse:1975}
% Hotokezaka et al., 2013a \cite{Hotokezaka:2013iia}
% Hotokezaka et al., 2013b \cite{Hotokezaka:2013b}
% Hotokezaka et al. (2015) \cite{Hotokezaka:2015zea}
% Hotokezaka et al., 2016). \cite{Hotokezaka:2015cma}
% Hayashi et al., 1962 \cite{Hayashi:1962}
% Hofmeister et al., 1964 \cite{Hofmeister:1964}
% Hillebrandt et al., 2013 \cite{Hillebrandt:2013gna}
% Heger and Woosley, 2010 \cite{Heger:2008td}
% Harris et al., 2014 \cite{Harris:2014}
% Hauser and Feshbach, 1952 \cite{Hauser:1952}
% Hix and Thielemann (1999) \cite{Hix:1999}
% Hix and Meyer, 2006 \cite{Hix:2005pf}


%% I I I I I I i 

% Iwamoto et al., 1999 \cite{Iwamoto:2000as}
% Ishimaru et al. (2015) \cite{Ishimaru:2015}
% Iliadis et al., 2002 \cite{Iliadis:2002zz}

%% J J J J J J J 

% Just et al., 2015 \cite{Just:2014}
% Ji et al. (2016) \cite{Ji:2016}
% Jin et al., 2015 \cite{Jin:2015txa}
% Jin et al., 2016 \cite{Jin:2016pnm}
% Jones et al., 2015 \cite{Jones:2015}
% José and Hernanz, 1998 \cite{Jose:1997vf}


%% K K K k k K K K 

% Kaeppeler et al., 1994 \cite{Kaeppeler:1994K}
% Kyutoku et al., 2015 \cite{Kyutoku:2015gda}
% Korobkin et al., 2012 \cite{Korobkin:2012uy}
% Kulkarni, 2005 \cite{Kulkarni:2005jw}
% Kasen et al. (2013) \cite{Kasen:2013xka}
% Kasen et al., 2015 \cite{Kasen:2014toa}
% Kasliwal et al., 2016 \cite{Kasliwal:2016uhu}
% Kaeppeler et al., 2011 \cite{Kaeppeler:2011}


%% L L L L L L L L 

% Lodders, 2003 \cite{Lodders:2003}
% Lattimer and Prakash, 2005 \cite{Lattimer:2004sa}
% Li and Paczynski, 1998 \cite{Li:1998bw}
% LIGO Scientific Collaboration et al., 2015; \cite{TheLIGOScientific:2014jea}
% Lee and Ramirez-Ruiz, 2007 \cite{Lee:2007js}
% Lippuner and Roberts, 2015 \cite{Lippuner:2015gwa}
% Law et al., 2009 \cite{Law:2009}
% Leung et al., 2015 \cite{Leung:2015fxa}
% Limongi and Chieffi, 2003 \cite{Limongi:2003ui}
% Lippuner et al., 2017 \cite{Lippuner:2017tyn}
% Lunney et al., 2003 \cite{Lunney:2003}
% Longland et al., 2014 \cite{Longland:2014}
% Lattimer et al., 1977 \cite{Lattimer:1977}


%% M M M M M M M M 

% Moller et al., 1995 \cite{Moller:1993ed}
% Mathews and Cowan, 1990 \cite{Mathews:1990}
% Martínez-Pinedo et al., 2012 \cite{MartinezPinedo:2012rb}
% Mösta et al., 2014 \cite{Mosta:2014jaa}
% Mösta et al., 2015 \cite{Mosta:2015}
% Mendoza-Temis et al., 2015 \cite{Mendoza-Temis:2014mja}
% Matteucci et al., 2014 \cite{Matteucci:2014}
% Matteucci 2012 \cite{Matteucci:2012}
% Metzger et al., 2010 \cite{Metzger:2010}
% Metzger et al., 2015 \cite{Metzger:2014yda}
% Metzger, 2017 \cite{Metzger:2016pju}
% Martin et al., 2015 \cite{Martin:2015hxa}
% Müller, 1986 \cite{Mueller:1986}
% Mumpower et al., 2012 \cite{Mumpower:2011ar}
% Mumpower et al., 2016 \cite{Mumpower:2015ova}
% Möller et al., 2003 \cite{Moller:2003}
% Meyer and Adams (2007) \cite{Meyer:2007}


%% N N N N N N N N 

% Nishimura et al. (2015) \cite{Nishimura:2015nca}
% Nakar, 2007 \cite{Nakar:2007yr}
% Nomoto et al., 1997 \cite{Nomoto:1997if}
% Nollett and Burles, 2000 \cite{Nollett:2000fh}
% Nishimura et al., 2017 \cite{Nishimura:2017zdi}
% Nakar and Piran, 2011 \cite{Nakar:2011cw}

%% O O o O O O O O 

% Orlov et al., 2000 \cite{Orlov:2000}


%% P P P P P P P P 

% Peters, 1968 \ctie{Peters:1968}
% Pruet et al., 2006 \cite{Pruet:2005qd}
% Price and Rosswog, 2006 \cite{Price:2006fi}
% Perego et al., 2014 \cite{Perego:2014fma}
% Paxton et al., 2011 \cite{Paxton:2011}
% Parikh et al., 2013 \cite{Parikh:2012hx}
% Prantzos et al., 1990 \cite{Prantzos:1990}
% Panov et al., 1995 \cite{Panov:1995}
% Panov et al 2001 \cite{Panov:2001}


%% Q Q Q Q Q Q Q Q Q 

% Qian and Woosley, 1996 \cite{Qian:1996xt}
% Qian, 2000 \cite{Qian:2000bh}

%% R R R R R R R R R 

% Rolfs and Rodney, 1988 \cite{Rolfs:1988}
% Roederer et al.,2010 \cite{Roederer:2010}
% Roberts et al., 2010 \cite{Roberts:2010}
% Radice et al., 2016 \cite{Radice:2016dwd}
% Roberts et al., 2011; \cite{Roberts:2011}
% Rauscher and Thielemann, 2000 \cite{Rauscher:2000fx}
% Roberts et al. (2017) \cite{Roberts:2016igt}
% Ramirez-Ruiz et al., 2015 \cite{Ramirez-Ruiz:2014fsa}
% Richers et al., 2015 \cite{Richers:2015lma}


%% S S S S S S S S S 

% Sekiguchi et al., 2015 \cite{Sekiguchi:2015dma}
% Sekiguchi et al., 2011 \cite{Sekiguchi:2011zd}
% Surman et al., 2008 \cite{Surman:2008qf}
% Shaviv, 2012 \cite{Shaviv:2012}
% Suess and Urey (1956) \cite{Suess:1956}
% Seitenzahl et al., 2009 \cite{Seitenzahl:2009}
% Sneden et al., 2008 \cite{Sneden:2008}
% Sneden et al. (2009) \cite{Sneden:2009}
% Straniero et al., 2006 \cite{Straniero:2005hc}
% Suda et al., 2008 \cite{Suda:2008na}
% Singer et al., 2014 \cite{Singer:2014qca}\
% Strohmayer and Brown, 2002 \cite{Strohmayer:2001ue}
% Seitenzahl et al., 2013 \cite{Seitenzahl:2013}
% Starrfield et al., 2016 \cite{Starrfield:2016}
% Schatz et al., 2001 \cite{Schatz:2001xx}
% Schatz, 2013 \cite{Schatz:2013}
% Siegel and Metzger (2017) \cite{Siegel:2017nub}
% Shen et al., 2015 \cite{Shen:2015}

%% T T T T T T T T T 

% Tytler et al., 2000 \cite{Tytler:2000qf}
% Thielemann et al., 2011 \cite{Thielemann:2011}
% Thompson et al., 2001 \cite{Thompson:2001ys}
% Tsujimoto et al. (2017) \cite{Tsujimoto:2017}
% Tanaka and Hotokezaka (2013) \cite{Tanaka:2013ana}
% Tanvir et al., 2013 \cite{Tanvir:2013pia}
% Timmes et al., 2000 \cite{Timmes:2000}
% Truran et al., 1966 \cite{Truran:1966}
% Truran et al., 1967 \cite{Truran:1967}
% Thielemann et al., 1986 \cite{Thielemann:1986}
% Timmes (1999) \cite{Timmes:1999}


%% V V V V V V V V V 

% van de Voort et al., 2015 \cite{VanDeVoort:2015}
% Vlasov et al. (2017) \cite{Vlasov:2017nou}


%% Y Y Y Y Y Y Y Y Y 

% Yang et al., 2015 \cite{Yang:2015pha}

%% W W W W W W W W W 

% Woosley et al., 2002 \cite{Woosley:2002}
% Wanajo, 2006 \cite{Wanajo:2006mq}
% Wanajo et al., 2011 \cite{Wanajo:2010mc}
% Wanajo and Janka, 2012 \cite{Wanajo:2012}
% Wanajo, 2013 \cite{Wanajo:2013} 
% Wanajo et al., 2014 \cite{Wanajo:2014}
% Wheeler et al., 2000 \cite{Wheeler:2000}
% Winteler et al., 2012 \cite{Winteler:2012}
% Winteler, 2013 \cite{Winteler:2013} 	 # PhD thesis
% Wehmeyer et al., 2015 \cite{Wehmeyer:2015}
% Wallner et al. (2015) \cite{Wallner:2015}
% Wollaeger et al., 2017 \cite{Wollaeger:2017ahm}
% Weaver et al., 1978 \cite{Weaver:1978}
% Woosley et al., 1973 \cite{Woosley:1973}
% Wagoner, 1973 \cite{Wagoner:1973}
% Weiss and Truran, 1990 \cite{Weiss:1990}
% Woosley et al., 2004 \cite{Woosley:2003cd}
% Woosley and Hoffman, 1992 \cite{Woosley:1992}

%% ======================================================================
%% ====================================================================
%% 
%%     KILONOVA
%% 
%% ====================================================================
%% ======================================================================

\chapter{Kilonova}

\section{Radiation Transport Modeling of Kilonovae and Broad-lined Ic Supernovae}
\red{this is based on the Barnes PhD thesis on Opacities for Kilonva (Rad.Transport)}

Both, BNS mergers and SNe Ic-BL are associated with gamma ray bursts (short and long respectively)
\subsection{Introduction}
\red{Very very basic and pooly cited. Unusable}

\subsection{Modelling Kilonova}

With regards to the electromagnetic counterparts to mergers, the large LIGO error-boxes and contaminants in the transient sky obsruring the character of the EM signal, the successfull detection and interpretation depends senibly on accurate models of the transient. 
The first escimations of the electromagnetic emission for the $r$-process decay powered transiet was done by \cite{Li & Paczynski (1998)}. Due to the absence of accurate predicitons for ejecta properties and radiactive decay models, the parametrized study was carried out via use of one-dimensional analytic model. The analysis suggested that the transient powered by the decay of $r$-process synthesized elements would be short and bright, varying strongly with ejecta properties. 
The simplest approach to the transient's photometry can be provided with the help of two key quantities, describing its peak. The luminocity $L_{\text{pk}}$ and time it takes to achive it $t_{\text{pk}}$.
The Arnett's Law profides a rough peak time and luminocity with ejecta parameters

\begin{align}
    t_{\text{pk}} \sim \Bigg(\frac{M_{ej}\kappa}{\upsilon_{ej} c}\Bigg)^{1/2} \\
    L_{\text{pk}} \sim \dot{E}_{\text{rad}}(t_{\text{pk}})
\end{align}

where $M_{ej}$ and $\upsilon_{ej}$ are the mass and characteristic velocity of the ejected material, $\kappa$ is its opacity, $c$ is the speed of light, and $\dot{E}_{\text{rad}}$ is the time-dependent rate of energy generation by radioactive decay. 

The ejecta's opacity defines the ease with which the radiation diffuses through it. The mass of the ejecta, composed of unstable nuclei sythesized in $r$-process nucle, sets the radiactive energy scales for the system $\dot{E}_{\text{rad}}\propto M_{\text{ej}}$. in both BNS and NSBH cases
The ejecta mass and veliocity depend on the parameters of the binary \cite{(Oechslin et al. 2007; Sekiguchi et al. 2016; Kyutoku et al. 2015}.

\gray{Very crude BNS discription. Not usefull} 
Modelling the binary neutron star merger is a complicated task. The process plays out in a strong gravity reginve. Stong magnetization of both neutron stars makes magnetic field one of the dominant factors \cite{(Price & Rosswog 2006).}, alongzde the neutrino irradiation. Thus, accurate modelling of these mergers require GRMHD (with radiation) to capture the physics of merger and mass ejection. In addition treatment of the NS EOS has to be considered, as it describes how easy it is for a NS to be tidally disrupted and what is the energetics of the collision. \cite{(e.g. Bauswein et al. 2013; Hotokezaka et al. 2013a)}

\gray{Useless, -- se Lippuner Kilonova discussion}
Kilonova is powered by the decay of many complex chains of decay, each occuring on its own timescale and with a unique energy spectrum. The experimental data for many heavy nuclei sythesisze in $r$-process is lacking and hence, the energy deposition of their decay chains is uncertain. 

\gray{Mostly useless, -- but with a new citatioN!}
For the nuclei for which the experimental data is lacking, the neclear mass models are used. These models preduict neutron capture cross-sectio nand properties of their radioactive decay. 
Nuclear mass models, calibrated to the avaialble heavy nuclei closer to the valley of stability, can significanntly diverge, predicting the properties of heavy neutron-rich nuclei beahvior. Thus, ultimately the final abundances are affected. \cite{(Eichler et al. 2015; Mendoza-Temis et al. 2015).}
The nuclear mass model choice affects the nucleosytnehsis and susequencly the decay and the power available to the electromagnetic counterpart \url{https://arxiv.org/abs/2010.03668}. 

There is a strong dependency of the funal $r$-process abundances on the initial conditions in the ejecta. see \cite{(Lippuner & Roberts 2015}.

\gray{Just to say that Kilova modelling inherit all the uncertanties of Hydro and Nucle and opacities strongly affect it as well}
Finally, one of the main challenges in modelling the Kilonova emission is to dectribe the radiation propagation withing the emitting and absorbing environemtn. The radiation transport (Ideally 3D) requires full density profiles of the ejecta, its radiarcitivity, inheriging the hydrodynamical and nuclear physics uncertanties. In addition, to describe the absoprtion of radiation propagating through the ejecta, the absorbtion cross section, usually described in terms of opcity $\kappa$ ($[\kappa] = \text{cm g}^{-1}$), must be included. The frequency dependent opacitis of matter composed of heavy elements with complex atomic structure strongly affects properties of the electromagnetic transient. Understanding the opacities is vital toe producing accurate kilonova models. 

%%

\subsection{Monte Carlo radiation transport in expanding atmospheres}
\red{Not very well rephrased. Might be usefull, but only for appendix}

Radiation transport is described in \cite{Rybicki & Lightman (1986),}.
The description of solving the RT equations in expanding atmospheres is presented in \cite{Kasen et al. (2006).}.
Radiation transport computation allow to obtain the monochromatic specific Intensity $I_{\nu}$ for a particular system. The specific intensity depends on the position, direction and frequency and is related to the luminosity as 

\begin{equation}
    L = \frac{\text{d}E_{\text{rad}}}{\text{d}t}\iiint I_{\nu} \text{d}\nu \text{d}A \text{d} \Omega
\end{equation}

where the frequency $\nu$ goes through the selected range or the entire electromagnetic spectrum, $A$ is the area and $\Omega$ is the solid angle. 

The radiation transport calculations determine the $I_{\nu}$ through integrating the transport equation 

\begin{equation}
    \frac{\text{d}I_{\nu}}{\frac{d}s} = -\alpha_{\nu} I_{\nu} + j_{\nu}.
\end{equation}

where $s$ is the distance passed, $\alpha_{\nu}$ and $j_{\nu}$ are the monochromatic extinction and emission coefficients. The former defines how much energy a photon looses to the scattering and absoption as a fucntion of the distance. 

\begin{equation}
    \alpha_{\nu} = n\sigma_{\nu} = \rho\kappa_{\nu}, \hspace{3mm} [\text{cm}^{-1}],
\end{equation}

where $\sigma_{\nu}$ is the cross-section for a particular interaction, $n$ is the number density of particles that photons are scattered or absobed on, $\kappa$ is the monochromatic opacity, and $\rho$ is the mass density. 
Typical sources of opacity in exanding atmospheres and ejected material in astrophysical context are the electron scattering, $~\sim  0.02$, bound-free opacity (photoionisation), bound-bound or "line" opacity (photoexatation).

The emission coefficient $j_{\nu}$ describes the addition of energy to the $I_{\nu}$, (per time, unit of volume, solid angle and frequency). 
In case of a thermal equilibrium the the $j_{\nu}$ is related to a Plank function $B_{\nu}$ as $j_{\nu} = \alpha_{\nu; \text{abs}}B_{\nu}(T)$, where $\alpha_{\nu; \text{abs}}$ is the component of opacity that describes absorption. 

\red{A particular code}
A code \texttt{SEDONA} allows to study the radiation transport via a time-dependent Monte Carlo method. It assumes \magenta{LTE} and uses the \magenta{expansion opacity} to discibe the interaction of photos with expanding absorbinb medium. See details in \cite{Kasen et al. (2006).}.

\gray{Very crude disctiption, but since I did not do any of this, it might be sufficient} 
MC methods to solve the radiation transfer equations are based on "photon packages" that are injected into the medium and whose interaction outcome with it is computed via random number sampling. Thus, the result of each such process, \textit{e.g.,} photon-ion interaction, direction of the photon scattering, frequency at which a photon is re-emitted in a Compton scattering are determined via random number sampling. 

A particular use of random sampling is the estimation of length of the path that a photon packet takes within the medium between interactions. The packet can only be scattered or absorbed, \textit{i.e.,} $j_{\nu}=0$. Thus the RT equation reduces to 

\begin{equation}
    \frac{\text{d}I_{\nu}}{\frac{d}s} = -\alpha_{\nu} I_{\nu} \leftarrow I_{\nu}(s) = I_{\nu,0}\exp(-\alpha_{\nu}s).
\end{equation}

that has a following probabalistic intepretation. Consider two points $s_1$ and $s_2$. In continous radiation interpretation, the difference in radiation intensity between these two points can be arbitrary small, and hence the $I(s_1)/I(s_2)$ describes the amount of energy lost from the radiation beam.
However, in probabalistic interpretation, where descrete photon packets carry the energy, the relation $I(s_1)/I(s_2)$ describes the probability that between $s_1$ and $s_2$ the interaction and hence a change in phton packet energy will occure. 
This probability is sected in the interval $I_{\nu}(s_1)/I_{\nu}(s_2) = z \in (0,1]$. 
The distance, traveled by a packed between interactions $\delta s$ is then obtaind as $\delta s = - (1 / \alpha_{\nu}) \log(z)$. 

The opacity calculation presents a particualr challenge for solving the transport equation. This is particularly so in the expanding medium as the opacity is frequency dependend and the photon's frequency depends on the local fluid velocity. 
Considering the homologous expansion, ($\upsilon = r/t$, commonly used to describe the velocity profile of the expanding atmospheres in supernovae and ejecta in comparct binary mergers) the photon frequency (energy) measuredin the comoving frame continously dereceases as it traverses the fluid and thus the opacity encountered by this photon changes. 

The bound-bound opacities are particularly affected by the motion of the medium. The excitation of a bound-bound transition requires a photon to have the exact energy (frequency) of the transition, a resonsnce. Moving through the expanding medium, the photon whose frequency is continuously decreasing will enter and exit resonances with varise lines. This has to be accounted for by the opacity calculations. A particularly usefull formalism for that is the Sobolev (Expansion Oapccity) formalism. It assumes taht the energy region where resonance can occure is phsycally narrow. The Sobolev optical depth for a homogeneouse flow (\cite{Sobolev 1960}) is 

\begin{equation}
    \tau = \frac{K_{\lambda} c t_{\text{exp}}}{\lambda_0}
\end{equation}

where $K_{\lambda}$ is the integrated line strength, $t_{\text{exp}}$ is the expansion time, and $\lambda_0$ is the wavelength of the line center.
Note that in non-homogeneous system $\tau$ depends on the velocity gradient of the matter in the direction of the photon's propagation).

Using the expansion opacity $\tau$, the effective continoum opacity can be stimated through averaging over individal lines in bins in wavelength space \cite{Karp et al. (1977), Eastman & Pinto 1993,}

\begin{equation}
    \alpha_{\text{exp}}(\lambda_c) = \frac{1}{ct_{\text{exp}}} \sum_i \frac{\lambda_i}{\Delta \lambda_c} (1 - e^{-\tau_i})
\end{equation}

where $\lambda_c$ is the wavelength of the center of the bin, $\tau_i$ is the Sobolev optical depoth of line $i$, and the sum runs over all lines within the bin. 

%%
%%
%%

\section{Effect of a high opacity on the light curves of radioactively-powered transients from compact object mergers}
\red{Based on paper: \cite{Barnes & Kasen (2013)}}


\gray{Overview of the paper. This might be used as a background. But it is outdated. }
The merger of two NS produces graviational waves and an eletromagneitc transent powered by the decay of newly synthesized heavy elements. Little is know about the optical properties of these heavy $r$-process eleemnts. Their opacity was first assumed to be similar to that of the Iron group elements. 
However later [see many refs] it was shown to be orders of magnitude higher.
In this section the effect of lanthanides-rich, high opacity is studied in application to the multiwavelength time-dependend 1D radiation transfer calculations. The study of different ejecta mass and velocities is taken.
%
Main fundings: 
Higher opacity leads to the longer lasting lightcurves ($\sim$weeks), with emission shirted to the lower freqyency bands of the spectrum. This is the result of a stong optical \magenta{line blanketing}. The temperature at the late times is a representative of the recombination temperature of lanthinides $\sim 2500$~K and spectrum is blackbody. 
Adding an additional, lathanides-free (predominantly Nickel) outflow shows a distinctive 2 component SED with a bright optical peal and an infrared peak. 

%%

\subsection{Introduction}
\red{Very crude. Badly rephrased. beneral info and motivation}

\gray{A generic introduction}. Understanding the compact object binary merger leads to a new insight into many areas in astrophysics. 
The BNS and NSBH mergers are belived to be the central engines of the sGRB \cite{(Paczynski 1986; Narayan et al. 1992), [and many others]}.
The material decompressing from nuclear densitiutes, the ejecta, is a possible cite for the $r$-process nuclesosyntehsis \cite{(Lattimer & Schramm 1974, 1976) [and many others]}.
Hence, mergers influence the cosmic chemical evolution and may be the dominant soirce of the heaviest, $r$-process elements \cite{(e.g. Eichler et al. 1989b; Rosswog et al. 1998; Freiburghaus et al. 1999; Rosswog et al. 2000; Rosswog 2005; Goriely et al. 2005). [and many more]}.
These events has successfully been detected as souirces of gravitational waves. \cite{LIGO}
The accurate identification of electromagnetic counterparts to mergers thus unhances the available information from graditational waves \cite{(e.g. Schutz 1986, 2002; Stubbs 2008; Bloom et al. 2009)} and provides new intput into aforementioned fields of study.

The comprehansive study of possible electromagnetic counterparts to mergers is given in \cite{Metzger & Berger (2012)}. It was shown that among possible options, sGRB, orphan radiu and optical afterglows, and optical kilonova, the latter is the best option for aiding the sky localisation of GW signals. \gray{Source of Kilonova, BNS ejecta, tidal, shocked}.
The brightness of the transent is set primaraly by the mass of the ejecta. However, its duration depends on the diffusion time of radiation through the ejecta -- and thus on mass, velocity and opacity. 
The nuclear heating from the decay of $r$-process elements was modelled in detain \cite{(Metzger et al. 2010; Roberts et al. 2011; Goriely et al. 2011, [add Lippuner]}, the opacities are still rather uncertain. 

\gray{This might be relevant}
The first study of the electromagnetic counterpart to mergers was based on a simplifed 1D spherically symmetric model, that emitted a black body spectrum assuming opacities being similar to electron scattering ones \cite{Li & Paczynski (1998)}. A more recent, 1D but full MC radiation transport models with radiatvie heating informed by the nuclear reaction networks and opacities assumed to be close to Iron peak oapcities \cite{(Metzger et al. 2010; Roberts et al. 2011)}. Both were qulitatively similar, ginving peal luminocities $10^{41-44}$ in optical/UV, rise time around a day, and colors shiting to red post peak. 

In this paper, more accurate opaciites are used. The effect on lightcurves is shown to be strong. 
The atomic data and opacities are taken from \cite{Kasen et al. 2013}.
Higher opaciites led to lighcurves being longer and dimmer, and redder. Instead of sharp peak at $\sim$1~day, the lightcurve rises and falles slowly on a timescale of $\sim 1$~weak. SED is supressed in optical, and most energy is emitted in FIR. 
These findings help for EM counterparts identification and searches. 

%%

\subsection{Opacity of r-process ejecta}

It was shown for SNe ejecta, that among electron scattering, free-free and photoionisation, the bound-bound transitions from iron group elements dominate opacities in optical (blue) part of the spectrum. \cite{(see e.g. Pinto & Eastman 2000)}. 
The pseudo-continoum bound-bound opacities are 'induced' by millions of doppler-briadened lines. Continoue doppler-shifting on phostons that traverse the ejecta assures that they fall into resonance with numerous transitions. Thus, ejecta's velocity gradient enhances the effective line opacities \cite{Karp et al. (1977)} and later, \cite{(Friend & Castor 1983; Eastman & Pinto 1993; Hoeflich et al. 1993).}. 

In light of this, the extinction coefficient, defined through the average over discrete wavelength, can be written as \cite{Eastman & Pinto (1993)}

\begin{equation}
    \alpha_{\text{exp}}(\lambda_c) = \frac{1}{ct_{\text{exp}}}\sum_i \frac{\lambda_i}{\Delta\lambda_c}(1-e^{-\tau_i}).
\end{equation}

where $t_{\text{exp}}$ is the time since mass ejection, $\lambda{c}$ is the central wavelength of the bin, $\Delta\lambda_c$ is the bin width, $\tau_i$ is the Sobolev optical depth of a line \cite{(Sobolev 1960)}, and the sum runs over all lines in the bin.

The epxansion opacity then

\begin{equation}
    \kappa_{\text{exp}} = \frac{\alpha_{\text{exp}}}{\rho}
\end{equation}

where $\rho$ is the gas density. 

The use of Sobolev $\tau_i$ requires two statements to be assumed:
Compare to the velocity scale over which ejecta proprieties vary, the intrinsic width of lines is small.
Lines (their intrinsic profiles) do not overlap. 
Notably, withing the expanding ejecta the first criterion can be satisfied. However, the second may not. In \cite{Kasen et al. 2013} the example of a more rigorous radiation transport is considered with non-Sobolev optical depth of lines.

Optical depth calculation requires the information regarding atomic level population. The simplest approach is to assume the local thermodynamic equilibrium (LTE).
IT was shown that in the context of Type Ia SN, the LTE is established below the photosphere \cite{Baron et al. (1996),}. The effects of non-LTE on the broadband fluxes months after Type Ia SNe was shown to be $\sim30\%$ in U, B, V, R and $\sim50\%$ in I band \cite{Jack et al. (2011)}. Thus, if the ejecta is optically thick the LTE assumption holds, more or less. When ejecta becomes transparent, as for I band, the LTE assumption is not longer accurate. In case of BNS ejecta, low density but higher opacity is expected to lead to similar situation. Thus, the LTE is applicable to model the vicinity of the lightcurve peak, but not much after that. Closer the one weak after the peak, the LTE breaks. It is also important to note, that other uncertanties most likely dominate the results, such are the atomic data uncertanties and assumed ejecta geometry. 

In the case of very optically thick lines $\tau\gg 1$, the expansion opacity, $\kappa_{\nu}\text{exp}$ becomes insensitive ot the actual opacity of lines, as $1-e^{-\tau_i}\approx 1$, and the number of distinct optically thick lines becomes a key parameter in determining it. However, for the heavy elements $Z>28$, the amount of line data available is sparse. The lathanides, having complex valence f-shells are expected to have a strong contribution to the opacity. 

In case of inaccesasability of line data, the number of lines should scale with the number of stubstates, for a given electron orbitale, $C=g! / (n! (g-n)!)$ whith $g=2(2l+1)$, where $n$ is the number of valence eletrons, $l$ is the angular momentum quantum number of the valence shell. For the open $f$ shell $(l=3)$, the number of liens thus is expected to be high. Hence, opacity of ejecta with lanthinides is expected to have higher opacity then pure iron ejecta, assuming the equal probability for a line to be optically thick. 

Fraction of actinides in ejecta is considered to be too small to make a contribution.

In addition, the proad range of temperatures and densities in the ejecta requires to model the opacity of elements with different ionisation states. Rarely the data for higher the doubly ionized elements is available however. Assuming lower ionisation states leads to underestimation of opacity. This is especially important for early $~1-2$~days lightcurve. Moreover, rarely the low frequency (long $\lambda$) line data is available, prohibiting the estimation of IR and NIR lightcurves. Notably, in case of BNS merger ejecta, most of the energy is radiated in low frequency bands. 

An alternative is to use the atomic structure modelling to obtain the lanthinides line data.

Another more crude alternative is to use several representative species to approximate the opacity of the ejecta. The ejecta can be devided into the iron-like elements, neodimium-like elements (lanthinides) amd calcium for a low-opacity filter (which have intermediate complexity for their region of the periodic table). Each is a representative of its group, approximating it, as in general, all ions have uinique set of strong lines. 

The extinction opacity then 

\begin{equation}
    \alpha_{\text{exp}} = \frac{1}{ct_{exp}} \sum_z N_z \sum_i \frac{\lambda}{\Delta \lambda_c} (1 - e^{-\tau_i(\rho_Z)})
\end{equation}

where $Z$ runs over the representative elements, (Fe and Nd here), $N_Z$ is the number of elements in the group that is apporximated by the element with $Z$, the $\rho_Z = \chi_Z \rho$ is the density with $\chi_Z$ being the mean mass fraction of the representative elements. 

%%
%%
%%

\subsection{Light curves of r-process transients}
\red{Rad.trasport calcualtions with updated oapcities}

Consider a spherically symmetric outflow going through the homologous expansion. Note that there were studies that the effect of ejecta anisotropy does not qualitatively change the shape of lightcurves \cite{(Roberts et al. 2011}. The density profile is split into the dense outflow, $\rho\propto r^{-\delta}$ and atmosphere, $r\propto r^{-n}$ with the transition occuring at $\upsilon_t$. 

\begin{eqnarray}
    \upsilon_t = 7.1 \times 10^{8} \xi_{\upsilon} \Big(\frac{E_{51}}{M}\Big)^{1/2} \text{cm s}^{-1}, \\
    \rho(r,t) = \xi_{\rho}\frac{M_{\text{ej}}}{\upsilon_t ^3 t^3}\Big(\frac{r}{\upsilon_t t}\Big)^{-\delta}, \hspace{3mm} \upsilon < \upsilon_t \\
    \rho(r,t) = \xi_{\rho}\frac{M_{\text{ej}}}{\upsilon_t ^3 t^3}\Big(\frac{r}{\upsilon_t t}\Big)^{-n}, \hspace{3mm} \upsilon > \upsilon_t \\
\end{eqnarray}

with $n>\delta$ and $\xi_{\rho}$ and $\xi_{\upsilon}$ are normalisation constants (giving specific energy and mass when profile is integrated).

Previous studies have given a broad range ejecta mass and vleocity that depend on binary parameters as well as input physics for BNS \cite{(e.g. Rosswog et al. 1999; Oechslin et al. 2007; Goriely et al. 2011; Bauswein et al. 2013; Hotokezaka et al. 2013b)} and of neutron star-black hole mergers \cite{(e.g. Janka et al. 1999; Lee 2001; Rosswog 2005)}. Feducial values for $M_{ej}$ and $\upsilon_{ej}$ are assumed and varied within limits. 

Ejecta compostition is assumed to the homogeneous of either pure $^{56}$Ni or $r$-process elements. The decay energy of $r$-process material is split into fission fragments ($10\%$) and $\beta$-decay $90\%$. The $\beta$-decay energy is split further into neutrino energy $25\%$ and $\gamma$-ray energy $75\%$. For the fragments and leptons the prompt thermolisation is assumed, while $\gamma$-rays are tracked by the radiation transport code. 

%%

\subsubsection{Light Curves}

The code used for rad. traposrt is \texttt{SEDONA} \cite{(Kasen et al. 2006}. The code starts at $0.1$~days after the merger, following the ejecta expasion, cooling as well as heating due to radiation and radioactivity. The opacities $=f(\lambda)$ for each element group Iron-like,  is computed using code \texttt{Autostructure} and for Neoudimium-like elements using \cite{Kurucz & Bell (1995a)} line data, respectively.
It is assumed that lines are completerly absibiong, the ionisation states follow LTE.

The result is the, when $r$-process opacities are used, the lightcurves rise more slowly, are broader wit hdimmer peaks. Such transiets are long lasting $\sim 1$~week.

Theotrical expectations for lightcurve duration stems from the radaition effective diffustion time \cite{(Arnett 1979)},

\begin{equation}
    t_d \approxeq \Big( \frac{M_{ej} \kappa}{ \upsilon c} \Big)^{1/2}
\end{equation}

where $\upsilon$ is a characteristic ejecta velocity and $\kappa$ an appropriately wavelength-averaged opacity.

Thus, longer diffusion times (and thus longer peak times) for the $r$-process ejecta is domianted by larger $10-100$ times larger opacitiies of these elements. Lossing more energy to expamsion, the ejecta have less to emit at the peak times. Thus the peak is dimmer. 

Another result is that the Arnett's law is roughtl ysatsified (the instantaneous radioactive energy input roughly matches the luminocity at peak) \cite{(Arnett 1979, 1982).}

\magenta{Bolometric Lightcurve} behavior with ejecta parameters have the following realtions. More massive ejeecta produces brighter, longer lasting transent. Higher velocitis gresult in shorter rise time and larger peak luminocities, but they alsi decline more reapidlly.

\magenta{Broadband lightcurves} shows that $r$-process material, its high opacity, leads to the suprerssion of the emission in optical band, while most of the eneregy is located at IR and FIR bands. In addition, the emission at high freuqncy bands, U, B, V is suppressed due to strong line-blanketing and lightcurves decay rapidly with time. This behaviour stems from the evolution of photospheric properties of $r$-process transiets, assuming that the observed SED roughly corresponds to a blackbody at the photospheric temperature and radius.

The photosphere can be defiled as 

\begin{equation}
    \tau(r_{ph}) = -\int_{\infty}^{r_{ph}} \bar{\kappa}_{P}(r)\rho(r)\text{d}r = 1,
\end{equation}

where $\bar{\kappa}_P$ is a \magenta{Plank mean opacity} evaluated from the line data.

The evolution of $T_{ph}$ and $\upsilon_{ph}$ consisits of first steady decline, that is an indicative of the expansion and cooling. However, at some point the $T_{ph}$ plateous, at $\sim2500$~K, which is the temperature that correspnds to the first ionisation state of lanthinides. Thus at this temperature a shart decrease in opacity uccures as elements recombine to neutral (Kasen +2013). 
Formed in the outer, cooler layers, the ionisation front moves inward, below which the photons are still trapped in high opacity material, and above which they escape. This photosphere shifts with ionisation fronts. Lightcurve color during this process remains approximattly constant, representing the temperature of lanthinide recombination. The process ends, when the ionisation fron reaches the inner boudnary of the ejecta, rendering its fully transparent and neutral. 

%%

\subsubsection{Uncertainties in the Opacities}

In this work additional source of uncertainties comes from \texttt{Autostructure}, whose initial optimisation leads to atomic level energies differ from expemtimental values. The compartions between different optimisations and resultsted structures us gine is \cite{Kasen et al 2013}.

The effect on bolometric lightcurves is less shart features with lower peak values and shallowere decay. This mimics the lower opacity.Overall the difference is about a factor of a $\sim 3$. Broadband lightcurves give a more noticable difference. In R-band the effect on the peak magnitude reaches $\sim 2$. 
Notably, that the stronger uncertanties arise from the approximation made, that all lanthinides species caan be approximated with Nd and its structure (some lanthinides can have $\times 10$ more lines), rresulting in underestimation of the opacity. 

%%

\subsubsection{A $^{56}$Ni-powered transient}
Ejecta from accetion disks, -- high $Y_e$, lathinidels-free.
\cite{(Surman et al. 2006, 2008; Metzger et al. 2008; Darbha et al. 2010}
but this is also adressed in lippuner with 
\cite{lippuner+2015}
For instance, if the electron fraction in ejects $Y_e \sim 0.5$, the composition is expected be primarely of nickel. This gives a contribution to the $r$-process powered transient. 

Modelled are two shperical non-interacting components, ejected immedeatly after merger, with the same density profiles and compositon that of pure $r$-process and that of pure $^{56}$Ni. 

In case where the fraction of the Nickel ejecta in the total ejecta is small, $M_{Ni} \ll M_{r-p}$, the addition of Nickerl leads to a faster occuring peak $\sim1$~day, and a longer plateu.
In case of cmparible mass of two ejecta compnents, the $^{56}$Ni one dominate the emission i nthe first $\sim 5$~days.
The effect of $^{56}$Ni on the SED is that it dominates the emission in U, B, V, R bands (optical bands) while the $r$-process material sets the emission in IR and FIR bands, 

%%

\subsubsection{Conclusion}

Strong effect of (high) lanthanides opacities on the emerge transient are shown. 
Higher opcities result in overall broader and deemer bolometric lightcurves, reaching $1-2$ weeks in total. For broad-band lightcurves change as well, as strong line-blanketing supresses the emission in ptical bands, leading to most amount of energy is emitted at NIR, IR and FIR. The photospherical properties behave predicted values, untill the temperature reaches the recombination of Lanthinides at which it plateous and ionisation fron moves through ejecta, rendering it treanparent and neutral.
This have an important observational consiquences and prospects for detectability. In particular the spft of SED to IR, and long decay time, and color temperature set by the lanthinides recombination.
In case of the combined ejecta, $r$-process and $^{56}$Ni, the SED includes the IR and optical compnents domianted by the former and latter respectively. 
The detectability is estimated to be up to $\sim 60$~Mpc with peak magnitude $M_{R}\sim -13$ visible to Pan-STARRS or PTF satellites. Optical emission would likely to be missed due to its supression.

The addition of lanthanides-free ejecta leads to the bright emission if optical bands with $M_{B}\approx M_{R} \approx -15$, that can be detected by several space observatories.  

%%
%%
%%

\section{Radioactivity and thermalization in the ejecta of compact object mergers and their impact on kilonova light curves}

Based on \cite{Barnes, Kasen,Wu, & Martinez-Pinedo (2016)}
The paper looks into the uncertanties in radiactive deceay and how its fragments thermallize, providing time-dependent thermilisation efficiencies. Fitting formulae are provided. It investigates the dependency of the net thermolisation efficiency on the $r$-process yields, and ejecta properties. Via radiation trasport models, new lightcurves are produced. 

%%

\subsection{Intro}

BNS (NSBH) source of kiloherz grav. waves \cite{(Abadie et al. 2010)} and EM counterparts: GRB (when matter fron tidal disruption event accretes onto a BH) \cite{(Paczynski 1986; Eichler et al. 1989a; Narayan et al. 1992)} , and optical/IR Kilonova \cite{(Li & Paczynski 1998; Metzger et al. 2010; Roberts et al. 2011; Barnes & Kasen 2013)}, generated by the decay of unstable heavy elements sythesized via $r$-process, \cite{Arnould et al. 2007} in the ejecta. The possible cites for $r$-process include the dynamical ejecta \cite{(Lattimer & Schramm 1974, 1976; Freiburghaus et al. 1999; Korobkin et al. 2012; Rosswog et al. 1999; Goriely et al. 2011)}, winds from the bound disk around the remnant \cite{(Fernandez & Metzger 2013; Perego et al. 2014; Just et al. 2015)} and within the accretion disk itslf \cite{[not here]}. 
Finally, the interaction between expanding ejecta and ISM creates radio afterglow \cite{(Nakar & Piran 2011)}. 
\gray{prompt emission form the free neutrons freeze out is missing between GRB and Kilonova}

Motivation for EM counterparts observations: 
sky localisation, identification of the host galaxy \cite{(Nissanke et al. 2013b; Kasliwal & Nissanke 2014; Holz & Hughes 2005; Dalal et al. 2006)}. Undeerstanding the structure of the neutron stars, their equation of states \cite{(Bauswein et al. 2013; Hotokezaka et al. 2013b; Bauswein et al. 2015)}.

Consider the Kilonova emission, It is believed to be isotropic \cite{(Roberts et al. 2011; Bauswein et al. 2013)}, peaking at around $1-10$~days postmerger \cite{(Barnes & Kasen 2013; Tanaka & Hotokezaka 2013; Grossman et al. 2014)}. THis allows a quick graviational wave event follow-up. In addition to providing information on the source location, ejecta properties, Kilonovae also hints on the details of heavy elements nucleosynthesisi and on the origin of the $r$-process elements in the universe. 

Modelling the Kilonova emission is challenging due to uncertainties in $r$-process nucleosynthesis, and consequently the heating rates, as well as on the atomist structure of heavi elements, and consequently, opacities. The latter was adressed in \cite{(Kasen et al. 2013)}.

There are three main points that have to be adressed when modelling the kilonova emission. 
\begin{itemize}
    \item Available energy: total heating rates due to the decay of heavy elements.
    \item Decay channels: at different phases of Kilonova different elements and their decay dominate the energy input.
    \item Suprathermal decay products themrolisation efficiency. How the $\beta$- $\alpha$-particles, $\gamma$-gays and fission fragments trasfer energy to the matter.
\end{itemize}
The observed Kilonova then, is the re-emitted energy of what was absorbed from decay products. 

The thermolisation efficiently remains not well understood topic. Previous studies include \cite{Metzger et al. (2010)} and \cite{Hotokezaka et al. (2016)} where too short timescales and neglect of charged particles thermilsation (accounting for large fraction of rad. energy) was present.

The study of thermolisation of $r$-process decay energy is complicated by the not well constrained details of the $r$-process nucleosynthesis, where each synthesized heavy element has a complex decay chain and its own timescales, for which experimental data not always available. Uncertainty in ejecta composition are compounded by the ill-constrained other ejecta properties, such are mass and temperature.

In this \red{fucking} paper the thermolisation efficiency of the $r$-process decay products is considered. 

%%
%%
%%

\subsection{Properties of the kilonova ejecta}

\subsubsection{Ejecta model}

Based on the recent merger simulations, aspted the $M_{ej}=5\times10^{-3}M_{\odot}$ and $\upsilon_{ej}=0.2$~c, where the ejecta velocity is defined through explosion kinetic energy $E_{k} = M_{ej} \upsilon_{ej}^2 / 2 $ with a certain degree of variation. 
Ejecta is assumed to be spherically symmetric and expanding homologously with the density profile as in \cite{Barnes & Kasen (2013)}. 

%%

\subsubsection{Magnetic Fields}
\red{This is something new.}

Inhereted from the neutron starts or seeding and amplified by the turbulence during the merger, Kilonova ejecta contains resudual magnetic field \cite{(Kiuchi et al. 2014, 2015)}. 

In a sufficiently strong filed, the motion of the charged particles is confined to the "flux tubes", that trace the field lines, as their Larmor radii is smaller then the coherence length of the magnetic field. 

For a homologous expanding ejecta with frozen in magnetic flux, the field strength scales as 

\begin{equation}
    B(t) \approx \frac{B_0 R_0 ^2}{R_{ej}^2} \approx 3.7 \times 10^{6} B_{12} R_{6}^{-2} \upsilon_{2} t_d^{-2} \hspace{3mm} \text{G}
\end{equation}

where $\upsilon_2 - \upsilon/0.2$~c, $t_d$ is elapsed time in days, $B_0$ and $R_0$ are the initial ejecta magnetic field strength and radius. Scaling is done to typical values. Note that initial $B_0$ lies in $(10^{9},10^{15})$. \red{This is not 'afterglow' magnetic filed...}.

The maximum Larmor radius for a charged particle 

\begin{equation}
    r_{L, max} = \frac{(E+mc^2)\upsilon}{qBc},
\end{equation}

where $m$, $q$, $E$ and $\upsilon$ are the particles mass, charge, kietic energy and velocity respectively. The maximum Larmor radius is achieved when $\boldsymbol{\upsilon}\perp\boldsymbol{B}$.

Assuming fiducial valies for the kinetic enenergy of the fission fragments, $\alpha$, $\beta$, and fission fragments (and their masses), the $r_{L,max}(t)/R_{ej}(t)$ can be computed. Note, that the assumption that all particles follow flux tubes is not very precises for $\alpha$ and $\beta$ with that have $r_L/R_{ej}\ll 1$.
This is so-called flux tube approximation for the charged particles trajectories. 
This has importnat implications for Kilonova. For instance, radial fields can push the charged particles through the ejecta quickliy, reducing their thermolisation efficiency. On the other hand, the toroidal field traps the charged particles, inproving it. 

Three configurations for magnetic fields are considered: radial $\boldsymbol{B}\propto\hat{r}$ (inspired by the ejecta motion), toroidal $\boldsymbol{B}\propto\hat{\phi}$ ispired by the NS and disk rotation, and random, inspired by the turbulence in ejencta.
\red{There should have been more explanation...}

%%

\subsubsection{Composition}

The ejecta composition is important to determine its thermolisation. Not only becasue it sets the properties of the background material, that affects the decay products energy loss, but it also sets the distribution of the radioactive energy between various decay channels and energy spectra of the decay products. 

The composition (on a set of smooth-particle hydrodynamic trajectories \cite{(Goriely et al. 2011)}) is computed from the the $r$-process nucleosynthesis, that is tracked via NRN \cite{Mendoza-Temis et al. 2015}. The network starts with matter in NSE, at $T\sim6$~GK, densities from HD histories and initial electron fractions $0.015-0.055$. 
\gray{similar to what was done in Lippuner papers for hdyro evolved ejecta dor BNS simulations}

Two types of trajectories are identified: slow ($\sim 90\%$) where neutron capture empties the free neutron reserve, allowing full $r$-process, that produces elements of the 3rd peak, and fast ejecta, rapid expansion of which allows some free neutrons to remain (as the neutron capture timescale is longer) \cite{(see also: Just et al. 2015; Goriely et al. 2014; Metzger et al. 2015; Mendoza-Temis et al. 2015).}.
The created here ejecta model thus includes the 'inner ejecta' that exhausts in free neutron reservoir, and 'outer ejecta' that expands too rapidly for that. Summing up trajectories in each class, the mass-integrated composiitons are obtained. 

T ostiudy the ejecte composition, (for $r$-process and photodissiociation rates) several nuclear mass models are assumed: Finite Range Droplet Model (FRDM; Möller et al. 1995),
the Hartree-Fock-Bogoliubov model HFB21 \cite{(Goriely et al. 2010)}, the Weizsäcker-Skyrme
model (WS3; \cite{Liu et al. 2011)}, and the Duflo-Zucker model with 31 parameters (DZ31; \cite{Duflo & Zuker 1995)}. The analysis showed that the postiion of the third peak is the abundance of coddesponding numcei depends on the mass model. This in turn affects the late-time kilonova.

Weak interactions are not included into the SPH particles trajectories. However it is expected taht they can raise the electron fraction, the $Y_e$ at the onset of the $r$-process significatly \cite{(Wanajo et al. 2014; Sekiguchi et al. 2015; Goriely et al. 2015)}. To estimate how exatcly, a different, highet initial $Y_e$ is considered. At higher $Y_e$ the synthesisi of $3$rd peak elements is reduced and for very high $Y_e$ ($\geq0.2$) it is supressed.

Notably, on a timescales relevant for the kilonova $\leq10$~days, the change of the ejecta composition because of the decay of heavy elements is rather small and primarely driven by the $\alpha$ and $\beta$ decays, that leave the abundance average properties relatively uncahnged. 

%%

\subsubsection{Radioactivity}

The $r$-process synthesized elements decay through different channels and have complex emission spectra. In general the energy release can be approximated with the power law, $\dot{\varepsilon}=\varepsilon_0 t_d^{-\eta}$, where $\varepsilon_0\approx 10^{11}$ ergs s$^{-1}$ g$^{-1}$ and $\eta\in(1.1,1.4)$ \cite{(Metzger et al. 2010; Roberts et al. 2011; Goriely et al. 2011; Korobkin et al. 2012)}.

The radioactivity of $r$-process elements not only incledes the $\beta$-decay, but also the $\alpha$-decay that any \magenta{translead} nuclei would undergo. Fission is also important for heavy nuclei ($A>250$).

For the (in the kilonovae) Kilonovae at time $t_{exp}$ the dominant energy source are the decay of isotopes with half-life $t_{1/2}$ of order of $t_{exp}$. In turn the $t_{1/2}$ is related to the emitted energy $Q$ at the decay. Fission has the higher $Q$, followed up by $\alpha$ decay and $\beta$ decay. However, fission as a very short $t_{1/2}\leq1$~day, and thus contribution little to the Kilonova. The $\alpha$-decay, however, might be an important source of energy, in addtion to $\beta$ decay. Notably, the two energy from these two decays have different thermolisation efficiency that ought to be considered. Thus, the abundances of the translead nuclei, and thus the relative importance of two decays, are important to determine the thermilisation.

The anslsys showed that the $\beta$-decays is a prime source of energy at all times relevant to Kilonova. After $\sim1$~day after merger, fission contributes only about $\sim10\%$. The $\alpha$-decay is more importnat, but its energy generation depends strongly on the assumed mass model. As $\alpha$ decay releases more energy per decay then $\beta$ decay, it increases the total energy generation from deaying $r$-process isotopes from $\sim1.5$times at $t\leq1$~day and $\sim2$~times at $t\geq1$~month. 

\gray{SO, overall, alongside the beta-day, the alpha decay and fission might be important in powering the Kiloiva. Andwhile fission is only relevant very early on, alpha decay can have a dominat contribution depending of the mass model assumet, and at late times, t>1day. This is relevant for low Ye only. At high Ye, the neutron deplition reduces the amouunt of heavy neulei produced.}

%%

\subsubsection{Emission Spectra of Decay Products}

Uncertanty in nuclear data and broad range of decay cahins makes computing the energy spectra of $r$-process decay difficult. THus a certain range of contributing decays is considered. 
Experimatally obtained mass excess,  available at ((AME 2012; \cite{Audi et al. 2012; Wang et al. 2012})) for some nuclei and theoretical considerations, i.e. (FRDM) mass excesses, gives the decay energies for $\alpha$ and $\beta$ decays. 

\paragraph{$\beta$-decay}

The $\beta$-decay releases energy in form of kinetic energy of electrons, neutrinos and energy of gamma-rays. In addition, there is a contribution from $\beta$-delayed fission and $\beta$-delayed neutron- and $\alpha$-emission but they were found to be not important for Kiloniova.

Consider an isotome $i$ with $Y_i$ being its number abundance, $Q_{\beta,i}$ the decay energy and $\tau_{1/2, i}$ its half-life. Then the rate at which this isotope decay generates energy is 

\begin{equation}
    \dot{\varepsilon}_{\beta,i} = Y_i \frac{Q_{\beta, i}}/{\tau_{1/2, i}}.
\end{equation}

Values for the half-life and decay energy can be obtaiend from experiments or from theoretical decay models.

The emittied energy is destirbuted between $\beta$- $\gamma$- and neutrinios as $20\%$, $45\%$ and $35\%$. As neutrinos free stream from ejecta, the upper limit of $\beta$-decay thermolisation efficientcy is $65\%$. 

The energy spectrum of $\beta$-decay was found to be insensitive to the mass model (as the decay is cominated by the same nuclei with hal-flife of roder to the time since merger) and weakly dependent on the intiial composition (as higher Ye results in lower mass nuclei with 1-2 transitions to stability and energy spectrum dominated by beta-particles.)

\paragraph{$\alpha$-decay}

Very heavy nuclei $A>200$ may undergo $\alpha$-decay, releasing energy at disctrete values in narrow range $E_{\alpha}\in(5,9)$~MeV.

\paragraph{Fission}

The total $r$-process decay energy contains also spontaneous, neutron-induced and $\beta$-delayed fission of heavy elements $A\geq 250$, that overall accounds to a few percent at $t\leq1$~day. Its contribution at later times if very limited. The fission produces nuclei near double magic number $(A,Z)=(132,50)$. The average kinetic energy of the fragment is $\sim 100$~MeV. The mass distribution and energy spectra of the fission frgments has a complex behaviour that changes with assumed nuclear physics model. 

%%
%%
%%

\subsection{Thermalization Physics}

Description of the decay energy thermolisation within the ejecta

\subsubsection{Gamma-Rays}

Two processes are relevant for the energy loss of $\gamma$-rays: photoionization and Compton scattering. The opacities for the former can be obtained fomf the Photon Cross Section Database (XCOM; \cite{Berger et al. 2010})  the latter can be obtained from Klein-Nishina formula. These processes produce non-thermal (suprathermal) electrons that \gray{loose energy via synchrotron emission?}.
In Kilonova ejecta, composed of high $Z$ elements, the photoionization cross-section dominates the opacities up to $1$~MeV, after which the Compton scattering takes over. The opacity was found to depend weekly on the ejecta parameters variation within the Kilonova and assumed constant.

\subsubsection{$\beta$-particles}

The supra-thermal electrons loose their energy via several processes:
\begin{itemize}
    \item The colomb interactions with free thermal electrons. It is relevant when energy of non-thermal electrons far exceed the energy of thermal ones. This is also called 'plasma losses'. The amount of energy loset per unit time (see \cite{(Huba, J. D. 2013)}) $\dot{E}_{\beta}^{pl}\propto f(E_{\beta}, n_e, \lambda_{ee}, T)$, where $\lambda_{ee}$ is the Coulomb logarithm for electron-electron scattering. The formula holds as long as $E_{\beta}\gg k_{B}T$. The $n_e$ can be computed assiming a fixed ionisation state for all elements. 
    \item Ionisation and excitation of the atomic electrons. The energy loss due to that has a well-established formulation \cite{(Heitler 1954; Berger & Seltzer 1964; Gould 1975; Blumenthal & Gould 1970}; \cite{Chan & Lingenfelter (1993)} \cite{Milne et al. 1999)}. Here $\dot{E}_{\beta}^{IE} \propto f(n_{e,b}, \upsilon_{\beta}, E_{\beta}, \tau, \bar{I})$, where $n_{e,b}$ is the number density of bound electrons, $\tau=E_{\beta}/m_e c^2$, $\bar{I}$ is the average ionisation and excitation potential (see e.g., \cite{(Segre 1977)} and \cite{PChan & Lingenfelter (1993)} for how it can be computed). 
    \item Synchrotron emission. In the bagnetic field of strength $B$, the electron with velocity $\upsilon_{\beta}$ has an energy loss rate $\dot{E}_{\beta}^{syn}=(4/9)r_e^2 c \gamma^2 (\upsilon_{\beta}/c)^2 B^2$, where $\gamma$ is the particle lorentz factor \gray{$\Gamma$}. For kilonova the $\dot{E}_{\beta}^{syn}$ was found to be orders of magnitude smaller then $\dot{E}_{\beta}^{IE}$ and thus neglected. 
    \item Free-Free or Bremsstrahlung losses, where the energy loss rate can be written as $\dot{E}_{\beta}^{Brem} = n_i \upsilon_{\beta}(E_{\beta}+m_e c^2)Z^2 r_0^2 \alpha \phi_{rad}$, where $n_i$ is the number density of scattering speciesn, $\alpha$ is the fine-structure constant, $\phi_{rad}$ is the energy dependent empirical fitting constant (see \cite{Seltzer & Berger (1986)}).The free-free energy loss was found to contributiong moderately for $E_{\beta}\geq1$.
\end{itemize}

In the outer ejecta, whose ionisation degree is low, $\dot{E}_{\beta}^{IE}$ dominates the 'PL' and 'Brem' losses. The outer ejecta has higher thermalizatio nrates. 

%%

\subsubsection{Alpha-particles}

The main energy loss of suprathermal $\alpha$-particles experience is due to interaction with 1) bound and 2) free electrons (plasma losses), -- interaction with heavy ions and nuclei are not as important. The energy loss rate (\cite{Huba, J. D. (2013)}) reads $\dot{E}_i^{pl} = f(E_i, \mu_i, Z_i, n_e, \lambda_{ie}, T)$, where $E_i$ is the ion's kinetic energy, $\mu_i$ ions mass, $Z_i$ is the charge in, $\lambda_{ie}$ is the Coulomb logarithm for ion-electron scattering. The energy loss rates can be taken from NIST’s ASTAR database \cite{(Berger et al. 2005)}, which is however limited and mapping of a full ejecta composition on a reduced set present in database is required.

At low energies the plasma losses dominate, when $E_{\alpha}\leq1$~MeV. At high energies, however, interaction with bound eletrons become important.

%%

\subsubsection{Fission fragments}

\gray{skipped the method}

Fission fragments interact with atomic nuclei, as well as free and bound electrons. The latter are especially important at high energies, while the interaction with free electrons important up to $10$~MeV. Fission fragments thermilize more efficiently in the outer ejecta.

%%
%%
%%

\subsection{Analytic Results}

Consider the inner ejecta and the analytical treatment of thermalization from \cite{Metzger et al. (2010)}; \cite{Hotokezaka et al. (2016)}.

%%

\subsubsection{Analytic estimates of thermalization timescales}

The thermalization of each decay product and within each decay cahnnel sets the net thermalization of the energy from radioactive  decay of $r$-process material. In addition, density of the medium, set by its mass anc velocity, also affects the energy loss rate.
The density history can be approxumated considering an expanding homogenous sphere with mass $M_{ej}$, and kinetic energy $E_k = M_{ej} \upsilon_{ej}^2 / 2$ as $\rho(t) \propto M_{ej} \upsilon^{-3} t^{-3}$. 
The efficiency of thermalization drops at, $t_{ineff}$, when the ejecta expansion timescale catches up with the time needed for a particle to thermalize.
Recalling the kilonova peak time $t_{peak} \propto (A M_{ej} \kappa / (\upsilon_{ej} c))^{1/2}$ where $A=.32$ is the scaling factor from radiation trasport simulations \cite{(e.g. Barnes & Kasen 2013)}. 
Thus the thermalization have an important effect on the lightcurve if $t_{ieff} < t_{peak}$. 

For each decay produced the $t_{ineff}$ is different. 
\paragraph{$\gamma$-rays} thermalization depends on how efficiently they can deposite their energy before escapin the ejecta. This depends on the ejecta optical depth $\tau\approx\rho\kappa_{\gamma}R_{ej}$. IF the optical depth $\tau<1$, $\gamma$-rays can escape. Consider the $E_{\gamma}\geq 1MeV$, for which opacity of importance is the Compton opacity $\kappa_C\approx5\times10^{-2}$cm$^2$g$^{-1}$. For lowere energies the photoionisation opacity $\kappa_{PI}\approx 1$ cm$^2$g$^{-1}$ domiantes. 
One finds that for $\gamma$-rays $t_{ineff}<t_{peak}$.

\paragraph{$\beta$-particles}.
It was found that the loss of energy by the $\beta$-particles is rather constant with energy. The thermalization time is $t_{th}\propto(E_{\beta;0}, M^{-1}, \upsilon^{3}, t^{3}$. 
In order the thermalize efficielty, the expansion timescale $t_{exp}$ must be smaller then $t_{th}$. This freezes out at $t_{ineff}$. Comparting this to the $t_{peak}$, one finds that for a standart kilonova ejecta mass and velocity these two are compatible.

Next, consider magnetic foelds. 
Radial magnetic field allows $\beta$-particles to escape the ejecta avoiding thermalization of its energy. To escape timescale then is $t_{esc}\approx R_{ej}(t)/(\lambda \upsilon_{\beta,||})$, where the $\lambda R_{ej}$ is the coherence lendgth of the magnetic field, and $\upsilon_{\beta,||}$ is the parallel to the field line component of the particle velocity. 
Notably, for a typical parameters of the ejecta mass, velocity and a radai flied (coherence length $\lambda=1$) the escape time is less then a peak time and thus such losses are important. However for a disordered field, even a slight entanglement prevents paraticles from escaping.

\paragraph{$\alpha$-particles and fission fragments}

The thermalization of fission fragments and alpha-particles is similar to beta particles. However, being might more energetic, (especially the fission fragments) and they loose their energy faster. The efficientlcy of $\alpha$-paroticles thermalization is similar to that of the $\beta$-particles. Fission fragments can efficietly thermalize out to late times, $t_{ineff;ff}\gg t_{peak}$.

The thermalization of both $\alpha$-particles and fission fragments is slightly rediced by the fact that moving slowly through the expanding ejecta, with trajectories vound around magnetic field lines, they loose their kinetic energy (in the comoving frame).

\subsubsection{Summary of thermalization}

Particles that can thermalize efficiently at $t=t_{peak}$ are $\alpha$-particles and low energy $\beta$-particles. 
Less clear is the ability to therlamize of high energy $\beta$-particles and $\gamma$-rays. 
\gray{See fig. 3.8}

\subsubsection{Analytic thermalization model}

The model predicts the thermalization efficiency of massive particles. 
A set of assumptions is empolyed: the energy generation rate evolution (exponent), density profile of the ejecta, $\dot{E}=f(\rho)$ of a particles does not depends on its $E$; single $E_0$ is set for all particles of a given type.

Consider the thermalization efficiency, $f(t) = \dot{E}_{th}(t) / \dot{E}_{rad}(t)$, which is the ratio of energy emitted by radioactive decays to energy absorbed by the ejecta at $t$. 
Assume that the $\dot{E}_{rad} = \dot{\varepsilon}_0(t_0/t)$, where $\dot{\varepsilon}_0 = 10^{11}$M$_{ej}$ergs s$^{-2}$ and $t_0=1$~day. 
Assume that a single particle following a homologous expansion looses energy as $\dot{E}_{part}(t) = \psi \rho_0 (t/t_0)^{-3}$, with $\psi\rho_0=\dot{E}_{part}(t_0)$. 
Then the thermalization rate for an ensebmble of particles of a given type (with unique $\psi$) is $\dot{E}_{th}(t) = N(t) \psi \rho_0 (t/t_0)^{-3}$.
Consider the time $t$. At this time the particle energy satisfies $E_{part}(t)=E_0 - \int_{t_i}^{t}\psi\rho_0 (t'/t_0)^{-3}dt' = 0$, and then $t_i = ((\psi\rho_0 t_0^3 t^2)/(2E_0t^2 + \psi\rho_0 t_0^3))^{1/2}$. 
The number of particles that are still present at time $t$ is $N(t) = (\dot{\varepsilon}_0 t_0)/(2E_0) \ln[ 1+2(t/t_{ineff})^2 ]$, where $t_{ineff}$ is the inefficiency timescale (see prev.sec.)
Then the ratio between the energy of a partiles of type $p$ that by the time $t$ has bee thermalzied is 
\begin{equation}
    f_p(t) = \frac{\dot{E}_{th}}{\dot{E}_{rad}} = \frac{\ln\Bigg[1 + 2 \Big(\frac{t}{t_{ineff;p}}\Big)^2\Bigg]}{2\Big(\frac{t}{t_{ineff;p}}\Big)}
\end{equation}
This equation is valid for $\beta$- $\alpha-$ particles and fission fragments \gray{here the corresponding $t_{ineff}$ van be inserted.}

For the $\gamma$-rays however a separate treatment is required. The probabolity of a $\gamma$-ray to interact then defines its efficiency to thermalize. 
Consider the optical depth $\tau\approx\rho\kappa_{\gamma}R_{ej}$ with $\bar{\kappa_{\gamma}}$ as a $\gamma$-ray opacity averaged over the enission spectrum. Then, the interaction probability is $f_{\gamma}(t) \approx 1 - e^{-\tau} = 1-e^{-(t/t_{ineff;\gamma})^{-2}}$.

%%
%%
%%

\subsection{Numerical Results}
\gray{bried}

Consider the 3D transport of $\gamma$-rays, fission fragments, $\alpha$- and $\beta$-particles in the magnetized expanding medium. 


\paragraph{Thermalization efficiencies} The most efficient themalization, $f(t)>0.5$ is for fission fragments at late time ($t>2$~weeks). $\alpha$- and $\beta$- particles have lowere $f(t)\sim0.5$ and this time. The $\gamma$-rays thermalize withing few days. 

The magnetic fields play an important role, they allow to \textbf{diffuse}, if the field is radial or slightly tangled, reducing $f(t)$; \textbf{escape}, skipping the thermalization, reducing $f(t)$; and leading to \textbf{frame-to-frame} effects, the loss of a particle energy in a homologously expanding shell, in comoving frame, hence -- less energy particles has, the lower $f(t)$, which are of particular importance for $\alpha$- particles and fission fragments.
The $f(t)$ is reduced more for toroidal field, radial -- minimum, and random fields are in-between.

The relative thermalization efficiency of various outcomes is consistent between different ejecta setups. However, the exact values of $f(t)$ depend of the ejecta mass and velocity sensitively. In particular $f(t)$ rises with $M_{ej}$ and falls when $\upsilon_{ej}$ increases. 

The massive particle thermalization increases with increasing asphericicty. 

The $r$-process yields have an important effect on thermalization efficiency. Producing more trans-lead elements, that primarely decay via fission and emission of alpha particles, results in increesed importance of these particles in total thermalziation rates.

\subsubsection{Effects on the kilonova lightcurves}

\gray{The code \texttt{SEDONA} is used}.

\paragraph{Analytic fit to thermalization efficiency}

\begin{equation}
    f_{tot}(t) = 0.36 \Bigg[e^{-at} + \frac{\ln(1+2bt^d)}{2bt^d}\Bigg],
\end{equation}

where $a$, $b$, $d$ are the fitting constants. 
 
Consider the radiation traposrt code. Consider low $Y_e$ ejecta, that robustly reproduce lanthinides and actinides and that and fir which the enhanced opacities are to be used. The inclusion of sophysticated thermalization effciency models leads to following results.
The kilonova peaks at earlier time and dimmer, with further decreased luminocities later in time. This is especially important for low mass fast ejecta. Lightcurves of such ejecta fade quicker and is overall less bright. 
In addition, the magnetic fields have an important effect on lightcurves. Random or toroidal fields, that confine particles, increase thernalisation efficiency, raising the kilonova luminocity. 
Effects of nucleosynthesis lies in setting the amount of translead uclei and the amount of energy produced by $\alpha$-decay. 

%%

\subsection{Late-time light curve}

The late time kilonova allows to guage the relative importance of $\alpha$- and $\beta$ decay, as in case of the former all the energy is efficiently transfered to particle, while in the latter a quarter is lost to neutrinos a $\gamma$-rays.

The difference in lightcurve late luminocity changes by a factor of two, depending on its $r$-process composition -- importance of $\alpha$-decay. 
Notably, presence of massive fission fragments would wirther increase it as they thermalize efficienly out to the late times. However, very neutron rich conditions are required.

An important insight into the ejecta composition can be inferred from considering the ration of the peak luminocity (where luminocity is equal to instantanous energy deposition rate (“Arnett’s Law”, Arnett 1982) ), to the late time luminocity, (as at low opacity the luminocity is again given by the energy deposition rate)

Assuming that energy deposition rate from $r$-porcess decay is $\dot{\varepsilon}\propto t^{-\eta}$, 
then

\begin{equation}
    \\frac{L(t_{peak})}{L(t_{tail})}  = \Bigg[\frac{t_{peak}}{t_{tail}}\Bigg]^{-\eta} \frac{f_{tot}(t_{peak})}{f_{tot}(t_{tail})}
\end{equation}

where $t_{tail}$ is late times $t_{tail}>t_{peak}$ and $\eta$ is the decay index. 
This ratio allows to guage the thermalization efficiency and hence the decay products and hence the composition. 

Note, haveter, that given other uncertanties, the difference in $L_{peak}/L_{tail}$ might be difficult to asses.

%%

\subsection{Conclusion}

The study of thermalization efficiency of decay products of heavy nuclei synthesized via $r$-process within the ejecta was conducted. The dependency on time and composition of thermalization efficiency was inferred. It was shown taht $f(t)$ can call below $0.01$ at late times, significatly modifying lightcurve evolution. For lower $Y_e$ the $f(t)$ is higher, as produced heavy nuclei are prone to $\alpha$-decay, with $\alpha$-particles thermalizign efficiently. (less efficiently do $\beta$-partociels and then $\gamma$-particles thermalzie)

Rad.transfer simulations incorporating the thermalization efficiency show dimmer and earlier peaks then was what reported before.

The defining Kilonova feature is red color. 

%%
%%
%%

\section{A GRB and Broad-lined Type Ic Supernova from a Single Central Engine}
\red{skimmed}
\red{missed :) }

%%
%%
%%

%%
%%
%%

\section{Kilonova by Brian D. Metzger [paper; overview]}

BBH detections -- window into the Universe -- foramtion channels \cite{(Abbott and et al. 2016c)}.
BBH detecttions -- probing GR \cite{(Miller 2016)}
GW + EM observ. -- big picture \cite{(Bloom et al 2009)}
Similar situation was with GRB, biving info on SN \cite{(Fruchter et al 2006; Fong and Berger 2013)}.
GW (+EM) constraining 'z' of the source can help with cosmic expansion history \cite{(Holz and Hughes 2005; Nissanke et al 2013)}. 

BNS or NSBH required for EM signal. Predicted rates are $~0.2 - 300$ per year \cite{(Abadie and et al. 2010; Dominik et al 2015)}. Observations of pulsar systems give $\sim 8 $ BNS per year \cite{(Kalogera et al 2004; Kim et al 2015)}. 

A good sky localisation is only possible if EM signal accompanies GW \cite{Abbott and et al. 2016b,a}, even when future GW detectors are operational \cite{(Fairhurst 2011; Nissanke et al 2013; Rodriguez et al 2014).(Somiya 2012)}.

%% sGRB
Observational \cite{(Fong et al 2014b)} and theoretical \cite{(Eichler et al 1989; Narayan et al 1992)} evidence link mergers to sGRB. 
sGRB is a GRB with a duration less then 2 seconds \cite{Nakar 2007; Berger 2014)}.
BNS and NSBH are associated with sGRB. This is due to a lack of an alternative mechanism for sGRB, \textit{e.g.,} a collapse of an accreting NS, rotating as a solid body is not expected to produce an accretion disk sufficient for sGRB \cite{Margalit et al (2015),Shibata (2003)}. 
Powered by the accretion of disk onto the compact BH (or NS) withing seconds is expected to generate sGRB.
Afterglow of the sGRB can be detected in X-ray. Gamma-ray telescopes have worse angular resolution. 

The detection rate fo sGRB in ALIGO detectrion volume is low \cite{(Metzger and Berger 2012)}. This is because the emission for GRB is beemed into a narrow solid angle by the bulk relativistc motion of the jet \cite{Fong et al 2015; Troja et al 2016)}. However, the necessety to hava a binary plane 'face-on' for the GRB detection results in GW's begin slightly brighter. This increases the overall search volume by around an order of magnitude.

In case of the off-axis jet, its detection becomes possible when jet starts to slow down due to interaction with ISM. Thus, eventually, an observer enters the casual emission region of the synchrotron afterglow \cite{(Totani and Panaitescu 2002)}.
This is so-called 'orphan afterglow'.
Its rapid evolution in X-ray requires on observer to be close to the jet axis. In optical, the volution is slower and an observer within an angle, twice the jet openning angle can detect it \cite{(Metzger and Berger 2012, their Figs. 3 } This off-axis afterglow is not particularly usefill as a coutnerpart to mergers. This is however less so for a structured jet, taht contrary to the 'top hat' jet has an angular structure with wings albeit being slower, still ultrarelativistic and can produce a luminouse optical synchrotron afterglow. \cite{(Perna et al 2003; Lamb and Kobayashi 2016)}

Additionaly, a jet interacting with ejecta can generate a mildly relativistic 'cocoon' of shocked jet material. Emission from it is more isotropic. Here the main uncertanty is the mixing of ejecta and jet material is the main questions \cite{(Lazzati et al 2016; Nakar and Piran 2017)}

%% Kilonova
A more isotropic counterpart, day-to-week long, thermal, supernovae-like transient, powered by the radioactive decay of heavy, neutron rich elements, synethsized in the exapnding merger ejecta \cite{(Li and Paczyinski 1998)}. They accompany BNS abd NSBH (where ejecta os present) mergers and surve as a probe of origin of meaviest elements in the universe \cite{(Metzger et al 2010b)}.

%%

\subsection{Historical Background}

Neutron capture synthesizing half of the elements beyond iron \cite{Burbidge et al (1957) and Cameron (1957)}.
If the time scale for neutron capture is shorter than that of the $\beta$-decay -- this is $r$-process. Its astrophysical origin is still not fully understood \cite{(e.g., Qian and Wasserburg 2007; Arnould et al 2007; Thielemann et al 2011}. 
SN were a promising cite, as they have short delay after the star formation -- allowing to explaing the early MP starts abundances \cite{(Mathews et al 1992; Sneden et al 2008)}. In particular, the high entropy $\nu$-wind, where the $\alpha$-rich freeze out occures, resulting in high ration of neutrons to seed nuclei, from proto-neutron star were considered a promising cite \cite{(Duncan et al 1986; Qian and Woosley 1996)}. 
Later models however disfavoured this channel \cite{(Thompson et al 2001; Arcones et al 2007; Fischer et al 2010; Huudepohl et al 2010; Roberts et al 2010; Martinnz-Pinedo et al 2012; Roberts et al 2012)}. 
Another possibility for an $r$-process was wind launched by magneto-centrigugal acceleration from proto-neutron stars born with high spin and large $B\geq10^{14}$G. \cite{(Thompson et al 2004)} See also \cite{(Winteler et al 2012)} and \cite{(Thompson 2003; Metzger et al 2007; Vlasov et al 2014)}. However, the confirmation from 3D MHD-SNe simulations is yet to come. In addition, the rate of such hyper-energetic supernovae is considerably lower then CCSN. \cite{(Podsiadlowski et al 2004)}, requireing very high yelds per event to explain galactic abundances.

Merger of NSs or NS \cite{Meyer 1989). Symbalisty and Schramm (1982)} and BH \cite{(Hulse and Taylor 1975), Lattimer and Schramm (1974, 1976)}, the decompression of the neutron-rich ejecta, and BH was considered as a promising cite.
The connection between BNS and sGRB was made by \cite{Blinnikov et al (1984)} and \cite{Paczynski (1986)}. 
Later it was showed that BNS merger eject material \cite{Davies et al (1994)} and that this material indeed allow for the $r$-process to occure \cite{Freiburghaus et al (1999)} (in the BNS model of \cite{(Rosswog et al 1999)})

The composition of the ejecta from BNS/NSBH merger and a wind from proto-neutron star differs. The latter represents a gradual acceleration of matter, allowing for the neutrino absorption to raise the electron fraction of the wind. In the former, the assymetry and rapid dynamics allow a large freaction of the ejected matter to preserve its low value of $Y_e$.

Producing around $10^{-3}M_{\odot}$, to $10^{-1}M_{\odot}$ of $r$-process material BNS nad NSBH, they can eplain the average ober the age of the Galaxy production rate of heavy $r$-process nuclei within the large uncertainty \cite{(Qian 2000),(Bauswein et al 2013a).}. 

Additional support for rare, high yield events being the main source of $r$-process elements.
Ocean floor Measuremnts of $^{244}$Pu displays lower abundances by $\sim 100$ with respect to expectations from a frequent low yeild events. \cite{(Wallner et al 2015; Hotokezaka et al 2015)}. 
A particular case of a dwarf galaxy Reticulum II, with stars very enriched with $r$-process elements suggests taht there has been a single event with of high yield \cite{(Ji et al 2016)}.

A problem of BNS being the dominant source, the time delay, can be overcome if certain formation channels are considered \cite{(Belczynski et al 2002; Voss and Tauris 2003; Ramirez-Ruiz et al 2015)}

%%

\cite{Li and Paczynski (1998)} suggested that an electromagnetic transient can be posered by the radiactive decay of the material ejected in BNS or NSBH mergers. They also showed that contrarty to the normal SNe, the ejecta would quicly become transparent to its own emission, peaking on a timescale of around a few days. 
The main difficulty in this pioneering work was the lack of a nucleosynthesis model to model to estimate the radiaactive heating of the ejecta. 

A simplified approach here canb e based on following arguments. If the total energy release by the radioactive decay of isotopes '$i$' is $\dot{Q}_i \propto \exp(-t/\tau_i)$, where is $\tau_i$ is the half-life. If $\tau$ are distributed equally per logarithmic time (at any $t$ the dominant species have $\tau\sim t$), the heating rate of the ejecta at time $t$ is

\begin{equation}
    \dot{Q}_{LP} = \frac{f M c^2}{t}
\end{equation}

where $f$ is free parameter and $M$ is the ejecta mass.

Notably, later models, that considered NRN computed on thermodynamic histories of the expanding ejecta (from NR simulations) indicated that heating ray depends on time, at late time, via a steep power law \cite{(Metzger et al 2010b; Roberts et al 2011; Korobkin et al 2012)}. See also \cite{Hotokezaka et al (2017)}
for the discussion on physical principles behind this decay.

In addition, provieded by \cite{Li and Paczynski (1998)}, normalization $f$ resulted in overestimation of the peak luminocity of the Kilonova, that plagued the Kilonova seraches for the upcoming decade \cite{Rosswog (2005), (Dong et al 2016), (Bloom et al 2006), Kocevski et al 2010}. 

The first self-consistent estimation of the heating rates based on the NRN calculations of the $r$-process in the ejecta, carried out by the \cite{Metzger et al (2010b)}, showed that the based on the dynamical ejecta electromagnetic transient is $\sim10^3$ times brighter then nove -- \textit{i.e.,} the Kilonova term was coined. It was also shows the ejecta electron fraction does not affect the heating rates considerably, and performed the first estimations ofthe thermalization efficienty of decay products. 

The term macronova was however coind by \cite{Kulkarni (2005)} who considered a transient powered by the decay of radioactive $^{56}$Ni and free neutrons. However, as it was shown later, $^{56}$Ni can only be formed in small quantities on the outscurts of the ejecta

Besides the intricate radioactive heating rates, determining the ejecta opacity presents a formidable challenge. 
The general lack of experimental data and numerical models of the singly and doubly ionized heavy $r$-process elements opacity, complicates the issue. Initially, gray iron group opacities, used by supernovae community, were adopted \cite{Roberts et al (2011)}. However, it was shown later that this is an underestimation of the $r$-process elements opacities \cite{Kasen et al (2013)}. The high line density and complex atomic structures of lanthinides and actinides can raise their opacities significantly. This was later confirmed by \cite{Tanaka and Hotokezaka (2013).} . Higher opacities shirt the peak time to later times $\sim 1$~weak \cite{(Barnes and Kasen 2013)} and shirt the spectral peak from optical/UV to NIR. 

%%

\subsection{Basic Ingredients}

Consider a shell of an ejecta, expanding with constant velocity, $\upsilon$, such that $R\approx\upsilon t$ at any point in time $t$. Assume spherical symmetry, justified by ejecta previouse lateral expansion \cite{(Roberts et al 2011; Grossman et al 2014; Rosswog et al 2014).}

Assume further that the ejecta is hot. The thermal energy is not immedieately radaited away due to high optical depth and long diffusion timescales:

\begin{align}
    \tau &= \rho \kappa R = \frac{3}{4}\frac{M\kappa}{4\pi R^2}\\ 
    t_{diff} &\approx \frac{R}{c}\tau = \frac{3}{4}\frac{M\kappa}{4\pi c R} = \frac{3}{4}\frac{M\kappa}{\pi c \upsilon t}
\end{align}

where $M$ is the ejecta mass, $\kappa$ is the opacity (cross section per unit mass), $\rho$ is the density,. \textit{e.g.,} $\rho=3M/(4\pi R^3)$ is the mean density.

The diffusion timescale decreases with ejecta expanding. 
When $t_{diff}$ reaches $t$, the radiation can escape the ejecta \cite{Arnett 1982}. Hence, the characteristic timescale of the peak of emitted radiation 

\begin{equation}
    t_{peak} = \Big(\frac{3}{4}\frac{1}{\beta}\frac{M\kappa}{\pi \upsilon c}\Big)
\end{equation}

where the constant $\beta$ depends on the exact density profile of the ejecta. 
The $t_{peak}$ is of order of days for lanthinides-free and weeks for lanthinides rich ejecta.

Regading the ejecta temperature. If there is no additional heating, even ejected with $10^9$~K, the ejecta cools very quuicly via adiabatic losses. Thus, when it reaches $R_{peak} = \upsilon t_{peak}$, when it is transparent, it is effectively cold and invisible. 

However, there is an additional source of energy withing the ejecta. Several of them, in fact. These might include the contribution from the central engine, $r$-process element radioactive heating). These contribute to the total heating rate $\dot{Q}(t)$, that relates to the peak luminocity of the transient via \textit{Arnett's Law} \cite{(Arnett 1982)}.

Thus, overall, three ingredients are required to understand the observations of Kilonovae. These are 
\begin{itemize}
    \item The ejecta properties: $M$, $\upsilon$, $Y_e$,
    \item The composition of the expanding ejecta and its optical opacity.
    \item Dominant sources of energy within the ejecta, heating rate $\dot{Q}(t)$, and how efficient this energy thermalizes.
\end{itemize}

\subsubsection{Ejecta in BNS and NSBH}
\red{Mostly skipped.}
Dynamical Ejecta (tidally or via compression induced heating) \cite{Fernandez and Metzger 2016}. For NSBH only the former is possible and requires low BH mass and its high spin. \cite{(Foucart 2012)}
For BNS, the remannts fate determins the ejecta properties \cite{(Shibata and Uryu 2000; Shibata and Taniguchi 2006).}. Wether it is a stable, long lived, or a promptly collapsing massive neutron star \cite{(Hotokezaka et al 2011; Bauswein et al 2013a).}. This fate is not immedeatly known and depends on the EOS and rotational support \cite{(Baumgarte et al 2000; Ozel et al 2010; Kaplan et al 2014)}. This led to the intoroduction of somewhat misleading nominglature, massive, supermassive, hypermassive and supramassive NS, whose lifetime and support agaisnt collapse after merger is the distinguished characteristic.

The minimum (of maximum) allowed NS mass is set by pulsar observations \cite{(Demorest et al 2010; Antoniadis et al 2013)} and is around $M_{max}(\Omega=0) = 2M_{\odot}$.

Typical ranges of mass/velocity for dynamical ejecta.

Processes in dynamical ejecta: quasi-radial pulsations of the remnant after the interphase contact (rebounce?) creating \textit{shock-heated matter} \cite{(Oechslin et al 2007; Bauswein et al 2013a; Hotokezaka et al 2013a)}; and spiral arms from \textit{tidal} interactions during the merger, which expand outwards in the equatorial plane due to angular momentum transport by hydrodynamic processes.

In NSBH, the tidal forces, disrupting the star, lead to the ejecta, confined largely to the equatorial plane \cite{(Kawaguchi et al 2015)}

Ejecta electron fraction strongly depends on the neutrino treatment. Absence of the weak interactions lead to a very low $Y_e$ \cite{(Goriely et al 2011; Korobkin et al 2012; Bauswein et al 2013a; Mendoza-Temis et al 2015).}. Inclusion of the electron-positron captures, neutrino irradiation in GR leads to broader range $Y_e$. \cite{(Sekiguchi et al 2015; Radice et al 2016).}. 

%%

Shorly after the formation the disk undergoes a quick accretion phase on NS or a BH. At this time the disk generates outflwo, driven by the neutrino heating \cite{(Surman et al 2008; Metzger et al 2008d)}, as itself, a disk is a source of neutrinos, similarly to proto-neutron star \cite{(Popham et al 1999)}.
Studies shows that in case of a long-lived remnant, this mechanism can ejecta $\sim10^{-3}M_{\odot}$, albeit from the NS intslef, rather then from the disk \cite{Dessart et al 2009; Perego et al 2014; Martin et al 2015; Richers et al 2015)}

It was shown the the disk evolution is guvern by the angular momentum transport, mediated by the magnetic stresses, generated by MHD turbulence, that in turn is a result of MRI \cite{Metzger et al 2008a,Metzger et al 2009}. The disk expands and cools, eventually becoming geometrically thick configuration \cite{(see also Lee et al 2009; Beloborodov 2008)}.  The outflow in this stage is generated by the viscous turbulent heating and nuclear recombination \cite{(Kohri et al 2005)}. Due to the week interaction freeze-out, the composition of the outflow becoming neutron-rich \cite{(Metzger et al 2008a, 2009)}. Meanwhile the neutron, located within the degenrate disk, is shielded and enters the outflow only when disk expands sufficiently for week interaction to cease being important.

A broad range of compositions and masses of the ejected material were estimated from modelling the accretion disks around BH \cite{Fernandez and Metzger 2013; Just et al 2015,Fernandez et al 2015a,Kiuchi et al (2015)} andmassive neutron star, reaching up to $0.9M_{disk}$ \cite{Metzger and Fernandez 2014),(Kasen et al 2015)}.

The remnant itself is predicted to be a source of a wind. Neutrino- or magnetically drivem, accompanying the remnant kelvin-Helmholtz contraction to the final cold state \cite{(Dessart et al 2009)}. This is of particualr relevence for stable remantnts. Winding of the magnetic field by latitudinal differential rotation \cite{(Siegel et al 2014)} in the beginning and subsequent Magneto-centrifugal acceleration modifes the composition and properties of the wind \cite{(Metzger et al 2007; Vlasov et al 2014)}.

%%

\subsubsection{Opacity}

Fig.4 \red{that I thing worth using here} displays various sources of opacity within the expanding ejecta, $3$~days postmerger.
%% free-free opacities
For the photons of the lowest energy (FIR), \textit{e.g.,} lowest frequency, the free-free absoption in the ionized gas is the dominant source of opacity. Expansion, the recombination removes free $e$, also decreases $\rho$, and thus $\kappa_{ff}$. 
%% bound-bound opacities
For NIR/optical $\nu$ photons, the bound-bound transitions are the main source of opacity. Here the dependency on the ejecta composition strongly affects the \textit{effective} continuum opacity. If the most complex atoms in the ejecta belong to the iron group, with valence $d$-shell electrons, then the opacity is moderate. However, presence of elements with valence, partially filled $f$-shell, (lanthinides \& actinides), then the opacity increases by up to two orders of magnitude \cite{(Kasen et al 2013; Tanaka and Hotokezaka 2013; Fontes et al 2015, 2017)}. Bound-bound opacity rises with photon $\nu$ (as the number of lines).
The \magenta{plank mean expansion opacity} can be approximated as $\kappa_r = 200 (T/4000K)^{5.5}$ cm$^2$g$^{-1}$ for $T\in(1-4)\times10^3$~K and just $\kappa_r=200$ cm$^2$g$^{-1}$ for $T\in(4-10)\times10^{3}$~k, motivated by Fig.10 of \cite{Kasen et al (2013)}. More qccurate opacity estimations are palgue by the com-plexity in atomic structure. The quantum mecnahics models of high-Z atoms exists, but has to be calibrated to the so far absent experimental data.
%% Line opacity -> Effectve continoum opacity
Additional complexity arises in approximation the line opacities to the effective opacity.
One common way is to consider the line expansion opacity formalism \cite{Pinto and Eastman 2000}, that is based on the Sobolev approximation. This method was applied to kilonovae modelling by \cite{Barnes and Kasen (2013)} and \cite{Tanaka and Hotokezaka (2013)}. However, it is unreliable if line width is large (\textit{i.e.,}) if line spacing of strong lines become comparable to the intrinsic thermal line width \cite{(Kasen et al 2013; Fontes et al 2015, 2017).}. 
%% clumping
In addition, clumping that might occur when $T\leq10^3$~K might have a strong effect \cite{(Takami et al 2014)}. The formation of the 'r-process dust' may have a complex effect on optical/UV opacity. The process of dust formation is complex and not well understood in general \cite{(Cherchne and Dwek 2009; Lazzati and Heger 2016)}
%% Ejecta re-ionisation
For even higher energy photos, UV/X-ray, bound-free transitions dominate the opacity. For that ejecta ought to become mostly neutral, which occures natually as it cools, unless there is a source of ionizing radiation, \textit{e.g.,} the remnant. See \textit{e.g.,} for details \cite{Metzger and Piro (2014)}. Even though very large luminocities are required from the engine initially, they decrease rapidly as ejecta expands. Thus, at late times it is possible that ejecta would be re-ionized, especially in case of a long-lived remnant \cite{Metzger and Piro 2014)}. The re-ionisation can reduce the optical opacity, reducing the prominence of FIR peak, a generally regarded distingushed feature of a Kilonova.
%% X-ray,. Gamma-rays, thermalization
At even higher energies, hard X-ray photons, an imporatnt source of opacity is the electron scattering with Klein-Nishina corrections. Importantly, that if the wavelendgth of photones become smaller then the salae of an atom, the contribution from both, free and bound into nuclei electrons ought to be considered. At high energies, the scattering of photons is inelastic. However, these processes are important as ejecta opacity to very high energy photons, gamma rays with energy in order of MeVs, determins the thermalisation of the $r$-process decay products.
%% pair-creation
For an extremely high photon energies $h\nu \gg m_e c^2$a pair creation becomes important. In particular this is so if a remnant is magnetar with large spin-down luminosity. Then, the at peak of the Kilonova emission, the pair creation might prevent pair-creation photons from escaping the kilonova. 

%%

\subsection{Unified Toy Model}

The way to model electormagnetic emission from the ejecta is to perform multi-dimensional, multi-group radiative transfer simulation coupled to hydrodynamica (or MHD) simulation of the ejecta itself.

Here a simplified model is considered or a transient, powered by the radioactive decay within the ejecta only. Several other assumptions are made.
In particular, the ejecta expansion is homologous (faster matter ahead of slow one) \cite{(Rosswog et al 2014)}
The mass-velocity distribution is $M_{\upsilon} = M(\upsilon / \upsilon_0)^{-\beta}$, for $\upsilon \geq \upsilon_0$,
where $M$ is the total mass, $\upsilon_0$ is the average, minimum velocity. $\beta$ can be assumed to te $3$, \cite{(Bauswein et al 2013a)}. However see \cite{Piran et al 2013} for a more complex velocity profiles.

The diffusion timescale defines when the radiation escapes the ejecta. For a layer with $\upsilon$ and $M_{\upsilon}$ and opacity $\kappa_{\upsilon}$ it is 

\begin{equation}
    t_{d,\upsilon} \approx \frac{3}{4\pi}\frac{M_{\upsilon}\kappa_{\upsilon}}{\beta Rc} = \frac{1}{4\pi}\frac{M_{\upsilon}^{4/3}\kappa_{\upsilon}}{M^{1/3}\upsilon_0 t c}
\end{equation}

where $\beta=3$ was assumed. 

This equation implies that at time $t=t_{d,\upsilon}$ the radiation from the layer $M_{\upsilon}$ peaks.

The $M_{\upsilon}(t)$ is related to the total mass of the ejecta and peak time (when radiation diffuses from the entire ejecta)

\begin{equation}
    M_{\upsilon}(t) = 
    \begin{cases}
        M(t/t_{peak})^{3/2},& t<t_{peak}, \\
        M, &t>t_{peak}
    \end{cases}
\end{equation}

where $t_{peak} = (3M\kappa / (4\pi \beta \upsilon c))^{1/2}$ with $\upsilon = \upsilon_0$. \red{Did not understand}

Outer layers with $M_{\upsilon} < M$ peaks first, while the deepest layers peak later but set the luminocity of the whole ejecta (assuming that the the opacity is constant in the ejecta. 

The radial evolution of each layer $M_{\upsilon}$ of mass $dM_{\upsilon}$ is given by 

\begin{equation}
    \frac{dR}{dt} = \upsilon,
\end{equation}

and the layer's thermal energy changes according to 

\begin{equation}
    \label{eq:theory:mkn:energ}
    \frac{dE_{\upsilon}}{dt} = \underbracket{-\frac{E_{\upsilon}}{R_{\upsilon}} \frac{dR_{\upsilon}}{dt}}_{PdV\text{ losses}} - \underbracket{L_{\upsilon}}_{\text{rad. los.}} + \underbrace{\dot{Q}}_{\text{heating sources}},
\end{equation}

where the radiative losses take form

\begin{equation}
    L_{\upsilon} = \frac{E_{\upsilon}}{t_{d,\upsilon} + t_{lc,\upsilon}},
\end{equation}

in which the $t_{lc,\upsilon} = R_{\upsilon}/c$ limits the energy loss to the light crossing time (important for when the layer is optically thin) \red{did not understand}.

The heating sources $\dot{Q}$ include

\begin{equation}
    \dot{Q}(t) = \underbrace{\dot{Q}_{r,\upsilon}}_{\text{radioactivity}} + \underbrace{\dot{Q}_{mag}}_{\text{magnetar}} + \underbrace{\dot{Q}_{fb}}_{\text{fall-bak accretion}}
\end{equation}

Next, even though in principle the effect of radiation pressure on ejecta ought be considered, in case where radioactive heating, total energy input $\int \dot{Q}_{r,\upsilon}dt < E_{kin,0}$ of the ejecta, \cite{(Metzger et al 2011; Rosswog et al 2013)}, this effect can be neglected.
Meanwhile, central engine might provide enough energy to modify the free expansion model. Then the equation for the central shell velocity evolution reads 

\begin{equation}
    \label{eq:theory:mkn:velcenteng}
    \frac{d}{dt}\Bigg(\frac{M\upsilon_0^2}{2}\Bigg) = M\upsilon_0\frac{d\upsilon_0}{dt} = \frac{E_{\upsilon_0}}{R_0}\frac{dR_0}{dt}
\end{equation}

Here, the term with $E_{\upsilon_0}$ balances the PdV \textit{loss} term in the thermal energy eequation (for $dE_{\upsilon}/dt$)

To compute the mitted radtion, first assume the black-body emission, the thermal emission, wit heffective temperature 

\begin{equation}
    T_{eff} = \Bigg(\frac{L_{tot}}{4 \pi \sigma R_{ph}^2}\Bigg)^{1/4}
\end{equation}

where $L_{tot} = \sum(L_{\upsilon dm_{\upsilon}})$. is the total luminocity (cumuklative for all mass shells). 
At the point where optical depth $\sum(\kappa_{\upsilon}dm_{\upsilon})=1$ the photosphere is located wit hradius $R_{ph}(t)$. 
The flux density of the source at photon frequency $\nu$ is given by 

\begin{equation}
    F_{\nu}(t) = \frac{2\pi h \nu^3}{c^2} \frac{1}{\exp\Big(\frac{h\nu}{kT_{eff}}\Big) - 1} \frac{R_{ph}^2}{D^2}
\end{equation}

where, $D$ is the distnace to the source. (Cosmological effects are neglected here).

Additionally, the opacity $\kappa_{\upsilon}$ depends on the temperature of the layer $T_{\upsilon}$, that itself cna be comptued as 

\begin{equation}
    T_{\upsilon} = \Bigg(\frac{3E_{\upsilon}}{4\pi a R^{3}_{\upsilon}}\Bigg)^{1/4}
\end{equation}

assuming that the ejecta internal energy is dominated by the raditiona. 

Finally, in order to compute the electromagnetic emission from the ejecta, the equation Eq.~\eqref{eq:theory:mkn:energ} ought be solved for $E_{\upsilon}$ (and $L_{\upsilon}$) for every shell with $dM_{\upsilon}$ and $\upsilon>\upsilon_0$. 
The velocity distribution can be assumed fixed (\textit{e.g.,} $M_{\upsilon} = M(\upsilon/\upsilon_0)^{-\beta}$,  if only the internal heating are important. If however, the central engine energy input is important, the the velocity of the central layer evolves according to Eq.~\eqref{eq:theory:mkn:velcenteng}.
Initial kinetic energy of the ejecta is quicly removed by the adiabatic expansion and the thermal energy of the ejecta, when its emission peaks, is domunated by the heating.

%%

\subsection{$r$-process heating}

The rate of a such heating within the layer $dM_{\upsilon}$, that has a fraction $X{r,\upsilon}$ of $r$-process elements, specific heating for which is $\dot{e}_r(t)$ writes

\begin{equation}
    \dot{Q}_{r,\upsilon} = dM_{\upsilon}X_{r,\upsilon}\dot{e}_{r}(t).
\end{equation}

The heating occurs through a combination of $\beta$-decays, $\alpha$-decays and fission \cite{(Metzger et al 2010b; Barnes et al 2016; Hotokezaka et al 2016)}. These decay products them thermalize with an effciency $\varepsilon_{th,\upsilon}$, depending on interactions between them and thermal plasma.
Thus, neutrinos can freely escape the ejecta. Very high energy photos, gamma rays, are also free after about $\sim 1$~day as the Klein-Nishina opacity decreases \cite{(Fig. 4; Hotokezaka et al 2016; Barnes et al 2016)}.
The $\alpha$ and $\beta$ particles however interact efficiently with the matter via ionisation \cite{(Barnes et al 2016)} and Coulumb scattering \cite{(Metzger et al 2010b)}.
\red{Here it repeats the Barns et al findings on thermalization efficiency}
For a fixed energy $\alpha$-particles thermalzie more efficiently, then $\beta$-particles.
For charged particles, the process depends on the magnetic field strnegth and configuration \cite{Barnes et al (2016)}
Additionally, if actinides are produced in $r$-process, their decay, involving $\alpha$-parotciles allows for high thermalization efficiency. \red{double check with before, Barnes}. Thus, nuclear phsyics input, that determins the amount of actinides, have an effect on the thermalization efficinecy of $r$-process elements in the ejecta.


For a neutron-rich ejecta, $Y_e\leq0.2$, the heating rate is dominated by a large statistical ensemble of nuclei, and the following can be assumed \cite{Korobkin et al 2012},

\begin{equation}
    \dot{e}_r = 4\times 10^{18} \varepsilon_{th,\upsilon}(0.5 - \pi^{-1} \arctan[(t-t_0)/\sigma])^{1.3} \text{ erg } \text{s}^{-1} \text{g}^{-1}
\end{equation}

Here $t_0=1.3$~s, $\sigma=0.11$~s constants. The $\varepsilon_{th,m}$ is the thermalisation efficiency.

The heating rate prescibed by this equation has first a constant segment, $\propto1$~s, (depletion of free neutrons by $r$-process) and a decrease segment $\propto t^{-1.3}$, (when heavy nuclei decay to stability) \cite{(Metzger et al 2010b; Roberts et al 2011)}. 
Notably, at higher $Y_e$, the nuclear heating is doinated by specific nuclei and has a complex form.

It was shown however, that on a timescale relevant for Kilonovae, and $Y_e$ present in BNS ($Y_e\leq0.4$), the heating rate can be assumed constatnt within the accuracy of a few. \cite{(Lippuner and Roberts 2015, their Fig. 7)}.

The dependency of $\dot{Q}_{r.\upsilon}$ on the nuclear physics models was shown to be week, unlike the $r$-process abundances themselves \cite{Eichler et al 2015; Wu et al 2016; Mumpower et al 2016).}

Regarding the thermalization efficiency of the energy released, \cite{Barnes et al (2016)} provides a recepi that sets the $\varepsilon_{th,\upsilon}$ decreasing from $\sim0.5$ at around $1$~day to $\sim0.1$ at around $1$~week. 

\begin{equation}
    \varepsilon_{th,\upsilon}(t) = 0.36 \Bigg[ \exp(-a_{\upsilon}, t_{day}) + \frac{\ln(1+2b_{\upsilon} t_{day}^{d_{\upsilon}})}{2b_{\upsilon}t_{day}^{d_{\upsilon}}} \Bigg]
\end{equation}

where $t_{day}$ is the time in days, ${a_{\upsilon}, b_{\upsilon}, d_{\upsilon}}$ are the constants that depend on the ejecta layer properties, mass and velocity. 


\subsubsection{Red Kilonova: Lanthanide-Bearing Ejecta}

Lathanides-rich ejecta is present in tidal outflow in BNS and NSBH mergers.

Notably, a toy model predicts a lightcurve from such low-$Y_e$ ejecta that peaks in NIR (in relatively good agreement with Rad.Transf.Model of \cite{Barnes et al (2016)}) on a timescale of several days (week)
The disagreemnt with radiation transfer models most noticable in the post-peak period, where toy model predicts sharp decay, while Rad.Trasf. predicts smooth decline. The reason for it is the toy model's assumption of optically thick blackbody emission. As ejecta expands cools and become optically thin, this assumption breaks down.
It is important to note, that the toy model does not take into account other emergent sources of opacity at late times, such as clumping, dust foramtion, photo-ionisation from central engine. These may smooth the post-peak lightcurves.

\subsubsection{Blue Kilonova: Lanthanide-Free Ejecta}

High $Y_e$ ejecta ($Y_e > 0.3$) was shown to be present in BNS \textit{e.g.,} \cite{Wanajo et al 2014b; Goriely et al 2015)}. Such ejecta would lack elements of lanthinides group \cite{(Metzger and Fernandez 2014)} and thus have a different EM signature.
The emission from such ejecta peaks in optical/R,I bandson a time scale of days. It is 2-3 magnitudes brighter then 'Red Kilonova'. 
This emission is assumed to be of polar origin and contribute to the total electromagntic signature of the BNS ejecta. 

\subsubsection{Free Neutron Precursor}

The sufficiently high density and low veloicyt of the bulk of the ejecta assures that there is enough time for the $r$
-process to remove free neutrons. However, a small fraction of the ejecta was shown to have hight enough velocity to retain its free neutrins and escape the dens slow part, \textit{e.g.,} \cite{(Bauswein et al 2013a)}. The origin of this component is the shock-heated intefrace between two neutron stars as they collide. 
The outer layers of the ejecta then can be \red{superheated} by this \red{'neutron skin'}, modifying the Kiloniva signal. \cite{(Metzger et al 2015a; Lippuner and Roberts 2015)}

Here we consider such ejecta, that contains free neutrons. 
Consider layer $dM_{\upsilon}$, that contain a fraction $X_{n;\upsilon}$ of free neutons, specific heating rate of which $\dot{e}_n(t)$. Then, the heating rate in the layer reads 

\begin{equation}
    \dot{Q}_{r;\upsilon} = dM_{\upsilon} X_{n,\upsilon}\dot{e}_n(t).
\end{equation}

The initial mass fraction of neutrons $X_{n,\upsilon}$ is defined as 

\begin{equation}
    X_{n,\upsilon} = \frac{2}{\pi}(1 - Y_e)\arctan\Big(\frac{M_{n}}{M_{\upsilon}}\Big),
\end{equation}

is an arbitrary assumed interpolation between the neutron rich ($M\ll M_{n}$) inner layers with $X_{n} = 1-2Y_e$ and neutron-free ($M\gg M_n$) layers.

Assuming the averabe neutron half-0ife of $900$~s, the specific heating rate $\dot{e}_{n}$ is 

\begin{equation}
    \dot{e}_n = 3.2 \times 10^{14} \exp[-t/\tau_n] \text{ erg } \text{s}^{-1} \text{g}^{-1},
\end{equation}

Simultaneously, as fraction of free neutrons increases in outermost layrs, the fraction of $r$-process elements decreases as $X_{r,\upsilon} = 1 - X_{n.\upsilon}$, which has to be accounted for in Eq.~\eqref{eq:theory:mkn:energ}.

The effect of free neutrons on Kilonova lightcurves is the following.
Even for a very small mass, $\sim 10^{-4}M_{\odot}$ of freen-nutron ejecta, with $Y_e\sim0.1$, the UVR luminocities are increased considerably, in the first hours after merger.
The reason for that is, $\dot{e}_{n} > \dot{e}_r$ by at loeast an order of magnitude on a tiemscales up to 1 hour postmerger. Also, this timescale is comaprable with the diffusion time scale for the neutron mass layer. 

\begin{equation}
    t_{peak,\upsilon} \approx \Bigg(\frac{M_{\upsilon}^{4/3}\kappa_{\upsilon}}{4\pi M^{1/3}\upsilon_0 c}\Bigg)^{1/2} \sim 3.7 \text{ hours}.
\end{equation}

And 

\begin{equation}
    L_{peak} \approx \frac{E_n \tau_n}{t_{peak;\upsilon}^2} \propto 3\times10^{41}
\end{equation}

which is insensitive to the mass of the layer itself. 
This emission is expected to peak in optical/UV band due to the high ejecta temperature during the first hours after merger. 

%%

\subsection{Engine Power}

Additional heating for kilonova might come from the object, left after the merger. In case of BNS it might be the MNS. In case of NSBH it is a BH with accretion disk. This is expected to make Kilonova more luminous than what $r$-porcess products decay might produce.

A large fraction of Short GRB, $\sim(15-25)\%$ is followed by the prolonged ($10-100$~s) hump of $X$-ray emission, \cite{Norris and Bonnell 2006; Perley et al 2009 Kagawa et al 2015}

It is however uncertain, how much energy does the central engine provides. Here some examples are considered.

\subsubsection{Fall-Back Accretion}

A merger leaves a finite amount of mass bound gravitationally to the central object, that falls back of a time timescale of seconds to days \cite{(Rosswog 2007; Rossi and Begelman 2009; Chawla et al 2010; Kyutoku et al 2015)}.

\gray{This is a part of the outflow that was not energetic enough to leave the system. It eventually falls back on the central object. This is not the disk itself...}

The rate of fall-back at late timnes $t\gg 1$~s can be approximated by a power law 

\begin{equation}
    \dot{M}_{fb} \approx \Bigg( \frac{\dot{M}_{fb}(t=0.1~\text{s})}{10^{-3}M_{\odot}\text{s}^{-1}} \Bigg) \Bigg( \frac{t}{0.1 \text{s}} \Bigg)^{-5/3}
\end{equation}

The value $10^{-3}M_{\odot}\text{s}^{-1}$ is the normalization chosen for BNS. For NSBH it be different by an order of magnitude \cite{(Rosswog 2007)}. 

Notably, the fall-back of the material removed on a dynamical timescales, can be stalled by the continous winds from the disk \cite{(Fernandez et al 2015b)}

Additionally an onset of $r$-process heating in the disk might provide an additional source of outflow that would stall the fall-back on a seconds to minutes timescale \cite{Metzger et al 2010a)}.

On a longer timescale, days to weeks, there seems to be no mechansm that can suppress the fall back completely. This it might still be a relevant source of energy for Kilonva.

The matter that reaches the central objects accrets. This is super-Eddington accetion that releases energy, $L_{acc} \propto \dot{M}_{fb} c^2$ that can heat the ejecta and enhance  the Kilonova emission. Additionally, the accretion might result in the formation of a relativistc jet (similar to GRB) that might account for the extended $X$-ray emission that sometiems follow the GRB.
As accretion flow subsides, the jet power decreases and it becomes unstable to the magnetic Kink instability \cite{(Bromberg and Tchekhovskoy 2016)}. Then the energy is dissipated pramarely via heating up the ejecta, by magnetic reconnections instead of non-thermal emission. 

The fall-back acctretion can power a mildly relativist, wide-angle disk wind. As the wind collides with the (ejected prior) ejecta shells, its energy thermalizes. 

Overall, the ejecta heating rate due to fall-back accretion can be described as 

\begin{equation}
    \dot{Q}_{fb} = \varepsilon_{j}\dot{M}_{fb}c^2
\end{equation}

where $\varepsilon_{j}$ is the jet/disk wind efficiency factor. See \cite{Tchekhovskoy et al 2011). Kisaka and Ioka (2015)} for the discussion of efficiency.

For instace the 130603B, was detected with an NIR excess. It was initially attributed t othe radiactive heating \cite{Tanvir et al (2013)} \cite{Berger et al (2013)}. On the contrary, \cite{Kisaka et al (2016)} suggested that it might be attributed to the absorbed and re-emitted (reprocessed) $X$-ray emission. 

%%

\subsubsection{Magnetar Remantns}

The outcome of the NSBH merger is always a black hole. Meanwhile an outcome of the BNS merger depends sensetively on the maximum allowed mass for a non-rotating NS ($M_{max}(\Omega=0)$). 
This value is bounded, \textit{e.g.,} $\geq 2M_{\odot}$ \cite{(Demorest et al 2010; Antoniadis et al 2013)} and $< 3M_{\odot}$, where the upper limit is given by the casuality constrants on the EOS. 
Withing this boundaries the fate of the remnant is uncertain. Incidently, the observations shows that NS has mass $\sim 1.4M_{\odot}$. Merger of two of this objects thus result in a remnant of mass $\sim2.5M_{\odot}$ ($\approx7.5\%$ of the mass was lost to GW and neutrinos \cite{(Timmes et al 1996)}). If the resulting mass is lower then $M_{max}(\Omega=0)$, it promptly collapses. Otherwise a stable (short- or long-lived) remnant can be formed.

Consider a rotating remnant. An upper limit on a rotating object, is the object that is rotating close to the mass-shedding limit. 

Given the remnant's moment of inertia $I$ and \red{angular velocity} $\Omega$, then rotational period $P = 2\pi / \Omega$. 

Such object has energy 

\begin{equation}
    E_{rot} = \frac{1}{2}I\Omega^2 \approx 10^{53} \Big(\frac{I}{I_{LS}}\Big)\Big(\frac{M_{ns}}{2.3M_{\odot}}\Big)^{3/2}\Big(\frac{P}{0.7\text{ms}}\Big)^{-2} \text{ ergs }
\end{equation}

Here the remnatn's moment of inertial, $I$ is normmalized to $I_{LS} \approx 1.3\times 10^{45}(M_{NS}/1.4M_{\odot})^{3/2}$ g cm$^{2}$ (motivated by Fig.1 \cite{Lattimer and Schutz (2005}).

This energy exceeds by a factor of $10^3$ the ejecta kinetic energy of radioactive decay energy.
If this energy can be extracted via channels other then GW (\textit{e.g.,} EM torques), then the EM signal accompanying the merger would be significantly enhanced, \cite{(Gao et al 2013; Metzger and Piro 2014; Gao et al 2015; Siegel and Ciolfi 2016a)}. For a remnant that is supported by the differential roatation, only a part of the ritational energy is availalbe (as the rmenant would eventually collapse loosing it). The Fig.8 shows the dependency of the \textit{extractable} rotational energy as a function of the remnants mass.

The electromagnetic tourques allows to extract the totatinal energy from a remnant with strong magnetic fields. Such fies are expected for the merging NS, due to amplifications, reaching values found in galactic magnetars \cite{(Price and Rosswog 2006; Zrake and Mac-Fadyen 2013; Kiuchi et al 2014)}. However, this amplification occures at small scats and at early times post merger, producing a complex field topology, that evolves with time \cite{(Siegel et al 2014)}. The magnetic field strength at the end of the differential rotation phase is however uncertain. There are speculations that it might remain at $10^{15-16}$~G.

Consider an aligned dipole rotator (different from a vacuum the vacuum dipole). Its spin-down luminocity is \cite{Spitkovsky (2006); Philippov et al (2015)} 

\begin{equation}
    L_{sd} = 
    \begin{cases}
        \frac{\mu^2 \Omega^4}{c^3} = \frac{(B R_{ns}^3)^2 \Omega^4}{c^3} &\text{ if } t< t_{coll} \\
        0 &\text{ if } t> t_{coll}
    \end{cases}
\end{equation}

The charactersitic 'spin-down timescale' over which an order of unity fraction of the rotational energy is removed is 

\begin{equation}
    t_{sd} = \frac{E_{rot}}{L_{sd}}\Bigg|_{t=0}
\end{equation}

which is of an order of $\sim 150$~ms for a remnant of the mass $M=2.3M_{\odot}$, $I=I_{LS}$, $B=10^{15}$~G and $P_0=0.7$~ms, 

where $P_0$ is the initial spin-period.
The mass-shedding limit of this remnant is $P=0.7$~ms. 

The lifetime of the unstable remanant can be estimated as 

\begin{equation}
    L_{extract} = \int_0^{t_{coll}} L_{sd} dt
\end{equation}

where $t_{coll}$ is the time of the collpase, that marks olse the end of the extraction of the rotational energy. $L_{extract}$ is the total amount of energy extracted from rotation.
The $t_{coll}$ falls rapidly with the remnant mass, after it passes the stale NS upper limit.

Long-lived magnetar can power the 'prompt-like' X-ray emission (found in sGRB \textit{e.g.,} \cite{(Gao and Fan 2006)Metzger et al (2008b); Bucciantini et al (2012)} ). Additionally, the sGRB with extended emission were explaiend by phenomenological models of magnetar spind-down \cite{(Gompertz et al 2013)}. The observed X-ray and optical plateus were discusssed in \cite{(Rowlinson et al 2010, 2013; Gompertz et al 2015)} and the late-time excess emission was adressed in \cite{(Fan et al 2013; Fong et al 2014a)}. Notably, all models requrie rather large magnetic fields of $\sim 10^{16}$~G.

The formation of the jet and sGRB is subjected to uncertainties. 
It was argues that magnetar model is not viable due to heavy baryonic pollution in the polar region above the surface \cite{(Murguia-Berthier et al 2014, 2016).}. This led to the develpment of the model, in which GRB is generated after the remnant collapse to a BH, which might happen minutes after the merger. And while this still allow to explain the extended X-ray emission (magnetar spin-down and radiation diffussion through the ejecta). 
However, if in spin-down the remnant raches a solid-body rotation, a collapse of such a remnatn is not predicted to leave a massive disk, sufficient to power the GRB \cite{Margalit et al (2015)}. The disk that was formed after the merger is expected to be either accreted or spread out (via ) too much for short GRB to be generated. 

There are observational evidences that BH is not mandatory for producing a jet. For instance, the (e.g., Circinus X-1; \cite{Fender et al 2004}), galactic acretring NS.
While indeed the region above the neutron star is polluted by neutrino-driven wind on a time scale of seconds postmerger \cite{(Dessart et al 2009; Murguia-Berthier et al 2014, 2016)}, the expected strong magnetic field $B\gg 10^{15}$~G, small scale magnetic flux bundles (that dominate dynamically over the thermal or ram pressure of the wind) could confine the plasma \cite{(Thompson 2003)}. Then, originating from the disk open field lines, carrying the Poyntim flux of the GRB jet, would be relatively free of baryonic matter due to centrifugal barrier. 
Note, that sheer rotation of the NS would result in periodicity in openning of the polar field lines. This, in turn, might lead to a variability in the transinet (without requiring baryon pollution at all). 
The presence of the NS \textit{after} the GRB is suported by observations: extended X-ray emission that does not follow the model of the fall-back accreting BH. 
\gray{the early X-ray varaiablility is sometimes attributed to the afterglow phase, \cite{(Holcomb et al 2014)}, but it is too rapid for a foward or reverse shocks}

Magnetic spind-down power, injected into the merger ejecta (behind it) could enhance the Kilonova emission (\cite{Yu et al (2013)}). Similar mechanism was considered for the SLSN \cite{(Kasen and Bildsten 2010; Woosley 2010; Metzger et al 2014)}. This in essence, reminds one of a fall-black powered emission considered before.

A pulsar injects a relativistc wind of $e^{\pm}$ pairs into the surrounding environment (\textit{e.g.,} Crab Nebula). Near the termination shock, wind undegoes the shock dissipation, forming the so-called 'magnetar wind nebulae' of relativistic particles \cite{(Kennel and Coroniti 1984)}. The high density of the BNS merger environment assures a rapid cooling of these pairs (via synchrotron or inverse-compton emission) \cite{(Metzger et al 2014; Siegel and Ciolfi2016a,b)} generating the broadband emission (akin the emission from pulsar wind nebulae \textit{e.g.,} \cite{Gaensler and Slane 2006)}). The inner walls of the expanding ejecta would absorb, UV and X-ray photons, reprocess and emit in optical/IR \cite{(Metzger et al 2014)} contributing and enahncing Kilonova.

Notably, the magnetar wind-nebulae emission does not necessarly undergoes thermalization within the ejecta. If the spectral windows allow, \textit{e.g.,} for instance for hard X-ray
\footnote{where the bound-free transitions lie at lower energies. Additionally this is possible for hight enerngy $\gg$ MeV photons, that fall into the gap between declining Klein-Nishina cross-section and before the rise of $\gamma-\gamma$ opacities}
, the emission will escape the ejecta without being reprocessed.
Additionally, low-mass ejecta can undergo complete ionsiation and allow even lowere energies photones to pass without thermalization. This 'leaking radiation' might be an important EM signal to mergers \cite{(Metzger and Piro 2014; Siegel and Ciolfi 2016a,b; Wang et al 2016).}. 

consider the ejecta heating rate provided by the magnetar spin-down as 

\begin{equation}
    \dot{Q}_{sd} = \varepsilon_{th}L_{sd}
\end{equation}

where $\varepsilon_{th}$ is the thermalization efficiency, that ranges between $1$ when the ejecta is very opaque (hearly times) to a low value, for low opacities.

Notably, there is another sink for spin-down radiation that is of it utmost importance at early times, where high energy $\gamma$-rays are present in the nebula behind the ejecta \cite{Metzger and Piro (2014)}. These $\gamma$-rays create $e^{\pm}$ pairs (when compactness of the cloud is high). Coming in 'seed photons' can then be compton up-scattered on these particles, becoming energetic enought to produce a new $e^{\pm}$ pair. This initializes a 'pair cascade'. High fraction ($\leq 0.1$) of puslar spin-down power $L_{sd}$ falls into the rest pass of the $e^{\pm}$ pairs \cite{(Svensson 1987; Lightman et al 1987)}. Hence, for the spind-down radaition to reach (and theramlize within) the ejecta, it must diffuse through the 'pair cloud', experiencing $PdV$ adiabatic losses.
To paramterize the effect, introduce the Thompson optical depth of the pair cloud $\tau_{es}^n$. If This optical depth exceeds the optical depth of the ejecta itslef, then only a fraction of the actual magnetar spin-down power can be thermalzied within the ejecta. 
This effect of 'pair cloud' can be approximated by suppressing the observed luminosity. Floowing \cite{Metzger and Piro (2014) and Kasen et al (2015),} 

\begin{equation}
    L_{obs} = \frac{L}{1 + (t_{life}/t)}
\end{equation}

where $L$ is the Kilonova luminocity, computed from the energy equation \eqref{eq:theory:mkn:energ} (with magnetar heat source).
The $t_{life}/t$ is the caracteristic 'lifetime' of a non-thermal photon in the nebula, relative to the ejecta expansion timescale, written as

\begin{equation}
    \frac{t_{life}}{t} = \frac{\tau_{es}^{n} \upsilon}{c(1 - A)}
\end{equation}

where $\tau_{es}^n\propto Y L_{sd}$ and $A$ is the frequency averaged albedo of the ejecta ($A\propto 0.5$).

Overall, the pair trapping is able to reduce the effective luminocity of the magnetar powered kilonova by several orders of magnitude (due to reduce thermalization efficenty) at early times.

Energy input from the magnetar spind down, can in itself raise the observed peak luminocities. Note however, that in case of the only temporarly stable remnant, the energy import would be terminated at collapse.

%%

\subsection{Implications}

sGRB is a good smoking gun for Kilonova searches.
However, it, and its afterglow should not outshine the Kilonova. For instance, in GRB 130603B \cite{(Berger et al 2013; Tanvir et al 2013)} the observed NIR excess would require ejecta of $0.05-0.1M_{\odot}$ to be explaiend. This is generally too high for dynamical ejecta only \cite{(Hotokezaka et al 2013b; Tanaka et al 2014; Kawaguchi et al 2016).}, but might be achieved with winds from the disk and remnant \cite{(Metzger and Fernandez 2014)} see also \cite{Kasen et al 2015)}. However, high observed high luminocity might not be a result of radioactive heating alone, but hits towards the contribution from the central engine, fall-back accretion or spin-down luminocity.

Discussion on how different properties of the Kilonova affect detection possibilities and different biasas might araise.

\red{This might serve as a gread introduction to the thesis!}

%% --------------- 
%%
%% <<< GRB >>>
%%
%% ---------------

\chapter{Non-thermal emission from compact object merger}

\section{The Physics of Gamma-Ray Bursts \& Relativistic Jets}
\red{based on 250pp paper by Pawan Kumur and Bing Zhang | 2014}
\red{SUPPLIMENT it with more recent studies on sGRB}
\red{RECYCLE MORE}

Gamma-Ray Bursts (GRB) are irregular pulses of gamma-ray radiation with broken power-law (non-thermal) spectrum, peaking at KeV-MeV \cite{Band et al., 1993; Kouveliotou et al., 1993; Meegan et al., 1992}.

With respect to the duration, GRB are split into two categories: short GRB (sGRB), that last $< \sim 2$~s and long GRB that last $> \sim 2$~s. The latter are the result of the collapse of massive $\geq 15M_{\odot}$ stars, while the former, at least in part, is attributed to mergers of compact objects. Only very recently it was directly confirmed \cite{Abott+2017}. However, the exact physical origin of different duration GRB is not fully understood.
Indications that long GRB are associated with core-collapse supernovae, SNe, are two fold. These GRBs are typically observed in star-forming regions of their host galaxies \cite{(e.g. Bloom et al., 2002b; Fruchter et al., 2006; Christensen et al., 2004; Castro Ceron et al., 2006);} and several GRBs are spectroscopically associated with Type Ic SNe, albeit these GRBs were significantly less bright and might not be typical GRBs \textit{e.g.,} \cite{Liang et al., 2007a; Bromberg et al., 2011a}. Additionally, the late time behaviour of some GRBs includes a SNe-like "bump" in the optical and spectral changes that might imply that underlying SNe flux becomes dominant over GRBs \cite{(Bloom et al., 1999; Woosley and Bloom, 2006)}.

The GRBs are distant events, most of which were localized to outside the local group \cite{Mao and Paczynski, 1992; Piran, 1992; Fenimore et al., 1993}. Particularly useful for distance estimation were the observations of GRB afterglow, fading X-ray \& optical emission, that allow to estimate the redshift \cite{Costa et al., 1997; Frontera et al., 1998; van Paradijs et al., 1997}.

The luminocity of a GRB is such that it would make it the most energetic explosions in the universe if it was isotropic \cite{Kulkarni et al., 1999a}. However, observations of GRB afterglow (breaks in opcital \& X-ray) indicated that GRBs are highly beamed \cite{Rhoads, 1999; Sari et al., 1999}, which reduces the explosion energy to $10^{48}-10^{52}$~ergs \cite{Frail et al., 2001; Panaitescu and Kumar, 2001; Berger et al., 2003a; Curran et al., 2008; Liang et al., 2008a; Racusin et al., 2009; Cenko et al., 2010}.
Modeling of the broadband emission (radio to X-ray) showed that the GRB jet have narrow opening angles of $~\sim 2-10$~deg \cite{Rhoads, 1999; Sari et al., 1999; Frail et al., 2001; Panaitescu and Kumar, 2001; Berger et al., 2003a; Cenko et al., 2010}

Analysis of the multi-wavelength afterglow data for GRBs (e.g.\cite{Panaitescu and Kumar, 2002)} suggested the mechanism behind the afterglow emission is the synchrotron radiation from the forward,  external forward-shock, which forms when GRB-ejecta sweeps-up the circomburst medium
\footnote{The specific indications are the power law decay of the light curves, $F_{\nu}\propto^{-1}$ and power-law spectrum $F_{\nu}\propto\nu^{-0.9\pm 0.5}$.} 
\cite{(Rees and Meszaros, 1992; Paczynski and Rhoads, 1993; Meszaros and Rees, 1993, 1997a)}

The temporal behavior of many (but not all) GRBs shows a change, a steepening of the light-curve (to $F_{\nu}\propto t^{-2.2}$) at $\sim 1$~day after the burst. This is usually attributed to the 
%\gray{deceleration of the colimated GRB-outflow, jet, and decrease on the realtivisitc beaming. This in turn makes the edge of the jet visible to an observer.} 
finite angular extend of the GRB-ejecta, jet \cite{Rhoads, 1999, Sari et al., 1999}. When jet decelerates and relativistic beaming decreases (and the jet edge becomes visible), the optical and X-ray lightcurves decay achromatically faster. This achromatic transition from slow to faster decay is called "jet-break".

%% PROBLEMO -- jet-break is not a universal feature.
Notably, this jet-break is not observed in all GRBs which presents a question why
\cite{(Fan and Piran, 2006a; Panaitescu et al., 2006a; Liang et al., 2007b; Sato et al., 2007; Liang et al., 2008a; Curran et al., 2008; Racusin et al., 2009)}.

%% PROBLEMO -- GRB density seems uniform, but SSE models predict wind-like profile
Models of the broadband emission of GRBs with jet-break showed that the circomburst medium, CBM, is uniform with number density \red{$\sim 10^{-3}$} \cite{(Panaitescu and Kumar, 2002)}. If GRBs produced in collapse of massive stars \cite{Woosley (1993); Paczynski (1998)}, this contradicts the expected density profile from stellar winds, \textit{e.g.,} $\rho\propto r^{-2}$ \cite{Dai and Lu, 1998b; Chevalier and Li, 1999, 2000; Ramirez-Ruiz et al., 2001)} \red{this might be very outdated.}

%% sGRB
Regarding the short duration GRB. Their origin was first connected with the elliptical galaxes with older stellar population \cite{Gehrels et al., 2005; Fox et al., 2005; Barthelmy et al., 2005c; Berger et al., 2005; Panaitescu, 2006; Bloom et al., 2006; Guetta and Piran, 2006; Nakar, 2007} and thus with merger of neutrons stars. A more direct evidence came with the detection of GRB170817A \cite{Abbott et al 2017}, a sGRB that accombaned the GW detection and Kilonova.

Continuous observations of sGRB showed a complex time behavior of early afterglow X-ray emission, in particular a presence of a plateau ($F_{x}\propto t^{-1/2}$), after the initial sharp decrease ($F_{x}\propto t^{-3}$) which a standard forward shock model does not predict. This implied that early X-ray afterglow is shaped by a variety of physical processes \cite{Zhang et al., 2006}.
%% sGRB PROBLEMI -- prompt emission and early X-ray plateu, does the central engine matters after the burst?
Two main questions that stem from these observations: is the mechanism behind the prompt $\gamma$-ray emission and early afterglow emission is the same (or do they originate from the same outflow), and is the early X-ray radiation produced by the external shock (just a blast wave takes long time to become self-similar) or does it originate from an internal shock?
An indication that the long-lived central engine activity might affect the afterglow came from the observed sharp increase in X-ray flux (flares) omn a scale of minutes to hours after the end of the GRB \cite{(Burrows et al., 2005b; Chincarini et al., 2007, 2010; Margutti et al., 2011)}, which could not be attributed to the inhomogeneities in the CBM.
%% PROBLEMO!
Thus, the early X-ray behaviour of GRBs $t < 10^{4}$~s post-burst is not well understood and seems to be in tension with standard afterglow forward shock emission model.

%% PROBLEMI
\textcolor{red}{
    One of the foremost unanswered questions about GRBs is the physical mechanism
    by which prompt $\gamma$-rays the radiation that triggers detectors on board
    GRB satellites are produced. Is the mechanism the popular internal shock
    model 6 \cite{(Rees and Meszaros, 1994)}, the external shock model, or something
    entirely different? Are $\gamma$-ray photons generated via the synchrotron process
    or inverse-Compton process, or by a different mechanism? Answers to these
    questions will help us address some of the most important unsolved problems
    in GRBs  how is the explosion powered in these bursts? Does the relativistic
    jet produced in these explosions consist of ordinary baryonic matter, electron positron
    pairs, or is the energy primarily in magnetic fields?
}

Once again, while it is suggested that the high energy emission, after the propmt phase is produced by the synchrotron process in the external forward shock, \cite{(Kumar and Barniol Duran, 2009; Ghisellini et al., 2010).}, the mechanism behind the high and low energy $\gamma$-ray emission in the prompt phase remains unknown. 
Possible mechanisms include: inverse Compton and synchrotron emission in internal and external shocks \cite{(e.g. Rees and Meszaros, 1992; Dermer and Mitman, 1999; Lyutikov and Blandford, 2003; Zhang and Yan, 2011);} and 
photospheric radiation with contribution from multiple IC scatterings \cite{(e.g. Thompson, 1994; Ghisellini and Celotti, 1999; Meszaros and Rees, 2000b; Pe'er et al., 2006b; Pe'er, 2008; Giannios and Spruit, 2007; Ioka et al., 2007; Asano and Terasawa, 2009; Lazzati and Begelman, 2010; Beloborodov, 2010; Toma et al., 2011b; Mizuta et al., 2011; Nagakura et al., 2011; Bromberg et al., 2011a).}

%%
%%
%%

\section{Physical processes}

This section aims to outline a brief overview of the most relevant physical processes in GRBs. Tt is not meant as a comprehensive theoretical review. For this we refer the interested reader to the momnograph by \cite{Rybicki and Lightman (1979)}, as well as books of high energy astrophsycs by \cite{Longair (2010); Krolik (1999); Dermer and Menon (2009); Kulsrud (2005)}.

In this section we focus first on certain aspects of theory of special relativity, and relativistic hydrodynamics \red{this is where you put the NAVA and Peer models for dynamcs, Sedov Teylor and stuff maybe} 
and on radiation processes, synchrotron, \red{inverse-Compton} and \red{photon-pion} processes.

\subsection{Photon arrival time from a moving source, Doppler shift, Lorentz invariance of power etc}

Consider a moving source of radiation and an observer with a line of sight to the source. Let $\upsilon$, $\Gamma$ and $\theta$ be the source velocity, lorentz factor and angle with the line of sight. \red{REPLACE $\theta$ with $\Phi$ for consistency! (remember spreading jet)}

Consider three frames of reference, the comoving frame (usually denotted with a prime $'$), the lab frame, where the source is seen as moving with $\upsilon$ and observer frame. Then, if two photons are emitted in the comoving frame with time difference of $\delta t'$, which is in the lab frame $\delta t = \Gamma \delta t'$, the observer sees the two photons arrive with 

\begin{eqnarray}
   \delta t_{obs} &= \delta t + \frac{(d - \upsilon\cos(\theta) \delta t)}{c} - \frac{d}{c} \\
   &= \delta t (1 - \upsilon \cos(\theta) / c) \\
   &= \delta t' \Gamma (1 - \upsilon \cos(\theta) / c)\\
   &= \delta t' \mathcal{D}^{-1}
\end{eqnarray}

where $d$ is the distance to the source, and 

\begin{equation}
    \mathcal{D} = \frac{1}{\Gamma(1 - (\upsilon/c) \cos(\theta))} = \frac{1}{\Gamma(1 - \beta\cos(\theta))}
\end{equation}

os the Doppler factor. 
\gray{Note that if $\theta \ll 1$ and $\Gamma \gg 1$, the $\delta t_{obs} \approx (\delta t' / \Gamma) (1 + \theta^2 \Gamma^2)/2 = (\delta/\Gamma^2)(1 + (\theta\Gamma)^2/2)$}.

\red{Here the fig. like Fig.1. from the SOURCE can be}

Next, consider the trasformation of the photon frequencies. Once again $\nu'$ denotes the frequency in the comoving frame and $\nu$ denotes the frequency in the observer frame. For that we employ the standard Lorentz transformation pf the photne $4$-momentum in comoving frame, \textit{e.g.,}, $\nu'(1, \cos(\theta'), \sin(\theta'),0)$ to the lab frame $4$-momentum $\nu(1, \cos(\theta), \sin(\theta), 0)$

\begin{equation}
    \nu = \nu' \Gamma(1+\upsilon \cos(\theta')/c) \text{ \& } \nu\cos(\theta) = \nu' \Gamma (\cos(\theta') + \upsilon/c)
\end{equation}

or 

\begin{equation}
    \nu = \frac{\nu'}{\Gamma (1 - \upsilon\cos(\theta)/c)} = \nu' / \mathcal{D}
\end{equation}

which is a standard Doppler shift formula.

%%
%%
%%

\subsubsection{Relativisitc beaming of photons}

We have shown that $\nu = \nu' \mathcal{D}$, but also $\sin(\theta) = \sin(\theta')/\mathcal{D}$. Then the transverse component of the momentum is invariant under the Lorentz transformation, \textit{e.g.,}, $\nu_{\perp}' = \nu'\sin(\theta') = \nu\sin(\theta) \nu_{\perp}$. 
For a beem of photons it imples that the angular size of the beem is smaller in the lab frame than in the comoving frame by $\propto \Gamma$.
The solid angle of a conical beem of photons, $d\Gamma$ then 

\begin{equation}
    d\Gamma = \sin(\theta)d\theta d\phi = \sin(\theta') d\theta' d\phi' / \mathcal{D}^2 = d\Omega'/\mathcal{D}^2
\end{equation}

is smaller in the lab frame than in the comoving frame.

Next, consider a frequency integrated total energy radiated per init time over the $4\pi$ steradians, denoted as $P$. \gray{Assume that the photon been is symmetric under the parity transformation in particle rest-frame (energy radiated per unit of the solid angle in $\theta\phi$ and $\pi-\theta,\pi+\phi$ are the same} \red{assume emission is locally isotropic.}.
The the pwer in the lab frame $P = P'\Gamma\delta t'/(\Gamma\delta t') = P'$. Hence, power radiated by particles is \magenta{Lorentz invariant}.

%%
%%
%%

\subsubsection{Transformation of specific luminosity and specific intensity}

Consider a spherically symmetric source, expanding with Lorentz factor $\Gamma$. 

Introduce the \magenta{specific luminocity}, defined as the total energy that passes through the surface enclosing the spurce per unit time, per unit frequency, $L_{\nu} = dE / d\nu dt_{obs}$. 
As $d\nu dt_{obs} = d\nu' dt'$ and $E=\Gamma E'$, the Lorentz transformation of luminocity is

\begin{equation}
    L_{\nu} = \frac{dE}{d\nu dt_{obs}} = \Gamma \frac{dE'}{d\nu' dt'} = \Gamma L_{\nu}'
\end{equation}

 assuming that the $3$-momentum is zezo (as the spurce is spherically symmetric).

Next, introduce the \magenta{specific intensity}, defined as a flux per unit frequency and per unit solid angle, mediated by photons, transversing surface $dA$, perpendicular to the conical beam, confining the photons, 

\begin{equation}
    I_{\nu} = \frac{dE}{d\nu dt_{obs} dA d\Omega}
\end{equation}

that has a Lorentz transfprmation $I_{\nu} = \mathcal{D}^3 I_{\nu'}'$ as $d\nu dt_{obs} dA$ is the Lorentz invariant.

%%
%%
%%

\subsubsection{Observed lightcurve from a source that is suddenly turned off}

\red{I did not quite understand this section at all...}
\red{Also here a figure is required, Fig.2}
\red{it mainly says that when the source turns off, because of its finite angular extend and EATS, the observed flux does not switches off, but rapidly declies.}


Considering the variability of EM transients such as GRB it is important to asses how a sudden turn off of the source affects the observed emission. 
Consider a relativist thin shell moving within a cone with lorentz factor $L$. As it reaches the radius $r=R_0$ the source turns off. 
\red{
A point on the thin shell is characterized by $(r,\theta,\phi)$ where $\theta$ is the angle measured with respect to the line of sight to the observer. Then, photons, emitted at $(r=\upsilon t, \theta,\phi)$ arrive at the observer with a time delay with respect to a photone emitted at $r=0$ of
}

\begin{equation}
    t_{obs} = t - \frac{r \cos(\theta)}{c} = t(1-\frac{\upsilon\cos(\theta)}{c}) = \frac{t}{\Gamma\mathcal{D}}
\end{equation}

Now, consider the observed emission from the source ar frequency $\nu$. The starting time is $t_{0;obs}\approx(R_02c\Gamma^2)$, at which photons, emitted from $(R_0,0,0)$ arrive, At later times, $t_{obs}>t_{0;obs}$, the observer still sees photons emitted when $r < R_0$. 
Assume that the intrinsic emission spectrum is $I_{\nu'}' = I'\nu^{'-\beta}$.
Then, at $t_{obs} > t_{0;obs}$ the radiation from $\theta > \theta_t$ (where $\theta_t$ corresponds to $t_{obs} = R_0(1/\upsilon - \cos(\theta_t)/c)$) reaches the observer.
The observer flux \textit{e.g.,} $f_{\nu} \propto \int I_{\nu} d\Omega$, has the following Lorentz transformation $f_{\nu}\propto\int_{\theta_t} d\theta \sin(\theta_t) \mathcal{D}^{-(3+\beta)}$.

Now, consider a more rigorous deriviation of the tranformation of the specific flux in observer frame fromrelativistic source with comoving specific intensity $I_{\nu'}'$ and spectrum $\propto \nu^{' -\beta}$

\begin{equation}
    f_{\nu}(t_{obs}) = \int d\Omega_{obs} I_{\nu} \cos(\theta_{obs}) = 2\pi \int d\theta_{obs} \frac{ I_{\nu'_0}' \nu_{0}^{'\beta}\sin(2\theta_{obs})[(1+z)\Gamma]^{-(3+\beta)} }{ 2\nu^{\beta} [ 1-\upsilon\cos(\theta + \theta_{obs}) / c ]^{3+\beta} }
\end{equation}

where $\nu_0 '$ is a frequency that lies on the powerlaw segment of the spectrum for $I_{\nu'}'$. The lorentz transformation of the specific intensity was made above. The factor $(1+z)^{3+\beta}$ accounts for the redshift on the frequency. 

Assuning that $\sin(\theta)/d_{A} = \sin(\theta_{obs})/r$, the above integral writes 

\begin{equation}
    f_{\nu} \approx \frac{ 2\pi I' \nu' _0 \nu_{0}^{'\beta}\nu^{-\beta} }{[(1+z)\Gamma]^{3+\beta}} \Big( \frac{R_0}{d_A} \Big)^2 \int_{\theta_t}^{\pi / 2} d\theta \frac{\sin(\theta)\cos(\theta)}{(1-\upsilon\cos(\theta)/c)^{3+\beta}},
\end{equation}

where $\theta+\theta_{obs}$ in the denominator was replaced with $\theta$ as $\theta_{obs}\ll\theta$.
The integral is simple to compute. Ir yields

\begin{equation}
    f_{\nu}(t_{obs}) \propto (1 - \upsilon\cos(\theta_t)/c)^{-(2 + \beta)}\nu^{-\beta} \propto t_{obs}^{-(2+\beta)} \nu^{-\beta},
\end{equation}

This equation shows, that the observed radiation does not immedeatly turns off when the soruce switches off. The flux falls off rapidly with time and wanishes when $\theta_t$ exceeds the angular size of the soruce $(\theta_j)$.

%%
%%
%%

\subsection{Syncrotron Radiation}
\red{This is not exactly accurate formalism, does not take angle into account}
\red{Could be considerably imprved}

Consider an electron moving in the magnetic field, perpendicular to the field lines.
Let $\gamma_e$, $\upsilon_e$ be the electron's Lorentz factor and veloicity and $B$ the magnetic field strength.
The electric field in the electron rest-frame is $E=\gamma_e \upsilon_e B /c$. The electron acceleration in this field yields radiation, total power of which, according to the Larmor's formula, 

\begin{equation}
    P_{syn} = \frac{2q^4E^2}{3c^3m_e}=\frac{2q^4B^2\gamma_e^2\upsilon_e^2}{3c^5m_e^2}=\frac{\sigma_TB^2\gamma_e^2\upsilon_e^2}{4\pi c}
\end{equation}

where $\sigma_T = 8\pi q^4 / (3m_e^2c^4)$ is the Thompson cross section. 

The $P_{syn}$ is the lorentz invariant *as electric dipol radiation is Lorentz invariant).

Note, that for an isotropic pitch angle distribution, the average power $\langle P_{syn} \rangle = (2/3)P_{syn}$.

The angular speed of the electron (\textit{e.g.,} its Larmor frequency), is

\begin{equation}
    \omega_L = \frac{q B}{\gamma_e m_e c}
\end{equation}

\red{nice rephrasing}
Within the magnetic field, an electron is moving on a spiral trajectory. 
The relativistic beaming of emitted radiation leads to a distant observer being able to see this radiation, only when the electron velocity vector is within $\angle \sim \gamma_e^{-1}$ from the lone of sight. Correspondingly, only a fraction of orbital time, $t\sim1/(\pi\gamma_e)$, contributes to the observed radiaiton, which appear as a repeated pulse. The duration of this pulse is

\begin{equation}
    \delta t_{obs} \sim \frac{2}{\gamma_e \omega_L}\frac{1}{2\gamma_e^2}\sim \frac{m_e c}{q B \gamma_e^2}
\end{equation}

where we used $\delta t' = \delta t / \gamma_e$. 
Then the characteristic frequency of the synchtrontron radiation is given by an inverse of $\delta t$ and reads 

\begin{equation}
    \omega_{syn} \sim \frac{q B \gamma_e^2}{m_e c} \text{ and } \nu_{syn} = \frac{\omega_{syn}}{2\pi} \sim \frac{q B \gamma_e^2}{2\pi m_e c}
\end{equation}

where $\nu_{syn}$ is the cyclic frequency.
Note that here the factor $(3/2)\sin(\alpha)$, where $\alpha$ is the pitch angle between the electron's velocity and the magnetic field is \red{ommited}.

The synchrotron spectrum peaks at $\sim \nu_{syn}$. At $\nu < \nu_{syn}$ the $P_{syn}(\nu)\propto\nu^{1/3}$ (which is determined by the Fourier transform of the synchrotron pulse profile). At $\nu > \nu_{syn}$ the power decays exponentially. See \cite{RubicyLightman1979} for the calculation of synchrotron spectum.

The power per unit frequency at the peak of the spectrum is given 

\begin{equation}
    P_{syn}(\nu_{syn}) \sim \frac{P_{syn}}{\nu_{syn}} \sim \frac{\sigma_T B m_e c^2}{2 q},
\end{equation}

Now consider the distribution of electrons.
Commonly adopted is the power-law distcibution, $dn_e/d\gamma_e \propto \gamma_e^{-p}$, which results in emssion spectrum $f_{\nu}\propto\nu^{-(p-1)/2}$,
which is q consiquencye of 

\begin{equation}
    f_{\nu} = \int_{\gamma_{\nu}}^{\infty} d\gamma_e \frac{dn_e}{d\gamma_e}P_{syn}(\nu) \propto \nu^{-(p-1)/2}
\end{equation}

as $P_{syn}(\nu) \propto (\nu/\nu_{syn})^{1/3}$ for $\nu < \nu_{syn}$\red{where is this from?}.

Here

\begin{equation}
    \gamma_{\nu} \sim \Bigg(\frac{2\pi\nu m_e c}{qB}\Bigg)^{1/2}
\end{equation}

is the minimum Lorentz factor, above which electrons contribute to the specific flux, $f_{\nu}$
\red{why?}
\gray{This seems to be an equation for $\nu = f(\gamma)$ inverted, -- so is the $\nu$ a critical frequency?}

%%
%%
%%

\subsubsection{Effect of synchrotron cooling on electron distribution}

Consider the effects of electrons cooling. 
The characteristic frequency associated with it is $\nu_c$ and lorentz factor $\gamma_c$.
Electrons with lorenz factor $\gamma_e > \gamma_c$ can efficiently loose their energy to synchrotron radiation. Then, after the time $t_0$, their $\gamma_e$ drops below $\gamma_c$, 

\begin{equation}
    c^2 \frac{dm_e}{dt} \gamma_e = -\frac{\sigma_T}{6\pi} B^2 \gamma_e^2 c
\end{equation}

which result in 

\begin{equation}
    \gamma_c \sim \frac{6 \pi m_e c}{\sigma_T B^2 t_0}
\end{equation}

The corresponding characteristic frquency is called the synchrotron cooling frequency

\begin{equation}
    \nu_c = \frac{3}{4\pi} \gamma_c^2 \frac{q B}{m_e c}
\end{equation}

At $\nu_c$ the spectrum of the synchrotron radiation is changing, as electrons with $\gamma_e > \gamma_c$, the effects of cooling modify the electron distribution. 

Consider the continuity equation for electrons in the energy space 

\begin{equation}
    \frac{\partial }{\partial t}\frac{d n_e}{d\gamma_e} + \frac{\partial}{\partial \gamma_e}\Big[ \dot{\gamma_e}\frac{dn_e}{d\gamma_e} \Big] = S(\gamma_e)
\end{equation}

where $\dot{\gamma_e} = -\sigma_T B^2 \gamma_e^2 / (6\pi m_e c)$ is the rate at which electron lorentz factor changes due to losses, $S(\gamma_e)$ is the injection rate of electrons into the system.
Assume that the minimum Lorentz factor of injected electrons is $\gamma_m$, \textit{e.g.,} where $S(\gamma_e) = 0$ for $\gamma_e < \gamma_m$.
Then if $\gamma_c < \gamma_e < \gamma_m$ the solution to the equation $dn_{e}/d\gamma_e \propto \dot{\gamma_e}^{-1} \propto \gamma_e^{-2}$.

Then, for this electron distibution the synchrotron spectrum is $f_{\nu}\propto\nu^{-1/2}$ 
\footnote{If $B=f(t)$, then the distribution function for $\gamma_e$ evolves with time and is not a simple pwoer law with index $2$, see \cite{Uhm and Zhang, 2014b}.}

For $\gamma_e > \gamma_c > \gamma_m$, the solution to the equation is $dn_e/d\gamma_e \propto \gamma_e^{-p-1}$ (assuming the constant $B$ field, the steady state). Then the synchrotron spectrum reads $f_{\nu}\propto\nu^{-p/2}$.

%%
%%
%%

\subsubsection{Synchrotron self-absorption frequency}

If the photone absorption by the inverse-syncrotron process is important, another charactersitic frequency, $\nu_a$, can be determined. Consider the Kirchhoff's law, \red{stating that the emergent specific flux cannot exceed the black-body flux corresponding to the appropriate electron temperature} which is

\begin{equation}
    k_BT\approx \max(\gamma_a,\min[\gamma_m,\gamma_c])m_e c^2 / 2.7
\end{equation}

where $\gamma_m$, $\gamma_c$ and $\gamma_a$ are electron Lorentz factors corresponding to $\nu_m$, $\nu_c$ and $\nu_a$.
Then the \red{synchrotron self-absorption frequency $\nu_a$ is the frequency where the emergent synchrotron flux is equal to the black body flux}

\begin{equation}
    \frac{2m_ec^2\max(\gamma_a,\min[\gamma_m,\gamma_c])\nu_a^2}{2.7c^2}\approx\frac{\sigma_T B m_e c^2 N_>}{4 \pi q}
\end{equation}

where the LHS is the Plunk function in the Rayleigh-Jeans limit and $N_{>}$ is the column density of electrons with Lorentz factor larger then $\max(\gamma_a\min[\gamma_m,\gamma_c])$.


Finally, the order of characteristic frequencies determines the emergent synchrotron spectrum for a distribution of electrons. 
See Fig.X for fast and slow cooling regimes \cite{Sari et al 1998}.

%%
%%
%%

\subsubsection{Maximum energy of synchrotron photons}

Consider a shock front. Scattering back and forth, particles within it accelerate via the \magenta{first order Fermi process}, increaseing their energy $\times 2$ times, at every front of the shock.
In order to determine what is the maximum energy a particle can reach consider the following. A charged particle of mass $m$ accelerates while crossing the shock front on a timescale $\sim$ Larmor time, $t'_L = mc\gamma/(qB')$, where primed quantities are measured in the rest frame of the fluid and $\gamma$,lorentz factor on a particle in the frame, comoving with the shock. 
The particle can accelerate to $\gamma$ only if it losses less then half of its energy to synchrotron emission in $t'_L$. Then 

\begin{equation}
    \frac{4 q^4 B^{'2}\gamma^2 t'_L}{9 m^2 c^3} < \frac{m c^2\gamma}{2} \text{ or } \nu\propto \frac{q B' \gamma^2}{2\pi m c} < \frac{9 m c^3}{16\pi q^2}
\end{equation}

Thut, for electron the maximum synchrotron photon energy is $\sim 50$~MeV and for proton it is $\sim 100$~GeV in the shocked fluid comoving frame. \red{assuming Bohm diffusion limit.}
This limit can be exceeded in case of highly inhoogeneous magnetic field \cite{Kumar et al. (2012)}.

%%
%%
%%

\subsubsection{Inverse-Compton radiation}

%% SINGLE electron, Single Photone
The inverse-Compton scattering is the scattering of photons by electrons of larger energy, resiling in increase in photon energy on average.
Consider electrons with $\gamma_e$ and photons with frquency $\nu$. Let $h\nu\gamma_e \ll m_e c^2$. The average frequncy of scattered photons then $\nu_s\sim\nu\gamma^2_e$.
\gray{
    This can be seen from considering the scattering in the rest frame of the electron.
    Let the incident photon have frequicy $\nu' \sim \nu\gamma_e$. (See eq.for Doppler Shift). If $h\nu'\ll m_e c^2$, the scattering is elastic (electron recoil is negligable) and the post-scattering angle distribution is a dipol function. 
    Then, transforming the $\nu'$ into the original frame results in $\nu_s\sim\nu\gamma_e^2$.
}

%% Single electron, Radiation Field
Consider a radiation field with photon density $u_{\gamma}$, and an electron moving through it. Then, the power in IC-scattered photons is (assuming $h\nu\gamma_e\ll m_e c^2$)

\begin{equation}
    P_{ic} \sim \sigma_T \int d\nu \frac{u_{\nu} c}{h\nu} h\nu\gamma_e^2 \sim \sigma_T u_{\gamma}\gamma^2_e c;
\end{equation}

where $u_{\nu}d\nu$ is the energy density in photons of frequency between $\nu$ and $\nu+d\nu$, such $\int d\nu u_{\nu} = u_{\gamma}$. From $P_{sync}$ (see eq.above.somewhere) and this eqution $P_{sync}/P_{IC} \sim u_{B}/u_{\gamma}$, where $u_{B}- B^2 / 8\pi$.

Now consider, that the radiation field is generated by the synchrotron process, \textit{i.e.,} photons are produced by and scattered on the same electrons (to typically much higher energies). This process is called \magenta{synchrotron-self-Compton} or SSC.
The relative importance of IC process is specified by the Compton paramter $Y$ for a population of energetic electrons. Consider an energy density in photons for synchrotron process

\begin{equation}
    u_{\gamma} = \int dr \int d\gamma_e \frac{P_{syn}}{c}\frac{dn_e}{d\gamma_e} = \frac{\sigma_T (\delta R) B^2}{6\pi} \int d\gamma_e \gamma^2_e \frac{d n_e}{d\gamma_e} = \frac{\sigma (\delta R) n_e B^2}{6\pi}\langle\gamma_e^2\rangle
\end{equation}

where $\delta R$ is the radial width of the source, and 

\begin{equation}
    \langle \gamma_c^2\rangle = \frac{1}{n_e} \int d\gamma_e \gamma_c^2\frac{dn_e}{d\gamma_e}.
\end{equation}

Invoking the formula \red{which} for the $u_{\gamma}$ for synchrotron radaition, the Compton parameter reads 

\begin{equation}
    Y \sim P_{IC} / P_{syn} \text{ where } \tau_e = \sigma_T (\delta R) n_e
\end{equation}

is the optical depth of the source to Thrompson scattering.

\paragraph{IC spectrum.}

In order to obtain IC radition spectrum, the seed photon spectrum is to be convolved with electron distribution \cite{RubikiLightman1979}

\begin{equation}
    f_{IC}(\nu_{IC}) \approx \frac{3\sigma_T (\delta_R)}{4} \int d\nu \frac{\nu_{IC}}{\nu^2}f_{syn}(\nu) \int \frac{d\gamma_e}{\gamma_e^2}\frac{dn_e}{d\gamma_e}F\big( \nu_{IC} / 4 \gamma_c^2\nu \big)
\end{equation}

where 

\begin{equation}
    F(x) \approx \frac{2}{3}(1-x), \text{ and } x = \frac{\nu_{IC}}{4\gamma_e^2\nu}
\end{equation}

To qualitatively asses the spectrum, assume that the seed photon spectrum is a $\delta$-fucntion around frquency $\nu_0$. Electron distribution is apower law with inxex $p$.
Consider the low energy side, where the spectrum is cut off at $\gamma_m$, is proportional to $\nu_{IC}$ for $\nu_{IC} < 4\gamma_m^2\nu_0$. Then, if synchrtron self-absorption is neglected, then the IC spectrum at low energies is much steeper than the hardest synchrotron spectrum $\propto\nu^{1/3}$.
Now, consider the high energy side, $\nu_{IC} > 4 \gamma_m^2\nu_0$. There, the IC spectrum approaches $\propto \nu_{IC}^{-(p-1)/2}$, same as the synchrotron process spectrum.

\paragraph{IC in Klein-Nishina regime}

The assumed non-elastic scattering of photons is only valid as long as photon energy is lower then $m_e c^2$ in the comoving frame. When this condition is not longer valid, the electron recoil in the scattering can no longer be ignored. Additioanlly, the cross-section becomes smaller then $\sigma_T$ (decreasing with rising photon energy). 
The electron recoil also leads the the change in upper limit of the scattered photon energy, $\sim m_e c^2 \gamma_e / 2$. See \cite{RubikiLightmann1979} for equations.

%%
%%
%%

\subsubsection{Hadronc processes}
\red{very brief}

Under the hadronic processes one understands the followign processes.
The photon-pion process, \textit{i.e.,} the production of pions ($\pi^0, \pi^+$ and $\pi^-$), the decay of $\pi^+$ produces $p^+$ with high lorentz factor that can cool via synchrotron processes.
The Bethe-Heitler pair production process.
Others...

%%
%%
%%

\section{Afterglow Theory}

Consider a dynamics of a relativistic blast wave propagating through a circumburst medium (CBM). Such scenario is a universal part of the GRB theory, that can be treated independently. 
Assume that such "fireball" has a Lorentz factor $\Gamma_0$ and a total "isotropic equivalent" energy $E$. The CBM has a density profile described by $n(R) = (A/m_p)R^{-k}$.

The theory of relativist shocks with applications to AGN jets was developed by \cite{Blandford and McKee (1976)}. Later, the theory was successfully applied to GRB afterglows \cite{Costa et al., 1997; van Paradijs et al., 1997; Frail et al., 1997}. 
Importantly, the power law behaviour of the afterglow lightcurves is natually reproduces the self-similar nature of the self-similar blast wave solution.

\red{here the physical understanding is emphasized, not the derivation}

\red{This is badly rephrased. Could be improved with a pic from Nava}
Consider the reference frame comoving with the shocked fluid. Then, $\Gamma$ is the LF of this fluid with respect to the unshocked one. The density of the unshocked fluid in this reference frame is $\Gamma n$, and its particles are seen as streaming towards the shocked fluid with LF $\Gamma$. Generally, for unshocked fluid it is assumed that the thermal energy of its particles is much lower than the rest mass, or in other words, that the CBM is cold. 
Passing through the cold CBM, shock front randomizes the particle, protons, velocity vectors, raising their thermal energy to $\Gamma^2 m_p c^2$ (while their LF remains unchanged). 
In the \red{lab frame the average energy of each down stream proton is $\Gamma^2 m_p c^2$, from which it follows that at radius $R$, the total emergy in the shocked plasma }

\begin{equation}
    E \approx \frac{4\pi A R^{3-k}c^2 \Gamma^2}{3-k}
\end{equation}

where $AR^{-k}$ is the density of the CBM at radius $R$ and $4\pi A R^{3-k}/ (3-k)$ is the total swept up mass.

From this equation the dynamic of a blast wave can be inferred. For instance, assuming $E=\text{const}$, $k=0$ (uniform density in CBM), the blast wave lorentz factor $\Gamma\propto R^{-3/2}$.

The radius from the center of explostion (CoE) at which $\Gamma = 1/2 \Gamma_0$, initial value, when also $1/2 E_0$ is deposited into CBM, is called \textit{deceleration radius}, $R_d$. 
For the uniform CBM, it is $R_d\propto E^{1/3} n^{-1/3} R_{0,2}^{-2/3}$.

Additionally, shock compresses the plasma. For \red{highly relativistc shocks}, the compression is $4\Gamma$, giving the density in the comoving frame $4 \Gamma n$.
It also accelrates the inbound particles to a power-law distribtuion function. Additionally, it apmplifies the magnetic fields. 

Essencially, this is all that is required for computing the afterglow emission.

\red{Now consider a slightly more rigorous derivation of $R_d$ and compression ratio and entropy generation by the blast wave.}

Now, consider a relativistic shock propagating into a cold upstream medium.
The evolution of physical properties of the shock is guverned three conservation laws: baryon number, $n' \Gamma c$, energy and momentum fluxes across the shock front. The latter two are a imbeded into the fluid energy momentum tensor 

\begin{equation}
    T^{\mu\nu} = (\rho' c^2 + p') u^{\mu} u^{\nu} + p' g^{\mu\nu},
\end{equation}

where $\rho' c^2$ is the total energy density, $p'$ is the pressure (in the plasma rest frame), $u^{\mu}$ is the $4$-velocity and $g^{\mu\nu}$ is the metric tensor.

Through some magic the conservation equations mentioned above can be written as \cite{Blandford and McKee, 1976; Rezzolla and Zanotti, 2013}

\begin{eqnarray}
    \frac{e_2'}{n_2'} = (\gamma_{21} - 1)m_p c^2 \\
    \frac{n_2'}{n_1'} = \frac{\hat{\gamma}\gamma_{21} + 1}{\hat{\gamma}-1} \\
    \gamma_{1s}^2 = \frac{(\gamma_{21} + 1) [\hat{\gamma}(\gamma_{21}-1)+1]^2}{\hat{\gamma}(2-\hat{\gamma})(\gamma_{21}-1)+2}
\end{eqnarray}

Here \red{same as in Nava picture that you should put} the $2$ and $1$ subscripts stand for downstram and upstream respectively, $e'$ is the internal energy density, $n'$ is the proton number density, (both in the local fluid rest frame), $\gamma_{21}$ is the relative Lorentz factor of plasma in region $2$ with respect to the region $1$, $\gamma_{1s}$ is the relative Lorentz factor of plasma in region $1$ with respect to the shock front and $\hat{\gamma}$ is the adiabatic index of the fluid.

For $\Gamma\gg1$ that usually describes early stage of the GRB afterglow \cite{(e.g. Piran, 1999)}, the $\hat{\gamma}=4/3$ \red{recall that in subrelativistc it is $5/3$}
Then, $n_2'/n_1' = 3 ((4/3)\gamma_{21} + 1) = 4\gamma_{21} + 3 \approx 4\gamma_{21}$ which implies that the downstream plasma is compressed with compression ratio $4\gamma_{21}$.
Similarly, approximating, the conservation equation $\frac{e_2'}{n_2'} \approx \gamma_{21} m_p c^2$ and the last equation can be simplifed to $\gamma_{1s} \approx \sqrt{2} \gamma_{21}$, which imlies that the shock front travels faster then the downstream fluid.

Next, consider the self-similar deceleration phase of the blast wave in the constant density CBM. There, the energy consirvation reads 

\begin{equation}
    E = \frac{4 \pi }{3 } R^3 n m_p c^2 \Gamma^2 = \text{ const}
\end{equation}

where $\Gamma = \gamma_{21}$ is the lorentz factor of the blastwave with respect to the unshocked medium, $R$ is the radius of the blast wave (from CoE).
Note, that in the comoving frame the average proton thermal enrgy is $m_p c^2 \Gamma$. In the lab frame it is $m_p c^2 \Gamma^2$. 
Overall, we observe that $\Gamma^2 R^3 = \text{ const}$ or 

\begin{equation}
    \Gamma \propto R^{-3/2}
\end{equation}

Now, consider the elapsed time in the observer frame. 
As both the blastwave and emitted photons are moving in the same direction with the speed difference of $\sim 1/2 \Gamma^2$, 

\begin{equation}
    t_{obs} \sim \frac{R}{2\Gamma^2 c} \propto R^4 \propto \Gamma^{-8/3}
\end{equation}

and 

\begin{equation}
    \Gamma \propto R^{-3/2} \propto t_{obs}^{-3/8}, \hspace{3mm} R\propto t_{obs}^{1/4}
\end{equation}

%%______________________________________
%% on the non-uniform CBM

Next, we consider \red{power-law stratified density profile}, 

\begin{equation}
    n = n_0 \Big(\frac{R}{R_0}\Big)^{-k}
\end{equation}

and obtain similar scaling relations.
\red{I do not really need this. I can go direcly to Peer model and Nava model}

\begin{equation}
    E = \int n_0 \Big( \frac{R}{R_0} \Big)^{-k} m_p c^2 \Gamma^2  4\pi R^2 dR = \text{ const}
\end{equation}

where $R^{3-k} \Gamma^2 \text{ const}$. 

After some derivation 

\begin{equation}
    \Gamma\propto R^{(k-3)/2}\propto t_{obs}^{(k-3)/(8-2k)}
\end{equation}

And if $k=0$, the previously derived relation for constant density CBM follows.

A particularly usefull case is the CBM filled with a free wind with constant mass loss rate $\dot{M}$ and wind speed $\upsilon_w$, that gives $\dot{M}=4\pi R^2 n \upsilon_w = \text{ const}$, or $n\propto R^{-k}$, \textit{i.e.,} the case of $k=2$ with $\Gamma\propto R^{-1/2}\propto t_{obs}^{-1/4}$.

%%_______________________________________
%% On the energy injection

Now consider the case where the energy of the blast wave is contiuously increasing. A possible physical scenario here is a long-lasting Poynting-flux dominated jet, feeding the fireball \gray{and suppressing the reverse shock}. 
Then the energy of the outflow from the central engine has to be included into the energy equation of the blast wave

\begin{equation}
    E_{tot} = E_0 + E_{inj}
\end{equation}

Consider a central engine with time dependent luminocity  $L(t) = L_0 (t_{obs}/t_0)^{-q}$. Then the energy equation reads

\begin{equation}
    E_{tot} = E_0 + E_{inj} = E_0 + \int_{0}^{t_{obs}} L(t)dt = E_0 + \frac{L_0 t_0^q}{1-q}t_{obs}^{1-q}
\end{equation}

where $E_0$ is the initial energy of the blast wave and $E_{inj}$ is the injected energy into the blast wave from the central engine.

Consider the case when energy injection increases with time noticably, $q<1$.

Then, when $E_{inj} \gg E_0$ for $q<1$, the blastwave scaling 

\begin{equation}
    E_{tot} \sim E_{inj} \propto t_{obs}^{1-q}.
\end{equation}

and for the constant density CBM, $\Gamma^2 R^3 \propto t_{obs}^{1-q}$ which eventually leads to $\Gamma\propto R^{-(2+q)/(4-2q)}\propto t_{obs}^{-(2+q)/8}$.

And it is easy to see that if $q\rightarrow 1$, the 'no injection' case is resored.

%%____________________________________
%% Lorentz factor stratification of the ejecta as the Energy injection

The energy can be added to the blast wave in a form of velocity stratified ejecta, when the wave decelerates, \textit{e.g.,}

\begin{equation}
    E\propto \gamma^{1-s}\propto\Gamma^{1-s}
\end{equation}

where $\gamma$ is the Lorentz factor of the ejecta and $\Gamma$ is the lorentz factor of the blast wave.
Here the effects of the reverse shock can also be neglected as energy injection comes when $\Gamma\sim\gamma$ \red{[How does this work in the Peer/Nava model?]}

This method is equivalent to the long-lived central engine with time dependent luminocity (at least for the dynamics of the blast wave), and the coefficient $s$ can be expressed in terms of $q$ \cite{Zhang et al., 2006}. 

For a unifrom density CBM the scaling realtion reads

\begin{equation}
    \Gamma\propto R^{-3/(1+s)}\propto t_{obs}^{-3/(7+s)}, \hspace{3mm} R\propto t_{obs}^{(1+s)/(7+s)}
\end{equation}

which then gives $s = (10-7q)/(2+q)$ and $q=(10-2s)/(7+s)$

\red{The question is, can I add L(t) to the dE/dr of the Nava model and it is all?..}

%%
%%
%%

\subsection{Afterglow synchrotron spectrum and lightcurve}

\red{Note that here the electron distribution function is implicitely assumed such that $dn/d\gamma$ peaks at $\gamma_m$ and for $\gamma>\gamma_m$, the $dn/d\gamma\propto\gamma^{-p}$,and for $\gamma<\gamma_m$, the distribution is uncertain. It could be thermal}

\cite{(Sari et al., 1998)} has shown that the multi-segment broken power law function can \gray{sufficiently well} describe the instantaneous afterglow spectrum. The spectrum consists of three cahractersitic frequencies: the \red{typical synchrotron frequency of the accelerated electrons with the minimum Lorentz factor} $\nu_m$, the cooling frequency $\nu_c$ and the synchrotron self-absorption frequency $\nu_a$.
The latter is usually the smallest one in the afterglow phase (at least for a few months \gray{but it depends on the CBM density}).
Depending on the order of $\nu_m$ and $\nu_c$ the spectrum falls into two broad categories. The \textit{slow cooling} case, when $\nu_m < \nu_c$ and a \textit{fast cooling} case when $\nu_m > \nu_c$. The scaling relations for these regimes are \cite{Sari et al. (1998)}

\begin{equation}
    f_{\nu} = 
    \begin{cases}
        f_{\nu,\max}\Big(\frac{\nu_a}{\nu_m}\Big)^{1/3}\Big(\frac{\nu}{\nu_a}\Big)^2 & \text{ if } \nu < \nu_a \\
        f_{\nu,\max}\Big(\frac{\nu}{\nu_m}\Big)^{1/3} & \text{ if } \nu_a < \nu < \nu_m \\
        f_{\nu,\max}\Big(\frac{\nu}{\nu_m}\Big)^{-(p-1)/2} & \text{ if } \nu_m < \nu < \nu_c \\
        f_{\nu,\max}\Big(\frac{\nu_c}{\nu_m}\Big)^{-(p-1)/2}\Big(\frac{\nu}{\nu_c}\Big)^{-p/2} & \text{ if } \nu > \nu_c \\
    \end{cases}
\end{equation}

for the slow cooling case and 

\begin{equation}
    f_{\nu} = 
    \begin{cases}
        f_{\nu,\max}\Big(\frac{\nu_a}{\nu_c}\Big)^{1/3}\Big(\frac{\nu}{\nu_a}\Big)^2 & \text{ if } \nu < \nu_a \\
        f_{\nu,\max}\Big(\frac{\nu}{\nu_c}\Big)^{1/3} & \text{ if } \nu_a < \nu < \nu_c \\
        f_{\nu,\max}\Big(\frac{\nu}{\nu_c}\Big)^{-1/2} & \text{ if } \nu_c < \nu < \nu_c \\
        f_{\nu,\max}\Big(\frac{\nu_m}{\nu_c}\Big)^{-1/2}\Big(\frac{\nu}{\nu_c}\Big)^{-p/2} & \text{ if } \nu > \nu_m \\
    \end{cases}
\end{equation}

from the fast cooling case. 

Here $f_{\nu;max}$ is the flux density which $f_{\nu}(\nu_m)$ for the slow cooling case and $f_{\nu}(\nu_{c})$ for the fast cooling. \red{this is the answer  I was looking for}. 

Note that in this formalism, the scaling relations, spectrum, does not depend on the blast wave dynamics. Only the peak flux $f_{\nu,\max}$, and the characteristic frequencies $\nu_a$, $\nu_c$, $\nu_m$ do. 

The characteristic frequencies $\nu_m$, $\nu_c$ can be calculated from the synchrotron frequency formula (see previouse sections)

\begin{equation}
    \nu = \frac{3}{4\pi}\gamma^2\frac{qB'}{m_e c}
\end{equation}

where $\gamma$ is $\gamma_{m}$ or $\gamma_c$. 

The minimum lorentz factor in the shock heated plasma with a given electron distribution (power law) is

\begin{equation}
    \gamma_m = g(p) \varepsilon_e (\Gamma - 1) \frac{m_p}{m_e}\frac{n_p}{n_e},
\end{equation}

where $\Gamma$ is the lorentz factor of the blast wave, $\varepsilon_e$ is the fraction of energy density of the shocked fluid given to electrons, $n_p$ and $n_e$ are the number densities of the protons and electrons respectively, and $g(p)$ is the dimensionless factor

\begin{equation}
    g(p) \approx 
    \begin{cases}
        \frac{p-2}{p-1} & \text{ if } p>2 \\
        \ln^{-1}(\gamma_M/\gamma_m), & \text{ if } p = 2
    \end{cases}
\end{equation}

where $\gamma_M$ is the maximum LF of electrons.

\gray{
    the $gamma_min$ and its dependency on $g(p)$ seems to come from equating the integral of the electron distribution function for $\gamma>\gamma_min$ to the total energy density of the shocked fluid, \textit{i.e.,} $4\Gamma (\Gamma - 1) n_p m_p c^2$ times the fraction of this energy avaialble, $\varepsilon_e$. 
    [If this is the case, maybe I can compute it numerically?..]
}

The cooling lorentz factor of electrons is derived based on the cooling timescale (see previous sections)

\begin{equation}
    \gamma_c = \frac{6\pi m_e c}{\sigma_T t' B^{'2} (1 + Y)}
\end{equation}

where $Y = u_{syn}/u_{B}$ is the synchrotron self-Compton parameter, which depends whether the photon-electron scattering is in Klein-Nishina limit (\textit{i.e.,} if photon energy exceeds $m_e c^2$),
the $u_{syn}$ is the synchrotron photon energy density and $u_B$ is the magnetic energy density.

The synchrotron self-absorption frequncy can be obtained by equating the black-body flux corresponding to the temperature of electrons with characteristic frequency $\nu_a$ to the emerging flux at $\nu_a$, 

\begin{equation}
    I_{\nu}^{syn}(\nu_a) = I_{\nu}^{bb}(\nu_a) \approx 2 k_B T \frac{\nu_a^2}{c^2}
\end{equation}

where 

\begin{equation}
    k_B T \approx \max[\gamma_a,\min(\gamma_c,\gamma_m)] m_e c ^2 /3
\end{equation}

where $\gamma_a$ corresponds to $\nu_a'$ as $\gamma_a = (4\pi m_e c \nu_a' / 3qB')^{1/2}$.

Assuming that a fraction, $\varepsilon_B$, of the energy density of the shocked CBM goes into the magnetic filed energy density, the magnetic field strength reads 

\begin{equation}
    B' \approx [32 \pi m_p c^2 \varepsilon_B n_p (\Gamma - 1)\Gamma]^{1/2}
\end{equation}

%%

Next the scaling for the $\nu_m$, $\nu_c$ and $\nu_a$ are provided based on the shock dynamics in different CBM. 
For instance, for the case of constant density medium see \cite{(e.g. Granot and Sari, 2002; Gao et al., 2013b)} and for wind-like CBM... 

%%

The maximum of the specific flux $f_{\nu,\max}$ is (see previous section)

\begin{equation}
    f_{\nu;\max} = \frac{(1+z)L'_{\nu'}\Gamma}{4\pi D_L^2} \approx (1+z)\frac{N_{tot}P_{\nu;\max}' \Gamma}{4\pi D_{L}^2}
\end{equation}

where $L'_{\nu'}$ is the specific luminocity in the jet comoving frame, $N_{tot}$ is the total number of electrons that contribute to radiation at frequency $\nu$

\begin{equation}
    P_{\nu',\max}' = \approx \frac{\sqrt{3} q^3 B'}{m_e c^2}
\end{equation}

is the pwoer radaited per unit frequency for one electron at the peak of the spectrum, or in other words it is the specific power for electron with thermal LF $\gamma\approx\min(\gamma_c,\gamma_{min})$ and $z$ is the reshift of the burst and $D_L$ is the luminocity distance to it.

Note that for a constant density CBM, the $F_{\nu,\max}$ is time independent as $F_{\nu,\max}\propto R^3\Gamma^2\propto E_{tot}$ total energy of the blast wave (which is constant for an adiabatic external shock). For the constant density CBM and wind-like CBM see full expression for the peak specific flux \cite{Granot and Sari, 2002; Yost et al., 2003}

Note more that for $\nu>\max[\nu_m,\nu_c]$, the observed specific flux \cite{(Kumar, 2000; Freedman and Waxman, 2001)} is independent on the CBM density and its stratification and depends weakly on $\varepsilon_B$, 

\begin{equation}
    f_{\nu} \propto E^{(p+2)/4} \varepsilon_e^{p-1}\varepsilon_B^{(p-2)/4}t_{obs}^{-(3p-2)/4}\nu^{-p/4}
\end{equation}

This is of particular importance for high energy emission from GRB.

%%
%%
%%

\subsection{Reverse shock}
\red{This is very short section, not very clear in the source}

If the GRB-ejecta magnetisation is not important dynamically $\sigma \ll 1$, (where magnetisation parameter $\sigma B^{'2}/(4\pi n_p' m_p c^2)$), the reverse shock (RS) is traversing the ejecta during the early afterglow. \gray{The RS decelerates the ejecta}

The RS-FS system can be represented via $4$ regions (see Fig.8 or a Fig from Nava) that consists of: $1.$ unshocked medium, $2.$ the shocked medium, (separated from the reion $1$ by the forward shock front), $3.$ the shocked ejecta and $4.$ the unshocked ejecta, with reverse shock front separating $3$ and $4.$.

The shock fronts $1-2$ and $3-4$ are surfaces of contact density-discontinuity.

%% observed
The RS heated ejecta was observed first in the case of GRB 990123 as an optical flash while the burst was still active \cite{(Akerlof et al., 1999; Sari and Piran, 1999a; Meszaros and Rees, 1999)}.

%% deriviation of RS 
\red{Here Nava paper would be much better}
The derivation of rhe FS-RS dynamics is based on the pressure equilium across the contact discontinuity surfaces (separating regions $2$ and $3$ \red{note that these are not shock fronts}). 
Consider and unmagnetized GRB jet. 
Introduce $\gamma_{12}$ and $\gamma_{34}$, lorentz factors of the FS (RS) heated CBM (GRB-ejecta) with respect to the unshocked CBM (GRB-ejecta). The shocked fluid pressure in regions $2$ and $3$ is $4\gamma_{21}^2 n_1$ and $4\gamma_{34}^2 n_4'$ respectivel, where $n_1'$ and $n_4'$ are the densities of the unshocked medium in the \textit{local} comoving frame.
Note that the lorents factor of the unshocked ejecta ($4$) with respect to the unshocked CBM ($1$), \textit{e.g.,} $\gamma_{41}=\Gamma_0=2\gamma_{21}\gamma_{34}$, where the latter RHS is dervied based on the addition of $4$-velocities. Then, together with pressure equilibrium across the discontenuity surface \gray{presumably $2$ and $3$} it yileds

\begin{equation}
    \gamma_{34} \approx (n_1/4n_4')^{1/4}\Gamma_0^{1/2} \text{ and } \gamma_{21} \approx (n_4'/4n_1)^{1/4}\Gamma_0^{1/2}.
\end{equation}

Note, that here the fact that the RS and FS are relativistc is assumed. \red{For non-relativistic RS the derivation is defferent}.

Note that the $\gamma_{31}$, the lorenz factor of the shocked jet with respect to the CBM is equal to $\gamma_{21}$, as both shocks are moving with the space velocity. 

Now, obtained $\gamma_{ij}$ allows to study the thermodynamic properites of the FS/RS system. For \red{full} derivation of these formalism see \cite{Sari and Piran (1995)}.

%% Radiation into?
TO simplify the calculations of rhe radiation from the RS, the finite size of the GRB-jet and constant lorentz factor of the ejecta are usually assumed. This size corresponds to the finite duration of the GRB.
\gray{The velocity stratified ejecta howere is possible and induce interesting features in the RS lightcurve \cite{(Rees and Meszaros, 1998; Sari and Meszaros, 2000; Uhm and Beloborodov, 2007; Genet et al., 2007; Uhm et al., 2012; Uhm and Zhang, 2014a)}}
Then, adiabatic coolding of electrons and decline in magnetic field strength (after the RS reaches the back of the ejecta) results in a steep fall of the RS radiation $\sim t^{-2}$ \cite{(Sari and Piran, 1999b)}. 

%% back to the dynamics
The FS/RS shock dynamics ultimately depends on the width of the ejecta, \cite{(Sari and Piran, 1995)}

\begin{equation}
    \xi = (l/\Delta)^{1/2}\Gamma_0^{-4/3},
\end{equation}

where $l$ is the Sedov radius \red{the radius at which the rest mass energy of the swept up CBM by the blastwave is equal to the initial energy of the GRB}. 
$\Delta$ is the thickness of the GRB-ejecta in lab frame, with $T$ being the duration of the burst in the CoE frame. 
Based on $\xi$ the GRB ejecta is considered to be a "thin shell", if $\xi > 1$ or a "thin shell" if $\Xi < 1$, and the shock dynamics is different in these two cases.

%% Literature
See \cite{Kobayashi (2000)} for the discussion of the FS/RS dynamics and radition emitted discussion.
Additionally see \cite{(Kobayashi and Zhang, 2003b; Zhang et al., 2003a; Kumar and Panaitescu, 2003; Wu et al., 2003; Kobayashi and Zhang, 2003a; Gao et al., 2013b)} for the FS/RS emisison properties.

Additionally, see \cite{Zhang and Kobayashi (2005); Mimica et al. (2009); Narayan et al. (2011)} for the shock solution for a jet with arbitrary magnetization $\sigma$ and the work of \cite{Fan et al. (2004)} for the case of $\sigma < 1$. 

The ration of microphysical parameters in FS and RS, as well as their lorentz factors determine how important is the RS emission. 
\cite{(Kobayashi and Zhang, 2003b; Zhang et al., 2003a; Nakar and Piran, 2004)}
If the emission from FS and RS can be separated in the afterglow data, then the RS radiation provides one of the few hints into the GRB-ejecta composition 
\cite{McMahon et al. (2006); Nakar and Piran (2004)}

With respect to the ratio FS/RS different observational features are present \cite{(Zhang et al., 2003a; Jin and Fan, 2007)}.

\textit{Type I: re-brightening}. Assuming common values for $\varepsilon_e=0.1$ and $\varepsilon_B=0.01$, for FS and RS, the optical lightcurve exhibits a double peak structure, with the first peak dominated by RS and second by FS. It was observed \cite{Kobayashi and Zhang, 2003b; Shao and Dai, 2005}. 

\textit{Type II: flattening}. If the RS has stronger magnetic field then FS, and the magnetisation parameter for region $4$ is small enough for not to supress the RS, the emission from the RS dominates the early afterglow, peaking when the RS reaches the end of the GRB-ejecta, after which it decays $\propto t^{-2}$ (\cite{Meszaros and Rees, 1999; Sari and Piran, 1999b}). At some point the decay takes over, cahnging the decay behaviour to $\propto t^{-1}$. This has observational support \cite{(Fox et al., 2003; Li et al., 2003; Zhang et al., 2003a; Kumar and Panaitescu, 2003; Gomboc et al., 2008}

\textit{Type III: no RS component}
Many observed optical afterglow shows a smooth hump with a post-decay slope consistent with FS emission and nos signatures of RS \cite{Molinari et al., 2007; Ryko et al., 2009; Liang et al., 2010}.
A possible explanation is that Poynting flux dominates the GRB-jet and supresses the RS 
\cite{Zhang and Kobayashi, 2005; Mimica et al., 2009}.

%%
%%
%%

\subsection{Jet Break}

\red{The convention is to call the jet with uniform structure a top-hat jet...}

Here we consider effects of a finite jet angle on the observed lightcurve.

The observations of GRB afterglow, specifically, the observed achromatic break, suggested that GRB-jet is collimated. The achromatic break in the lightcurve, usually referred to as "jet break", has in its origin two main effects \cite{Rhoads, 1999; Sari et al., 1999}. 

First, is a pure geometric effect, called "edge" effect \cite{(e.g.Meszaros and Rees, 1999; Panaitescu and Meszaros, 1999; Rhoads, 1999; Sari et al., 1999}. Specifically, for a GRB-jet with openning angle $\theta_j$, the observer sees only the radiation coming from within the $1/\Gamma$ cone due to relativistic beaming. Thus, if $\Gamma > 1/\theta_j$, the radiation from only a fraction of the jet an observer sees. Such "pre-jet-break" lightcurve appears indistinguishable from a lightcurve from isotropic fireball (as in any case only a cone of $1/\Gamma$ is visible). 
As the jet decelerates, however, the $1/\Gamma$ cone grows and when the photon beaming angle equates the openning angle of the jet-cone, the lightcurve shows a 'Jet break", after which the observed radiation starts to fall off more steely then before the "jet break". 
Note, the "edge effect" is geometrical and special relativistic, effect. The jet dynamics does not change as well as the temporal behavior of characteristic frequencies. Only the observed temporal behavior of lightcurves. 
In order to account for this effect, the factor $\theta^2_j/(1/\Gamma)^2 \propto \Gamma^2$ is introduced.
In the case of uniform CBM, where $\Gamma\propto t^{-3/8}$, the post-jet-break lightcurve falls off faster by a factor $\Gamma^2\propto t^{-3/4}$.

Note, that the effect of jet-break smears out by the integration over equal-time-arrival surface \cite{Kumar and Panaitescu, 2000b; Piran, 2000; Granot and Piran, 2012}.

%% LATERAL EXPANSION

Next, consider the jet sideways (lateral) expansion.
It was shown that the the jet lateral expansion, (when the sound waves cross it) occures apporximatelly at a time when the "edge effect" becomes important \cite{Rhoads (1999) and Sari et al. (1999)}.
%% See page 37 for original text. It is very messy.
Consider a sideways expansion speed of the jet in comoving frame. For relativistic plasma it is $c/\sqrt{3}$. Then, the jet openning angle grows as $\theta_j\sim\Gamma_{(ij)k}^{-1}$, as a result of time dialation and transverse speed being $\sim c$. Thus, in the jet comoving frame, the elapsed time is $1/\Gamma$ of the elapsed time in the lab frame. In its oww, rest frame the the transverse size of the jet epanding with $\sim$ is $\sim R/\Gamma$, while in the lab frame the transverse speed is $\upsilon_{\theta}\sim c/\Gamma$.
From the momentum equation if follows hwoever that for a relativisic plasma $\partial(\rho\Gamma^2\upsilon_{\theta})/\partial t \sim r^{-1}\partial p / \partial\theta$, which leads to $\upsilon_{\theta}\sim c/(\Gamma^2\theta_j)$.
See \cite{Kumar and Granot (2003)} \cite{Granot and Piran (2012)} for the detailed discussion.
\red{Note that in the 'code' we use the Granot and Piran (2012) model as well.}

Analytically it can be shown that, after the jet-break the jet $R$ increase slows down \textit{e.g.,}
\cite{Rhoads (1999); Sari et al.(1999); Piran (2000); Granot and Piran (2012)}.
A simple scaling relations for $\nu_m$, $\nu_c$ and $F_{\nu;max}$ as well as $f_{\nu}=f_{\nu}(\nu, t_{obs}, p)$ can be obtained.
Importantly, the flux at a frequncy taht lies above the $\min(\nu_c,\nu_m)$decays faster with the jet lateral expansion (compare to the "edge effect" alone).

It was shown that the sideways expansion becomes important only after the $\Gamma$ falls below $2$ \cite{(Granot et al., 2001; Kumar and Granot, 2003; Cannizzo et al., 2004; Zhang and MacFadyen, 2009; De Colle et al., 2012; Granot and Piran, 2012; van Eerten and MacFadyen, 2012; van Eerten et al., 2012)}
\red{Really??}

However, the observed lightcurve depends on the observer's vewving angle \textit{e.g.,} \cite{Zhang et al., 2014b; Ryan et al., 2014).}.

%% STRUCTURED JET

It is expected that the GRB jet has an angular structure with luminosity per solid angle and Lorentz factors depending on the the angle.
Commonly assumed structure times include the dependence of jet properties on the angle as a power-law function
\cite{(Meszaros et al., 1998; Rossi et al., 2002; Zhang and Meszaros, 2002b)} 
and Gaussian distribution 
\cite{(Zhang and Meszaros, 2002b; Kumar and Granot, 2003; Zhang et al., 2004a).}
The structure of the jet becomes very importnat when the jet is observed off-axis, while for an on-axis observer, the afterglow has a steeper decay in coamparsion to a simple "top-hat" jet \cite{Meszaros et al., 1998; Dai and Gou, 2001; Panaitescu, 2005}
For an off-axis observer, seeing the power-law jet, the "jet break" time depends on the viewing angle $\theta_{\upsilon}$ (contrary to the $\theta_j$ jet openning angle as in top-hat jet)
\cite{Zhang and Meszaros, 2002b; Rossi et al., 2002; Kumar and Granot, 2003; Granot and Kumar, 2003}.
For an off-axis observer, seeing the gaussian get, the observed lightcurve depends on whether the $\theta_{\upsilon}$ falls within the gaussian cone. If yes, then the observed emission is similar to the "top-hat" jet. If no, then it is similar to the power-law jet.
\cite{Kumar and Granot, 2003; Granot and Kumar, 2003}

Importantly, that structured jet allows to introduce the so called "quasi-universal" GRB, \cite{(Rossi et al., 2002; Zhang and Meszaros, 2002b; Zhang et al., 2004a)}. The GRB jets are considered to the similar, appear only different to the observer due to different viewing angles (see \textit{e.g.,} \cite{Zhang and Meszaros, 2002b}).
See the \cite{(Zhang and Meszaros, 2002b; Rossi et al., 2002} for the similarities in the luminocity functions of structured jets. 
It was however shown that quasi-universal jet does well agree with observations \cite{(Nakar et al., 2004}. But with more free parameters, it might be more consistent with observational constraints \cite{(Lloyd-Ronning et al., 2004; Zhang et al., 2004a; Dai and Zhang, 2005)}

%% TWO compnent Jet Model

As a subcategory of structured jets, a two-component model has been widely consdiered. Such model consits of a collimated energetic get with high $L_{\gamma,iso}$ and $\Gamma$, surrounded by a wider and slower \gray{cocoon}. 
Such model can account for certain observational features, \textit{e.g.,} for early jet break and subsequent rebrightening
\cite{Huang et al., 2004; Peng et al., 2005; Wu et al., 2005}.
Particular application of the model GRB 030329 \cite{(Berger et al., 2003b)} and GRB 080319B \cite{(Racusin et al., 2008).}.
Such model is particularly favioured for the collaspar scenario, where the narrow, highly relativist jet emerges from a star, in addition to the wide, less relativistic "cocoon" around it \cite{Ramirez-Ruiz et al., 2002; Zhang et al., 2004b}. 

%%

Another example of a structured jet is a "patchy" jet, where numerous "mini-jets" within a broad jet cone are the source of the bright emission \cite{Kumar and Piran, 2000a; Yamazaki et al., 2004b}.
Such structure can occure when there are non-uniform shells present.
The patches of the accelerated emission regions can be present within relativisitc outflowed due to magnetic reconnections or turbulence in a magnetically domianted jets \cite{Luticov and Blandford, 2003; Narayan and Kumar, 2009; Kumar and Narayan, 2009; Lazar et al., 2009; Zhang and Yan, 2011; Zhang and Zhang, 2014}

%%

\textit{Orphan afteglows}

The orphan afterglow is the afterglow detected without prompt $\gamma$-ray emission. This can occur if the line of sight to the observer lies outside the cone of relativistically beamed photons from the jet. However, as the Doppler beaming factor inceases (as the $1/\Gamma$ cone widens), and the observed emission rises. It peaks when the line of sight crosses the $1/\Gamma$ cone. After that, the lightcurve behaves in accordince with the post-jet-break afterglow \cite{Granot et al., 2002}.
\textit{dirty fireball} is one of the scenarios where the ophan afterglow is possible, as its low $\Gamma$ prevents the promt energetic emission from arising \cite{Huang et al., 2002).}. 
The detection of such afterglows however is very challenging.

%%
%%
%%

\subsection{Other Effects}

The effects of radiative losses on the blastwave dynamics, and afterglow lightcurves
\cite{e.g. Rees and Meszaros, 1998; Dermer et al., 1999; Meszaros and Rees, 1999; Huang et al., 1999; Bottcher and Dermer, 2000; Nava et al., 2013} and \red{are not discussed} here.

%%

\subsubsection{Naked afterglow and high-latitude effect}

At a certain distance from the CoE, a blastwave may encounter a void. But even though, the acceleration of new electrons (and their subsequent adiabatic cooling) stops, the observed emission does not shuts down instantaneously. The reason is the lorentz beeming. Photons from angles larger then $1/\Gamma$ with respect to the line of sight continue to contribute to the observed flux for some time. This is so-called "high latitude" radiation. It has a signature $f_{\nu}\propto t^{-\alpha}\nu^{-\beta}$, where $\alpha= \beta + 2$. See \cite{Fenimore et al. (1996); Kumar and Panaitescu (2000a); Dermer (2004)}.
This effect is believed to the responsible for steep delcining X-ray lightcurves of some GRBs \cite{Zhang et al., 2006}.

%%

\subsubsection{Energy injection}

In addition to the continous energy injection (from a central engine or outflow), discussed above, there is also a possibility of a dsecrete energy injection. This can occure if fast shells of ejecta catch up with the blast wave. Interaction of such shell with blast wave can be described in terms of five (six) different regions separated by three shocks \cite{(Kumar and Piran, 2000b)} (\cite{(Zhang and Meszaros, 2002c)}). 
Such energy injection leads to a diverse range of features in afterglow, as abrupt optical rebrightening. \cite{(e.g. Nardini et al., 2011} \cite{Zhang and Meszaros, 2002c}

%%

\subsubsection{Density Bumps}

It was suggested that the sudden incease in density of the curcumburst medium may introduce bump features in lightcurves \cite{Dai and Lu, 2002; Lazzati et al., 2002; Dai and Wu, 2003; Pe'er and Wijers, 2006}. However, numerical simulations \cite{Nakar et al., 2003; Nakar and Granot, 2007; Uhm and Beloborodov, 2007; Uhm and Zhang, 2014a; Geng et al., 2014)} showed that the re-brightening feature should appear very smooth dues to the integration over the (relativistc) equal-arrival-time surfaces, which manifests that the observed emission at any given time is given by the blastwave at different latitudes and emission times.
Moreover, for the high energy bands (above $X$-ray, where $\nu>\nu_c$), the \red{the observed flux is independent of the ambient density} \cite{Kumar, 2000; Freedman and Waxman, 2001}.

%%

\subsubsection{Synchrotron self-Compton}

The Synchrotron self-Compton emission allows electrons to cool more efficiently, reducing the corresponding frequency by $(1+Y)^2$ \cite{(e.g.Wei and Lu, 1998; Panaitescu and Kumar, 2000; Sari and Esin, 2001)}; where $Y = u_{sun}=u_{B}$ is the ratio of synchrotron photon energy density and magnetic field energy density. 
It also modifies the spectrum, adding a high energy component that in principle could dominate the GeV band and appear in late $X$-ray lightcurve if the ambient density is sufficiently large engough
\cite{Meszaros and Rees, 1993; Meszaros et al., 1994; Sari and Esin, 2001; Zhang and Meszaros, 2001b}.
Additionally, if the IC cooling occure in the Klein-Nishina regime, the index of synchrotron spectrum could be less steep \cite{e.g. Derishev et al., 2001; Nakar et al., 2009; Daigne et al., 2011; Barniol Duran et al., 2012}. 
While for most GRBs the GeV emission can be explaiend by the synchrotron mechanism operating in the forward shock alone, \cite{(e.g. Kumar and Barniol Duran, 2009, 2010)}, for certain cases, \textit{e.g.,} GRB 130427A \cite{Ackermann et al., 2014} a possible IC component might be required to explain the observations \cite{e.g. Fan et al., 2013a; Liu et al., 2013}.

%%

\subsubsection{Hard electron spectrum}

The effects of energy injection on the afterglow lightcurve, \textit{shallow slope}, cam be mimicked by a electron spectrum with $p\in(1,2)$. In this regime, the minimum electron LF depends on the maximum as 

\begin{equation}
    \gamma_m = \Bigg( \frac{2-p}{p-1} \frac{m_p}{m_e} \varepsilon_e \Gamma \gamma_M^{p-2} \Bigg)^{1/(p-1)}
\end{equation}

\cite{cf. (Dai and Cheng, 2001; Bhattacharya, 2001; Resmi and Bhattacharya, 2008}.

%%

\subsubsection{Effect of neutron decay}

A GRB-jet is expected to be launched from the environment where the free neutrons are present (high temperature and nuclear dissociation may produce them or they can be present in the low $Y_e$ environment). When the proton-neutron elastic collision optical depth falls below $1$, the neutrons stream freely
\cite{(Derishev et al., 1999; Bahcall and Meszaros, 2000; Meszaros and Rees, 2000a; Beloborodov, 2003b)} and $\beta$-decay, as $n\rightarrow p^+ + e^- + \bar{\nu}_e$ with a co-moving life-time of about $15$ minutes. A typical radius of neutron decay $R_{\beta} = c\tau_n'\Gamma_n\propto 10^{15}\Gamma$ (assuming uniform CBM). Occuring continoulsy in the GRB-ejecta, neutron decay can thus modify the prompt emission and early afterglow.
\red{\cite{Beloborodov (2003a)} and \cite{Fan et al. (2005a)}, who found that it can lead to a re-brightening feature in the otherwise power-law decay lightcurve. The signature is different for the ISM and wind cases \cite{(Fan et al., 2005a)}.}

%%

\subsubsection{Radiation front effect}

The high energy prompt emission from GRB is expected to alter the CBM composition before the GRB ejecta starts to propagate into it unltimately changing the properties of afterglow \cite{Madau and Thompson, 2000; Thompson and Madau, 2000; Meszaros et al., 2001; Beloborodov, 2002; Kumar and Panaitescu, 2004}.
The $\gamma$-ray photons, colliding with those photones, scattered by electrons in the CBM, enrich it with $e^--e^+$ pairs (that enhance the scattering, amplifying the process). Thus, $\gamma-$ray emission induce a pair-loading of the CBM, modifying also its velocity (through momentum deposition by the outward moving radiation front). This is or \red{particular} importance for high CBM density cases.

%%

\subsubsection{Transition to Newtonian phase}

\red{Note that 2/3 genergi models I implement in the code}
After a relativist blast wave has swept up the amount of material that is equal to the rest-mass of the GRB ejecta times the $\Gamma_0$, (\textit{e.g.,} $E_0/c^2$ with $E_0$ being the initial energy of the blast wave), the blast wave enters the Newtonian regime.
If the CBM has a uniform density, the radius at which the shock transitions to being sub-relativistic is 
\begin{equation}
    R_N \sim \Bigg( \frac{3E}{4 \pi c^2 n_0 m_p} \Bigg)^{1/3}
\end{equation} 
The dynamics of the non-relativistic blast wave (in unoform CBM) is described by a Sedov-van Neumann-Taylor solution
\begin{equation}
    \upsilon \propto R^{-3/2} \propto t^{-3/5}_{obs} \text{ and } R \propto t_{obs}^{2/5}
\end{equation}
That yields taht $B'\propto t_{obs}^{-3/5}$, $\gamma_m\propto\upsilon^2\propto t_{obs}^{-6/5}$, $\nu_m\propto B'\gamma_m^2\propto t_{obs}^{-3}$ and $\nu_c\propto t_{obs}^{-1/5}$ and $F_{\nu;\max}\propto t^{3/5}_{obs}$.
For a given $p=2.3$, the lightcurve decays faster then in the case of the isotropic relativistic case, but less faster then in the case of post-jet-break. 
So the lightcurve would show a steepening behavior if relativistic-to-Newtonian transition happens before
the jet break \cite{(Dai and Lu, 1999; Huang et al., 1999)}, while it would become less steep if the transition happens after the jet break \cite{(Livio and Waxman, 2000)}.
A generic dynamics model that connects the relativistic phase to nonrelativistic phase was developed by \cite{Huang et al. (1999)} and improved by \cite{Pe'er (2012)} and \cite{Nava et al. (2013)}. The shock wave evolution in the deep Newtonian regime has been studied by \cite{Huang and Cheng (2003)} in the context of GRBs.
The observations of the afterglow in the Newtonan phase is difficult in the optical, but could be done for nearby GRBs in radio. 
In particular, observations of GRB 030329 have shown a late time brightening of the fading lightcurve. Such brightening could me explained by the emission from a \magenta{counter-jet}, that becomes visible after the transition to a newtonian phase. 
\cite{van der Horst et al., 2008; Zhang and MacFadyen, 2009}

For the comprehensive analystical model of the spectra of the synchroton extrernal shock models in all regimes (fast/slow), temporal phasts (forward/reverse shocks, pre and post break, Newtonan phase) see the extended review by \cite{Gao et al. (2013b)}.

%%
%%
%%

\section{Afterglow observations and interpretations}
\red{brief}

[a brief history of afterglow observations prior and after the Swift, that made it possible to follow the GRB $~1$-min after the trigger.]

\subsection{Late time afterglow observations and interpretations}

Prior to \red{swift}: observations supported external shock synchrotron emission models.
\cite{(e.g. Meszaros and Rees, 1997a; Sari et al., 1998; Panaitescu and Kumar, 2001, 2002; Yost et al., 2003),}. For bright GRBs, a jet-break was observed around a day post-trigger \cite{(e.g. Rhoads, 1999; Sari et al., 1999}.
In radio \red{lightcurve peaks around 1 day and then falls}, \cite{e.g. Frail et al., 2000} as the $\nu_m$ (or $\nu_a$) passes through the radio band.
Sinchrotron afterglow model, the broken power alw for the broand band afterglow spectrum \cite{Wijers and Galama, 1999; Harrison et al., 1999}, fits well the observations.
High-quality observations in optical showed complex features, bumps and wiggles \cite{e.g. Holland et al., 2003; Lipkin et al., 2004}. Smooth features were attributed to CBM density inhomogenity \cite{(Lazzati et al., 2002; Dai and Wu, 2003; Nakar and Granot, 2007}, while sharp featers -- to the energy injection from central engine \cite{Katz et al., 1998; Kumar and Piran, 2000b; Zhang and Meszaros, 2002c; Granot et al., 2003}, or energy per solid angle variations \cite{(Kumar and Piran, 2000a; Yamazaki et al., 2004a} or existence of multiple components of the jet \cite{Berger et al., 2003a; Huang et al., 2004; Racusin et al., 2008}.
Overall, modelling of the GRB afterglow \cite{e.g., Panaitescu and Kumar (2001, 2002); Yost et al. (2003)} favioured external shock synchrotron models, expanding in constant censity medium, showing a clustering of jet-corrected isotropic kinetic enetgy \cite{Frail et al., 2001; Bloom et al., 2003),} -- suggesting a roughly constant energy reservoir for GRB

%%

\subsection{Early afterglow observations and interpretations}

The age of early afterglow observations came with the Swift. 
The early bight flast observed in GRB 990123 was not consistent with forward shock models, but was more suited to a reverse shock \cite{Meszaros and Rees, 1997a; Meszaros and Rees, 1999; Sari and Piran, 1999a,b}. Such obtical flash from the reverse shock is possible if the magnetic field in it is stronger than in forward shock. \cite{Fan et al., 2002; Zhang et al., 2003a; Kumar and Panaitescu, 2003}, but not too strong as to weaken the reverse shock itself and consequently the emergent flux \cite{e.g. Zhang and Kobayashi, 2005; Mimica et al., 2009; Narayan et al., 2011}. 
Reverse shock was thought to be resposible for radio flares, associated with optical flases, peaking at $\sim 1$~day \cite{e.g. Sari and Piran, 1999a; Kobayashi and Zhang, 2003b)}

\red{[Here: Fig 10 from source with AZfterglow general pictrue.]}
In X-ray, the canonical peacture consists of 5 regions (\cite{Zhang et al., 2006; Nousek et al., 2006,}):
\begin{itemize}
    \item 1. the steep decay phase which is the tail of prompt emission \cite{Barthelmy et al., 2005b}, can be attributed to a high altitude emission and the turn off of the central engine \cite{Kumar and Panaitescu, 2000a; Dermer, 2004; Zhang et al., 2006; Nousek et al., 2006; Liang et al., 2006a} or a time dependent activity of the central engine if the emission comes from smaller radii \cite{Fan and Wei, 2005; Barniol Baruran and Kumar, 2009}
    \item 2. shallow decay phase (or plateau) Can be attributed (in the case of external shock model) as energy injection into the blastwave \cite{Zhang et al., 2006; Nousek et al., 2006; Panaitescu et al., 2006b}
    \item 3. the normal decay phase (for a forward shock model)
    \item 4. the late steepening phase (jet break)
    \item 5. X-ray flares, believed to be powered by the late time engine activity 
    \cite{Ioka et al., 2005; Burrows et al., 2005b; Fan andWei, 2005; Zhang et al., 2006; Liang et al., 2006a; Lazzati and Perna, 2007; Chincarini et al., 2007; Maxham and Zhang, 2009; Margutti et al., 2010}
\end{itemize}

Alternatevely, X-ray afterglow can be understood from a poit of view of a two-comoennt model, with a "prompt component" being responsible for prompt emission and rapid decay, and "afterglow" comonent -- for the plateu and normal decay phase \cite{O'Brien et al., 2006; Willingale et al., 2007; Ghisellini et al., 2009}.

Puzzling in some GRB afterglows is the absence of the achromatic jet-break (temporal breaks, between phases 2-3 or 3-4) in optical and X-ray \cite{Panaitescu et al., 2006a; Fan et al., 2006; Liang et al., 2007a, 2008a; Huang et al., 2007}

%%

\subsubsection{Steep decay of early X-ray light-curve}

\red{more details on region [1]}
- Prompt emission has internal origin (pres-Swift)
- the X-ray flux in the phase 1 however, is not settled wether it is from rapid cessation of the prompt radiation \cite{(Kumar and Panaitescu, 2000a; Zhang et al., 2006} or emission from a somewhat less
rapidly dying central engine \cite{Fan and Wei, 2005; Barniol Duran and Kumar, 2009}.

Additionally, steep decay phase can be reproduced by models with rapidly expanding cocoon \cite{Pe'er et al., 2006a} rapid discharge of hadronic energy of the blastwave \cite{Dermer, 2007} high-latitude emission in the external reverse shock \cite{Uhm and Beloborodov, 2007; Uhm et al., 2012} sweeping of the external forward shock synchrotron spectrum with a low maximum frequency across the X-ray band \cite{Petropoulou et al., 2011}, where for the last three the cite of origin is assumed to the external shock.

%%

\subsubsection{Sudden increase in X-ray flux (flares)}

usually attributed to the re-start of the central engine. \cite{Burrows et al., 2005b; Zhang et al., 2006; Fan and Wei, 2005).}. This explanantion is supported by modelling.

%%

\subsubsection{Plateaus in X-ray light-curves}

This referres to the shallow decay on the phase 2. and subsequent segment 3 and 4. 

Explanations: energy injection to the decelerating external shock, that is laterterminated in the onset of the phase 3. \cite{Zhang et al., 2006; Granot et al., 2006; Fan and Piran, 2006b; Nousek et al., 2006; Panaitescu et al., 2006b}. This implies that achromatic breaks must occure across EM spectrum at the same time. This indeed was see in GRB 060614 (\cite{Mangano et al., 2007}) and GRB 060729 (\cite{Grupe et al., 2007}). However, ob served chromatic afterglows without a break cannot be explained.

Another explanation is the two component external shock model with narrow jet, dominating the X-ray emission, and a wide jet, producing mostly optical emission. \cite{e.g. Racusin et al., 2008}. This model can account for some observations if the free parameters of the model change considerablty \cite{de Pasquale et al., 2009).}. The model can be augmented by the addition of the reverse shock. For istance, it can be assumed that the observed chromatic lightcurves in X-ray and optical originate from the long-lasting RS rather than from the FS \cite{Uhm and Beloborodov (2007) and Genet et al. (2007)}. This is plausable as RS is can produce more diverse lightcurves being strongly dependent of the ejecta stratification, CBM density and microphsycis \cite{Uhm et al., 2012; Uhm and Zhang, 2014a}, but it does not explain why the FS emission is suptessed. A combination of a RS and FS emission, where in different bands different shocks might domiante, is more plausagle explanation. 

Additionally, allowing the microphysical parameters $\varepsilon_e$ and $\varepsilon_B$ to evolve might account for the chromatic behaviour of X-ray plateu. \cite{(Ioka et al., 2006; Panaitescu, 2006)}

Additionally, invoking a wind-like CBM, the shallow decay phase can be interpreted as the synchrotron radiation from the shock in the free-costing (pre-deceleration) regime \cite{Shen and Matzner (2012)}. However such model predicts only the achromatic afterglows (with a break in optical and X-ray at the same time) and in addition, requires low $\Gamma$, that is in disagreement with prompt emsission models.

More explanations include the fuiling the X-ray emission by scattered prompt emission in the galactic dust.

%%

\subsubsection{Steep decay following the plateau in X-ray light-curve}

Some rare GRBs display an afterglow with a very steep decay after the plateu (in X-ray). Such steep decay can only have an internal origin -- direct dissipation of a long-lived jet. This supports the idea, in addition to the existance of a plateu, suggests that some GRBs have a long-lived central engine.

It was also suggested, perhaps the whole X-ray emission is powered by a continous jet from the central engine, with the flux from the external shock, hidden within \cite{Ghisellini et al., 2007}. This idea is supported by the fact that the X-ray lightcurve roughly follows the accretion histopry within the frameword of collapsar model. \cite{Kumar et al., 2008a,b; Cannizzo and Gehrels, 2009; Lindner et al., 2010} or spin-down power of a magnetar central engine \cite{Yu et al., 2010; Metzger et al., 2011}. There, the $L_{x}\sim P_a$, where $P_a$ is the accretion pwoer or a spin-down power of the central engine.
Overall, there is a certain merit to models where the X-ray emission is atributed to the internal to the jet processes, while the optical emission is produced in an external shock, if the former shos chromatic behaviour, while if both lightcurves are acromatic, the emission is generated by the external forward shock.

Overall, three possible emissiion cites can be considered:
- erratic component, flates,
- the brocken power-law X-ray compoennt
- broken power-law optical component (if chromatic).
Similarly, there are at least three physical emisison cites:
- FS
- RS, 
- internal dissipation site within the outflow (prior to CBM interations) which are: outflow photosphere, internal shocks, or magnetic dissipation sites in highly magnetized jets.

Long-lived dentral engine makes system complex, adding energy to the FS and matter to the RS, making it long-lasting. Additionally, the angular non-uniformity of the jet $\Gamma$, would further complicate the material pile-up.

In the Swift era, the access to theearly GRB afterglow show how diverse and complex it. While late afterglow can be modelled within the external forward shock framework, this is only possible for a small semple of "well-behaved" early afterglows with clean achromatic behaviour. Many questions remain open. 

\red{COPY}
\gray{What fraction
    of afterglows can be interpreted within the standard external shock model?
    Are the differences between the two categories (afterglows that are due to
    FS and those that are not) due to intrinsic differences in the central engine
    properties or these due to external factors such as variations in CBM from one
    burst to another? For those bursts that can be interpreted with the standard
    FS model, what are the shock microphysics parameters, and why do they vary
    from one burst to another?
}

Statsitical study of \cite{X.-G. Wang et al. (2014}, in preparation) suggests that at least half of the observed GRBs require two different emission components fot X-ray and optical.

\red{Copy}
\gray{
    It is worth pointing out that short GRBs typically have fainter afterglows due
    to their lower energies and probably lower circumburst densities (Panaitescu
    et al., 2001). Comparing with the prompt emission properties, one nds that
    both long and short GRBs follow some similar correlations among prompt
    emission and afterglow properties (Gehrels et al., 2008; Nysewander et al.,
    2009; Kann et al., 2011). This suggests a similar radiative effciency and probably
    also a similar circumburst environment for both long and short GRBs
    (Zhang et al., 2007a; Nysewander et al., 2009).
}

%%
%%
%%

\subsection{High energy ($>10^2$~MeV) afterglow radiation}
\red{very brief}

%% possible mechanisms
A possible explanations for the observed delayed long lasting, high energy photons from GRB (eg  GRB 941017), are the external shock, SSC in it, \cite{Dermer et al., 2000; Zhang and Meszaros, 2001b}, SSC in FS and RS, and two cross IC processes where the photons from one shock front upscattered in another.
\cite{Wang et al., 2001a,b; Granot and Guetta, 2003; Pe'er and Waxman, 2004; Gupta and Zhang, 2007b; Fan and Piran, 2008; Zou et al., 2009b).}
Also, electrons in FS/RS can upscatter prompt $\gamma$-rays leading to high energy emission \cite{Meszaros and Rees, 1994; Beloborodov, 2005; Fan et al., 2005b}. Electron-positron pairs with high LF can upscatter even the CMB photons in the IGM (when the IGM magnetic field strength is small) \cite{Plaga, 1995}.

%% mechanisms supported by observations
Systematic study of observed RGBs above $100$ MeV shows that the mechanism resposible for high energy $\gamma$-rays (emitted after the prompt phase) -- is the synchrotron process in the external forward shock \cite{Kumar and Barniol Duran, 2009, 2010; Ghisellini et al., 2010} 
\gray{(as they typically arrive few seconds after the GRB trigger and it lasts for longer then $10^3$~s, longer then the burst)}

%%Reasons:
The spectral index and the decay of LAT lightcurve satisfies the closure relationship almost prefectly for synchrotron radiation from the shock heated circum-burst medium by the relativisitc jet of a GRB in the spectral regime  $\nu > \nu_c$ \gray{Cases where the temporal decline is just slightly steeper were attributed  othe effects of radiaitive losses affecting the shock dynamics \cite{Ghisellini et al. (2010)}. On the other hand, the IC cooling of high energy electrons later in time (when scattering is no longer supressed as it is in the deep Klein-Nishina) might explain the steeper decline as well \cite{Wang et al. (2010)}}
In that regime $\nu > \nu_c$, the specific flux depends only on the energy of the blast wave the fraction of energy in the relativist electrons, while being independent of CBM density and only weekly dependent on the $\varepsilon_B$ \cite{Kumar (2000)}. 
This allows to predict the Fermi-LAT band emission knowing the prompt $\gamma$-ray radiation \cite{Kumar and Barniol Duran, 2010}. 
Addiitonally, the early time Fermi data $t\sim10^2$~s allows one to predict the late optical and X-ray fluxes. It can also be done in reverse, to predict the flux at $100$~MeV at early times.
\red{Thus, it suggests}, the the observed high energy emission originates in the external shock by the synchrotron process.
Note, that generation of very high energy photons ($50\Gamma$~MeV - $5$~GeV) is compicated within the synchtron process (see Synchrotron section on maximum $\nu$). See also \cite{Kumar et al. (2012)}. 
The IC contribution is a possible solution. See \cite{Zhang and Meszaros (2001b)} for the shock parameter range of relevance. 
High energy emission via synchrotron mechansis in the highly relativistc external shock required very low $\varepsilon_B$ ($\sim 10^{-6}$) it order for the emission not to exceed observed values. Such low magnetic field however is sufficient for trapping electrons with thermal LF $\sim10^8$, (taht generates GeV photons), as long as IC losses are small for these electrons.

Notably the GeV emission, contrary to the X-ray, follows a simple power-law \cite{Liang et al., 2009; Evans et al., 2009}.
X-ray lightcruves topology is usually "steep-shallow-normal-steep". 

The GeV emission at the prompt phase is \red{however} lickly not to be dominated by the external shock component, and the external shock emission starts to become dominant after the prompt component, according to observations \cite{Zhang et al., 2011} and theoretical modeling \cite{Gao et al., 2009; Maxham et al., 2011; He et al., 2011; Liu and Wang, 2011}. 
During the prompt phase, the energy is still being added to the blastwave \cite{Maxham et al., 2011}.

%%
%%
%%

\section{Collisionless shock properties from GRB afterglow observations}
\red{statisitcal studies}


%% --------------- 
%%
%% References
%%
%% ---------------

\newpage

\bibliography{references}

\end{document}